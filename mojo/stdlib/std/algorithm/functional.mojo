# ===----------------------------------------------------------------------=== #
# Copyright (c) 2026, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
"""Implements higher-order functions.

You can import these APIs from the `algorithm` package. For example:

```mojo
from std.algorithm import map
```
"""

import std.sys
from std.collections import OptionalReg
from std.collections.string.string_slice import get_static_string
from std.math import align_down, ceildiv, clamp
from std.os import abort
from std.pathlib import Path

from std.gpu import (
    MAX_THREADS_PER_BLOCK_METADATA,
    block_dim,
    block_idx,
    grid_dim,
    thread_idx,
    PDLLevel,
    launch_dependent_grids,
    wait_on_dependent_grids,
)
from std.gpu.primitives.grid_controls import (
    pdl_launch_attributes,
)  # @doc_private
from std.gpu.host import DeviceContext
from std.gpu.host.info import B200, is_cpu, is_gpu
from std.runtime import tracing
from std.runtime.asyncrt import DeviceContextPtr, TaskGroup, parallelism_level
from std.runtime.tracing import Trace, TraceLevel, get_safe_task_id, trace_arg

from std.utils.index import Index, IndexList
from std.utils.numerics import FlushDenormals
from std.utils.static_tuple import StaticTuple

# ===-----------------------------------------------------------------------===#
# Map
# ===-----------------------------------------------------------------------===#


@always_inline
fn map[
    origins: OriginSet, //, func: fn(Int) capturing[origins] -> None
](size: Int):
    """Maps a function over the integer range [0, size).
    This lets you apply an integer index-based operation across data
    captured by the mapped function (for example, an indexed buffer).

    Parameters:
        origins: Capture origins for mapped function.
        func: Parameterized function applied at each index.

    Args:
        size: Number of elements in the index range.

    For example:

    ```mojo
    from std.algorithm import map

    def main() raises:
        # Create list with initial values to act on
        var list = List[Float32](1.0, 2.0, 3.0, 4.0, 5.0)

        # Function applied to the value at each index
        @parameter
        fn exponent_2(idx: Int):
            list[idx] = 2.0 ** list[idx]

        # Apply the mapped function across the index range
        map[exponent_2](len(list))

        # Show results
        for idx in range(len(list)):
            print(list[idx])
    ```

    Example output:

    ```output
    2.0
    4.0
    8.0
    16.0
    32.0
    ```

    :::note
    Don't confuse `algorithm.map` (this eager, index-based helper) with
    [`iter.map`](https://docs.modular.com/mojo/std/iter/map/),
    which returns a lazy iterator that applies a function to each element.
    :::

    """
    for i in range(size):
        func(i)


# ===-----------------------------------------------------------------------===#
# Vectorize
# ===-----------------------------------------------------------------------===#


@always_inline
fn vectorize[
    func: fn[width: Int](idx: Int) unified -> None,
    //,
    simd_width: Int,
    /,
    *,
    unroll_factor: Int = 1,
](size: Int, closure: func):
    """Simplifies SIMD optimized loops by mapping a function across a range from
    0 to `size`, incrementing by `simd_width` at each step. The remainder of
    `size % simd_width` will run in separate iterations.

    Parameters:
        func: The function that will be called in the loop body.
        simd_width: The SIMD vector width.
        unroll_factor: The unroll factor for the main loop (Default 1).

    Args:
        size: The upper limit for the loop.
        closure: The captured state of the function bound to func.

    The below example demonstrates how you could improve the performance of a
    loop, by setting multiple values at the same time using SIMD registers on
    the machine:

    ```mojo
    from std.algorithm.functional import vectorize
    from std.sys import simd_width_of

    # The amount of elements to loop through
    comptime size = 10
    # How many Dtype.int32 elements fit into the SIMD register (4 on 128bit)
    comptime simd_width = simd_width_of[DType.int32]()  # assumed to be 4 in this example

    fn main():
        var p = alloc[Int32](size)

        fn closure[width: Int](i: Int) unified {mut}:
            print("storing", width, "els at pos", i)
            p.store[width=width](i, i)

        vectorize[simd_width](size, closure)
        print(p.load[width=simd_width]())
        print(p.load[width=simd_width](simd_width))
    ```

    On a machine with a SIMD register size of 128, this will set 4xInt32 values
    on each iteration. The remainder of 10 % 4 is 2, so those last two elements
    will be set in two separate iterations:

    ```plaintext
    storing 4 els at pos 0
    storing 4 els at pos 4
    storing 1 els at pos 8
    storing 1 els at pos 9
    [0, 0, 0, 0, 4, 4, 4, 4, 8, 9]
    ```

    You can also unroll the loop to potentially improve performance at the cost
    of binary size:

    ```
    vectorize[closure, width, unroll_factor=2](size)
    ```

    In the generated assembly the function calls will be repeated, resulting in
    fewer arithmetic, comparison, and conditional jump operations. The assembly
    would look like this in pseudocode:

    ```
    closure[4](0)
    closure[4](4)
    # Remainder loop won't unroll unless `size` is passed as a parameter
    for i in range(8, 10):
        closure[1](i)
        closure[1](i)
    ```

    You can pass `size` as a parameter if it's compile time known to reduce the
    iterations for the remainder. This only occurs if the remainder is an
    exponent of 2 (2, 4, 8, 16, ...). The remainder loop will still unroll for
    performance improvements if not an exponent of 2.
    """
    comptime assert simd_width > 0, "simd width must be > 0"
    comptime assert unroll_factor > 0, "unroll factor must be > 0"
    debug_assert(size >= 0, "size must be >= 0")

    comptime unrolled_simd_width = simd_width * unroll_factor
    var simd_end = align_down(UInt(size), UInt(simd_width))
    var unrolled_end = align_down(UInt(size), UInt(unrolled_simd_width))

    for unrolled_idx in range(0, unrolled_end, unrolled_simd_width):
        comptime for idx in range(unroll_factor):
            closure[simd_width](unrolled_idx + idx * simd_width)

    comptime if unroll_factor > 1:
        for simd_idx in range(unrolled_end, simd_end, simd_width):
            closure[simd_width](simd_idx)

    for i in range(simd_end, size):
        closure[1](i)


@always_inline
fn vectorize[
    func: fn[width: Int](idx: Int, evl: Int) unified -> None,
    //,
    simd_width: Int,
    /,
    *,
    unroll_factor: Int = 1,
](size: Int, closure: func):
    """Simplifies SIMD optimized loops by mapping a function across a range from
    0 to `size`, incrementing by `simd_width` at each step. The main loop runs
    with a fixed SIMD width of `simd_width`. Any remainder (`size % simd_width`)
    is executed with a single final call using predication via the `evl`
    (effective vector length) argument.

    Compared to `vectorize` variants that run the remainder as scalar
    iterations (`width=1`), this version keeps the SIMD width fixed and passes
    the number of active lanes in `evl` for the last (partial) vector. The
    closure is responsible for honoring `evl` (e.g. using masked loads/stores)
    to avoid out-of-bounds accesses.

    Parameters:
        func: The function that will be called in the loop body. It must accept
            an `idx` and an `evl` (effective vector length). For all full SIMD
            iterations `evl == simd_width`. For the final partial iteration
            `0 < evl < simd_width`.
        simd_width: The SIMD vector width.
        unroll_factor: The unroll factor for the main loop (Default 1).

    Args:
        size: The upper limit for the loop.
        closure: The captured state of the function bound to func.

    The below example demonstrates how to set multiple values at the same time
    using SIMD registers, while handling the tail with `evl` by generating a mask:

    ```mojo
    from std.algorithm.functional import vectorize
    from std.sys import simd_width_of
    from std.math import iota
    from std.sys.intrinsics import masked_store

    comptime size = 10
    comptime simd_width = simd_width_of[DType.int32]()  # assumed 4 in this example

    fn main():
        var p = alloc[Int32](size)

        fn closure[width: Int](i: Int, evl: Int) unified {mut}:
            print("storing", evl, "of", width, "els at pos", i)
            var val = SIMD[DType.int32, width](i)

            # Optimization: Constant propagation eliminates this check in the main loop
            if evl == width:
                p.store[width=width](i, val)
            else:
                # Tail loop: Generate mask from EVL to prevent OOB
                var mask = iota[DType.int32, width]().lt(evl)
                masked_store[width](val, p + i, mask)

        vectorize[simd_width](size, closure)

        print(p.load[width=simd_width]())
        print(p.load[width=simd_width](simd_width))
        print(p.load[width=2](2 * simd_width))
        p.free()
    ```

    On a machine with a SIMD register size of 128, this will set 4xInt32 values
    on each full iteration. The remainder of 10 % 4 is 2, so the tail will be
    handled by a single call with `evl=2`:

    ```plaintext
    storing 4 of 4 els at pos 0
    storing 4 of 4 els at pos 4
    storing 2 of 4 els at pos 8
    [0, 0, 0, 0]
    [4, 4, 4, 4]
    [8, 8]
    ```

    You can also unroll the main loop to potentially improve performance at the
    cost of binary size:

    ```
    vectorize[simd_width, unroll_factor=2](size, closure)
    ```

    In the generated assembly the full-width calls will be repeated, resulting
    in fewer arithmetic, comparison, and conditional jump operations. In
    pseudocode:

    ```
    closure[4](0, 4)
    closure[4](4, 4)
    closure[4](8, 2)  # single predicated tail call
    ```

    Notes:
        - This implementation does not execute the remainder as scalar
          iterations. The closure must correctly handle `evl` to keep memory
          accesses in-bounds.
        - If `size < simd_width`, the loop will consist of a single call:
          `closure[simd_width](0, size)`.
    """
    comptime assert simd_width > 0, "simd width must be > 0"
    comptime assert unroll_factor > 0, "unroll factor must be > 0"
    debug_assert(size >= 0, "size must be >= 0")

    comptime unrolled_simd_width = simd_width * unroll_factor
    var simd_end = Int(align_down(UInt(size), UInt(simd_width)))
    var unrolled_end = Int(align_down(UInt(size), UInt(unrolled_simd_width)))

    for unrolled_idx in range(0, unrolled_end, unrolled_simd_width):
        comptime for idx in range(unroll_factor):
            closure[simd_width](unrolled_idx + idx * simd_width, simd_width)

    comptime if unroll_factor > 1:
        for simd_idx in range(unrolled_end, simd_end, simd_width):
            closure[simd_width](simd_idx, simd_width)

    var remainder = size - simd_end
    if remainder > 0:
        closure[simd_width](simd_end, remainder)


@always_inline
fn vectorize[
    func: fn[width: Int](idx: Int) unified -> None,
    //,
    simd_width: Int,
    /,
    *,
    size: Int,
    unroll_factor: Int = size if sys.is_gpu() else 1,
](closure: func):
    """Simplifies SIMD optimized loops by mapping a function across a range from
    0 to `size`, incrementing by `simd_width` at each step. The remainder of
    `size % simd_width` will run in a single iteration if it's an exponent of
    2.

    Parameters:
        func: The function that will be called in the loop body.
        simd_width: The SIMD vector width.
        size: The upper limit for the loop.
        unroll_factor: The unroll factor for the main loop (Default 1).

    Args:
        closure: The captured state of the function bound to func.

    The below example demonstrates how you could improve the performance of a
    loop, by setting multiple values at the same time using SIMD registers on
    the machine:

    ```mojo
    from std.algorithm.functional import vectorize
    from std.sys import simd_width_of

    # The amount of elements to loop through
    comptime size = 10
    # How many Dtype.int32 elements fit into the SIMD register (4 on 128bit)
    comptime simd_width = simd_width_of[DType.int32]()  # assumed to be 4 in this example

    fn main():
        var p = UnsafePointer[Int32].alloc(size)

        # The closure can capture the `p` pointer with unified {mut}
        fn closure[width: Int](i: Int) unified {mut}:
            print("storing", width, "els at pos", i)
            p.store[width=width](i, i)

        vectorize[simd_width](size, closure)
        print(p.load[width=simd_width]())
        print(p.load[width=simd_width](simd_width))
    ```

    On a machine with a SIMD register size of 128, this will set 4xInt32 values
    on each iteration. The remainder of 10 % 4 is 2, so those last two elements
    will be set in a single iteration:

    ```plaintext
    storing 4 els at pos 0
    storing 4 els at pos 4
    storing 2 els at pos 8
    [0, 0, 0, 0, 4, 4, 4, 4, 8, 8]
    ```

    If the remainder is not an exponent of 2 (2, 4, 8, 16 ...) there will be a
    separate iteration for each element. However passing `size` as a parameter
    also allows the loop for the remaining elements to be unrolled.

    You can also unroll the main loop to potentially improve performance at the
    cost of binary size:

    ```
    vectorize[width, size=size, unroll_factor=2](closure)
    ```

    In the generated assembly the function calls will be repeated, resulting in
    fewer arithmetic, comparison, and conditional jump operations. The assembly
    would look like this in pseudocode:

    ```
    closure[4](0)
    closure[4](4)
    closure[2](8)
    ```
    """
    comptime assert simd_width > 0, "simd width must be > 0"
    comptime assert unroll_factor > 0, "unroll factor must be > 0"
    comptime assert size >= 0, "size must be >= 0"

    comptime unrolled_simd_width = simd_width * unroll_factor
    comptime simd_end = align_down(size, simd_width)
    comptime unrolled_end = align_down(size, unrolled_simd_width)

    comptime for unrolled_idx in range(0, unrolled_end, unrolled_simd_width):
        comptime for idx in range(unroll_factor):
            closure[simd_width](unrolled_idx + idx * simd_width)

    comptime if unroll_factor > 1:
        for simd_idx in range(unrolled_end, simd_end, simd_width):
            closure[simd_width](simd_idx)

    comptime if size > simd_end:
        comptime if (size - simd_end).is_power_of_two():
            closure[size - simd_end](simd_end)
        else:
            comptime for i in range(simd_end, size):
                closure[1](i)


# ===-----------------------------------------------------------------------===#
# Parallelize
# ===-----------------------------------------------------------------------===#


@always_inline
fn sync_parallelize[
    origins: OriginSet,
    //,
    func: fn(Int) raises capturing[origins] -> None,
](num_work_items: Int):
    """Executes func(0) ... func(num_work_items-1) as parallel sub-tasks,
    and returns when all are complete.

    TODO: Currently exceptions raised by func will cause a trap rather than
          be propagated back to the caller.

    Parameters:
        origins: The capture origins.
        func: The function to invoke.

    Args:
        num_work_items: Number of parallel tasks.
    """
    # We have no tasks, so do nothing.
    if num_work_items <= 0:
        # No-op
        return

    # If profiling is enabled, and the caller's thread has an active profile
    # entry, each sub-task will also be profiled with a reference back to the
    # parent. Otherwise parent_id will be zero.
    var parent_id = tracing.get_current_trace_id[TraceLevel.THREAD]()

    @parameter
    @always_inline
    fn func_wrapped(i: Int):
        with FlushDenormals():
            try:
                with Trace[TraceLevel.THREAD, target = StaticString("cpu")](
                    "task", task_id=i, parent_id=parent_id
                ):
                    func(i)
            except e:
                abort(String(e))

    if num_work_items == 1:
        # Just run inline.
        func_wrapped(0)
        return

    @always_inline
    @parameter
    async fn task_fn(i: Int):
        func_wrapped(i)

    # Run sub-tasks using the 'default' runtime. If the caller is part of
    # Mojo kernel executing within the Modular Inference Engine then the
    # default runtime will be that established by the engine. Otherwise a
    # suitable runtime will be created if it does not already exist.
    var num_threads = parallelism_level()
    var num_per_lq_tasks = num_work_items // num_threads
    var num_global_queue_tasks = num_work_items % num_threads
    var tg = TaskGroup()
    var count = 0
    for _ in range(num_per_lq_tasks):
        for j in range(num_threads):
            tg._create_task(task_fn(count), j)
            count += 1
    for _ in range(num_global_queue_tasks):
        tg.create_task(task_fn(count))
        count += 1

    # execute Nth task inline. When using local queues, we need to know
    # this threads tid so that we do not push tasks into its queue.
    # This involves plumbing workerIDTLS from the threadpool. It may be
    # worth to do this. Until then we schedule all tasks through addTask
    tg.wait()


@always_inline
fn parallelize[
    origins: OriginSet, //, func: fn(Int) capturing[origins] -> None
](num_work_items: Int):
    """Executes func(0) ... func(num_work_items-1) as sub-tasks in parallel, and
    returns when all are complete.

    Parameters:
        origins: The capture origins.
        func: The function to invoke.

    Args:
        num_work_items: Number of parallel tasks.
    """

    _parallelize_impl[func](num_work_items, parallelism_level())


@always_inline
fn parallelize[
    origins: OriginSet, //, func: fn(Int) capturing[origins] -> None
](num_work_items: Int, num_workers: Int):
    """Executes func(0) ... func(num_work_items-1) as sub-tasks in parallel, and
    returns when all are complete.

    Parameters:
        origins: The capture origins.
        func: The function to invoke.

    Args:
        num_work_items: Number of parallel tasks.
        num_workers: The number of workers to use for execution.
    """

    _parallelize_impl[func](num_work_items, num_workers)


@always_inline
fn _parallelize_impl[
    origins: OriginSet, //, func: fn(Int) capturing[origins] -> None
](num_work_items: Int, num_workers: Int):
    debug_assert(num_workers > 0, "Number of workers must be positive")
    # Calculate how many items are picked up by each worker.
    var chunk_size = num_work_items // num_workers
    # Calculate how many workers need to add an extra item to their work.
    var extra_items = num_work_items % num_workers

    # We coalesce consecutive groups of work items into a single dispatch by
    # using the coarse_grained_func below.
    @always_inline
    @parameter
    fn coarse_grained_func(thread_idx: Int):
        # Calculate the consecutive range of work items this invocation is
        # responsible for.
        var start_idx = thread_idx * chunk_size + min(thread_idx, extra_items)
        for i in range(chunk_size + Int(thread_idx < extra_items)):
            func(start_idx + i)

    sync_parallelize[coarse_grained_func](num_workers)


# ===-----------------------------------------------------------------------===#
# tile
# ===-----------------------------------------------------------------------===#

comptime Static1DTileUnitFunc = fn[width: Int](Int) capturing[_] -> None
"""Signature of a 1D tiled function with static tile size.

The function takes a static tile size parameter and an offset argument,
i.e. `func[tile_size: Int](offset: Int)`.
"""

comptime Dynamic1DTileUnitFunc = fn(Int, Int) capturing[_] -> None
"""Signature of a 1D tiled function with dynamic tile size.

The function takes a dynamic tile size and an offset argument,
i.e. `func(offset: Int, tile_size: Int)`.
"""


comptime BinaryTile1DTileUnitFunc = fn[width: Int](Int, Int) capturing[
    _
] -> None
"""
Signature of a tiled function that performs some work with a dynamic tile size
and a secondary static tile size.
"""


@always_inline
fn tile[
    workgroup_function: Static1DTileUnitFunc, tile_size_list: List[Int]
](offset: Int, upperbound: Int):
    """A generator that launches work groups in specified list of tile sizes.

    A workgroup function is a function that can process a configurable
    consecutive "tile" of workload. E.g.
      `work_on[3](5)`
    should launch computation on item 5,6,7, and should be semantically
    equivalent to
      `work_on[1](5)`, `work_on[1](6)`, `work_on[1](7)`.

    This generator will try to proceed with the given list of tile sizes on the
    listed order. E.g.
        `tile[func, (3,2,1)](offset, upperbound)`
    will try to call `func[3]` starting from offset until remaining work is less
    than 3 from upperbound and then try `func[2]`, and then `func[1]`, etc.

    Parameters:
        workgroup_function: Workgroup function that processes one tile of
          workload.
        tile_size_list: List of tile sizes to launch work.

    Args:
        offset: The initial index to start the work from.
        upperbound: The runtime upperbound that the work function should not
          exceed.
    """

    # Initialize where to start on the overall work load.
    var current_offset: Int = offset

    comptime for tile_size in tile_size_list:
        # Process work with the tile size until there's not enough remaining work
        #  to fit in a tile.
        while current_offset <= upperbound - tile_size:
            workgroup_function[tile_size](current_offset)
            current_offset += tile_size


@always_inline
fn tile[
    workgroup_function: Dynamic1DTileUnitFunc,
](offset: Int, upperbound: Int, *tile_size_list: Int):
    """A generator that launches work groups in specified list of tile sizes.

    This is the version of tile generator for the case where work_group function
    can take the tile size as a runtime value.

    Parameters:
        workgroup_function: Workgroup function that processes one tile of
          workload.

    Args:
        offset: The initial index to start the work from.
        upperbound: The runtime upperbound that the work function should not
          exceed.
        tile_size_list: List of tile sizes to launch work.
    """
    # Initialize the work_idx with the starting offset.
    var work_idx = offset
    # Iterate on the list of given tile sizes.
    for tile_size in tile_size_list:
        # Launch workloads on the current tile sizes until cannot proceed.
        while work_idx <= upperbound - tile_size:
            workgroup_function(work_idx, tile_size)
            work_idx += tile_size
    # Clean up the remaining workload with a residue tile that exactly equals to
    #  the remaining workload size.
    # Note: This is the key difference from the static version of tile
    #  generator.
    if work_idx < upperbound:
        workgroup_function(work_idx, upperbound - work_idx)


@always_inline
fn tile[
    secondary_tile_size_list: List[Int],
    secondary_cleanup_tile: Int,
    workgroup_function: BinaryTile1DTileUnitFunc,
](
    offset: Int,
    upperbound: Int,
    *primary_tile_size_list: Int,
    primary_cleanup_tile: Int,
):
    """A generator that launches work groups in specified list of tile sizes
    until the sum of primary_tile_sizes has exceeded the upperbound.

    Parameters:
        secondary_tile_size_list: List of static tile sizes to launch work.
        secondary_cleanup_tile: Last static tile to use when primary tile sizes
          don't fit exactly within the upperbound.
        workgroup_function: Workgroup function that processes one tile of
          workload.

    Args:
        offset: The initial index to start the work from.
        upperbound: The runtime upperbound that the work function should not
          exceed.
        primary_tile_size_list: List of dynamic tile sizes to launch work.
        primary_cleanup_tile: Last dynamic tile to use when primary tile sizes
          don't fit exactly within the upperbound.
    """
    var work_idx = offset
    comptime num_tiles = len(secondary_tile_size_list)

    comptime for i in range(num_tiles):
        comptime secondary_tile_size = secondary_tile_size_list[i]
        var primary_tile_size = primary_tile_size_list[i]

        while work_idx <= upperbound - primary_tile_size:
            workgroup_function[secondary_tile_size](work_idx, primary_tile_size)
            work_idx += primary_tile_size

    # launch the last cleanup tile
    if work_idx < upperbound:
        workgroup_function[secondary_cleanup_tile](
            work_idx, primary_cleanup_tile
        )


# ===-----------------------------------------------------------------------===#
# tile2d
# ===-----------------------------------------------------------------------===#


comptime Static2DTileUnitFunc = fn[tile_x: Int, tile_y: Int](
    Int, Int
) capturing[_] -> None
"""Signature of a 2D tiled function with static tile size.

The function takes static tile size parameters and offset arguments, i.e.
`func[tile_size_x: Int, tile_size_y: Int](offset_x: Int, offset_y: Int)`.
"""


@always_inline
fn tile[
    workgroup_function: Static2DTileUnitFunc,
    tile_sizes_x: List[Int],
    tile_sizes_y: List[Int],
](offset_x: Int, offset_y: Int, upperbound_x: Int, upperbound_y: Int):
    """Launches workgroup_function using the largest tile sizes possible in each
    dimension, starting from the x and y offset, until the x and y upperbounds
    are reached.

    Parameters:
        workgroup_function: Function that is invoked for each tile and offset.
        tile_sizes_x: List of tile sizes to use for the first parameter of workgroup_function.
        tile_sizes_y: List of tile sizes to use for the second parameter of workgroup_function.

    Args:
        offset_x: Initial x offset passed to workgroup_function.
        offset_y: Initial y offset passed to workgroup_function.
        upperbound_x: Max offset in x dimension passed to workgroup function.
        upperbound_y: Max offset in y dimension passed to workgroup function.
    """
    # Initialize where to start on the overall work load.
    var current_offset_y: Int = offset_y

    comptime for tile_size_y in tile_sizes_y:
        while current_offset_y <= upperbound_y - tile_size_y:
            var current_offset_x = offset_x

            comptime for tile_size_x in tile_sizes_x:
                while current_offset_x <= upperbound_x - tile_size_x:
                    workgroup_function[tile_size_x, tile_size_y](
                        current_offset_x, current_offset_y
                    )
                    current_offset_x += tile_size_x

            current_offset_y += tile_size_y


# ===-----------------------------------------------------------------------===#
# Unswitch
# ===-----------------------------------------------------------------------===#

# Signature of a function that unswitch can take.
comptime SwitchedFunction = fn[sw: Bool]() raises capturing[_] -> None
"""Signature of a function that unswitch can take."""

# Version of unswitch supporting 2 predicates.
comptime SwitchedFunction2 = fn[sw0: Bool, sw1: Bool]() capturing[_] -> None
"""Signature for unswitch supporting 2 predicates."""


@always_inline
fn unswitch[switched_func: SwitchedFunction](dynamic_switch: Bool) raises:
    """Performs a functional unswitch transformation.

    Unswitch is a simple pattern that is similar idea to loop unswitching
    pass but extended to functional patterns. The pattern facilitates the
    following code transformation that reduces the number of branches in the
    generated code

    Before:

        for i in range(...)
            if i < xxx:
                ...

    After:

        if i < ...
            for i in range(...)
                ...
        else
            for i in range(...)
                if i < xxx:
                    ...

    This unswitch function generalizes that pattern with the help of meta
    parameters and can be used to perform both loop unswitching and other
    tile predicate lifting like in simd and amx.

    TODO: Generalize to support multiple predicates.
    TODO: Once nested lambdas compose well should make unswitch compose with
    tile in an easy way.

    Parameters:
        switched_func: The function containing the inner loop logic that can be
          unswitched.

    Args:
        dynamic_switch: The dynamic condition that enables the unswitched code
          path.

    Raises:
        If the operation fails.
    """
    if dynamic_switch:
        switched_func[True]()
    else:
        switched_func[False]()


@always_inline
fn unswitch[
    switched_func: fn[sw: Bool]() capturing[_] -> None
](dynamic_switch: Bool):
    """Performs a functional unswitch transformation.

    Unswitch is a simple pattern that is similar idea to loop unswitching
    pass but extended to functional patterns. The pattern facilitates the
    following code transformation that reduces the number of branches in the
    generated code

    Before:

        for i in range(...)
            if i < xxx:
                ...

    After:

        if i < ...
            for i in range(...)
                ...
        else
            for i in range(...)
                if i < xxx:
                    ...

    This unswitch function generalizes that pattern with the help of meta
    parameters and can be used to perform both loop unswitching and other
    tile predicate lifting like in simd and amx.

    TODO: Generalize to support multiple predicates.
    TODO: Once nested lambdas compose well should make unswitch compose with
    tile in an easy way.

    Parameters:
        switched_func: The function containing the inner loop logic that can be
          unswitched.

    Args:
        dynamic_switch: The dynamic condition that enables the unswitched code
          path.
    """

    if dynamic_switch:
        switched_func[True]()
    else:
        switched_func[False]()


@always_inline
fn unswitch[
    switched_func: SwitchedFunction2
](dynamic_switch_a: Bool, dynamic_switch_b: Bool):
    """Performs a functional 2-predicates unswitch transformation.

    Parameters:
        switched_func: The function containing the inner loop logic that has 2
          predicates which can be unswitched.

    Args:
        dynamic_switch_a: The first dynamic condition that enables the outer
          unswitched code path.
        dynamic_switch_b: The second dynamic condition that enables the inner
          unswitched code path.
    """
    # TODO: This could be a lot easier to write once parameter names can be
    #  removed.
    if dynamic_switch_a:

        @always_inline
        @parameter
        fn switched_a_true[static_switch: Bool]():
            switched_func[True, static_switch]()

        unswitch[switched_a_true](dynamic_switch_b)
    else:

        @always_inline
        @parameter
        fn switched_a_false[static_switch: Bool]():
            switched_func[False, static_switch]()

        unswitch[switched_a_false](dynamic_switch_b)


# ===-----------------------------------------------------------------------===#
# TileWithUnswitch
# ===-----------------------------------------------------------------------===#

comptime Static1DTileUnswitchUnitFunc = fn[width: Int, sw: Bool](
    Int, Int
) capturing[_] -> None
"""Signature of a tiled function with static tile size and unswitch flag.

The function takes a static tile size parameter and offset arguments,
i.e. `func[tile_size: Int](offset: Int)`.
"""

comptime Static1DTileUnitFuncWithFlag = fn[width: Int, flag: Bool](
    Int
) capturing[_] -> None
"""Signature of a tiled function with a static tile size, offset, and flag."""


@always_inline("nodebug")
fn tile_and_unswitch[
    workgroup_function: Static1DTileUnswitchUnitFunc,
    tile_size_list: List[Int],
](offset: Int, upperbound: Int):
    """Performs time and unswitch functional transformation.

    A variant of static tile given a workgroup function that can be unswitched.
    This generator is a fused version of tile and unswitch, where the static
    unswitch is true throughout the "inner" portion of the workload and is
    false only on the residue tile.

    Parameters:
        workgroup_function: Workgroup function that processes one tile of
          workload.
        tile_size_list: List of tile sizes to launch work.

    Args:
        offset: The initial index to start the work from.
        upperbound: The runtime upperbound that the work function should not
          exceed.
    """

    # Initialize where to start on the overall work load.
    var current_offset = offset
    var remaining = upperbound - offset

    comptime for tile_size in tile_size_list:
        # Process work with the tile size until there's not enough remaining work
        #  to fit in a tile.
        while remaining >= tile_size:
            workgroup_function[tile_size, True](current_offset, upperbound)
            current_offset += tile_size
            remaining -= tile_size

    # Use the last tile size to process the residue.
    if remaining > 0:
        workgroup_function[tile_size_list[len(tile_size_list) - 1], False](
            current_offset, upperbound
        )


comptime Dynamic1DTileUnswitchUnitFunc = fn[sw: Bool](Int, Int, Int) capturing[
    _
] -> None
"""Signature of a dynamic tiled unswitch unit function."""


@always_inline
fn tile_and_unswitch[
    workgroup_function: Dynamic1DTileUnswitchUnitFunc,
](offset: Int, upperbound: Int, *tile_size_list: Int):
    """Performs time and unswitch functional transformation.

    A variant of dynamic tile given a workgroup function that can be
    unswitched. This generator is a fused version of tile and unswitch, where
    the static unswitch is true throughout the "inner" portion of the workload
    and is false only on the residue tile.

    Parameters:
        workgroup_function: Workgroup function that processes one tile of
          workload.

    Args:
        offset: The initial index to start the work from.
        upperbound: The runtime upperbound that the work function should not exceed.
        tile_size_list: List of tile sizes to launch work.
    """

    # Initialize where to start on the overall work load.
    var current_offset: Int = offset

    for tile_size in tile_size_list:
        # Process work with the tile size until there's not enough remaining work
        #  to fit in a tile.
        while current_offset <= upperbound - tile_size:
            workgroup_function[True](current_offset, upperbound, tile_size)
            current_offset += tile_size

    # Use the last tile size to process the residue.
    if current_offset < upperbound:
        workgroup_function[False](
            current_offset,
            upperbound,
            tile_size_list[len(tile_size_list) - 1],
        )


@always_inline
fn tile_middle_unswitch_boundaries[
    work_fn: Static1DTileUnitFuncWithFlag,
    middle_tile_sizes: List[Int],
    left_tile_size: Int = 1,  # No tiling by default.
    right_tile_size: Int = 1,  # No tiling by default.
](
    left_boundary_start: Int,
    left_boundary_end: Int,
    right_boundary_start: Int,
    right_boundary_end: Int,
):
    """Divides 1d iteration space into three parts and tiles them with different
    steps.

    The 1d iteration space is divided into:
        1. [left_boundary_start, left_boundary_end), effected by left boundary.
        2. [left_boundary_end, right_boundary_start), not effected by any boundary.
        3. [right_boundary_start, right_boundary_end), effected by right boundary.

    work_fn's switch is true for the left and right boundaries, implying boundary
    conditions like padding in convolution. The middle part is tiled with static
    tile sizes with the switch as false.

    Parameters:
        work_fn: Work function that processes one tile of workload.
        middle_tile_sizes: List of tile sizes for the middle part.
        left_tile_size: Tile size for the left boundary region.
        right_tile_size: Tile size for the right boundary region.

    Args:
        left_boundary_start: Start index of the left boundary.
        left_boundary_end: End index of the left boundary.
        right_boundary_start: Start index of the right boundary.
        right_boundary_end: End index of the right boundary.

    `middle_tile_sizes` should be in descending order for optimal performance.
    (Larger tile size appeared later in the list fails the while-loop.)
    """

    var offset = left_boundary_start

    # Handle the edge case where filter window is so large that every input
    # point is effected by padding.
    var min_boundary_end = min(left_boundary_end, right_boundary_end)

    # Left boundary region.
    while offset < min_boundary_end:
        work_fn[left_tile_size, True](offset)
        offset += left_tile_size

    # Middle
    comptime for tile_size in middle_tile_sizes:
        while offset <= right_boundary_start - tile_size:
            work_fn[tile_size, False](offset)
            offset += tile_size

    # Right boundary region.
    while offset < right_boundary_end:
        work_fn[right_tile_size, True](offset)
        offset += right_tile_size


comptime Static1DTileUnitFuncWithFlags = fn[
    width: Int, left_flag: Bool, right_flag: Bool
](Int) capturing[_] -> None
"""Signature of a tiled function with left and right boundary flags."""


@always_inline
fn tile_middle_unswitch_boundaries[
    work_fn: Static1DTileUnitFuncWithFlags,
    tile_size: Int,
    size: Int,
]():
    """Tile 1d iteration space with boundary conditions at both ends.

    This generator is primarily for convolution with static shapes. `work_fn`'s
    flags hints the function to handle padding at the boundary. The size is the
    static output row size, i.e., WO dimension.

    Parameters:
        work_fn: Work function that updates one tile. It has two flags for
            left and right boundaries, respectively.
        tile_size: 1D Tile size.
        size: Iteration range is [0, size).
    """

    # Tile size covers the entire range, e.g., using 14x2 register tile for
    # 14x14 image. Both sides of the tile has boundary conditions.
    comptime if size <= tile_size:
        work_fn[size, True, True](0)
    else:
        # Set bounds of tile sizes on boundaries. E.g. for 7x7 image and
        # tile_size = 6, it's better to use tile_sizes 4 and 3 than using
        # 6 and 1 since the it's tricky to handle padding with very small
        # tile size.
        comptime tile_size_lbound = min(tile_size, size // 2)
        comptime tile_size_rbound = min(tile_size, size - size // 2)

        var offset = 0

        # left boundary
        work_fn[tile_size_lbound, True, False](offset)

        # middle
        @always_inline
        @parameter
        fn update_middle[_tile_size: Int](_offset: Int):
            work_fn[_tile_size, False, False](_offset)

        comptime num_middle_points = size - tile_size_lbound - tile_size_rbound
        comptime remainder = num_middle_points % tile_size
        # `tile` can't handle zero tile size.
        comptime tile_size_remainder = remainder if remainder > 0 else 1

        tile[update_middle, [tile_size, tile_size_remainder]](
            tile_size_lbound, size - tile_size_rbound
        )

        # right boundary
        work_fn[tile_size_rbound, False, True](size - tile_size_rbound)


# ===-----------------------------------------------------------------------===#
# Utilities
# ===-----------------------------------------------------------------------===#


@always_inline
fn _get_num_workers(problem_size: Int, grain_size: Int = 32768) -> Int:
    """Returns a number of workers to run in parallel for given problem_size,
    accounting for the available worker threads of the current runtime.

    Args:
        problem_size: The number of parallel tasks.
        grain_size: Minimum number of elements to warrant an additional thread.

    Returns:
        The number of workers to run in parallel.
    """
    # default grain_size copied from https://github.com/pytorch/pytorch/blob/20dfce591ce88bc957ffcd0c8dc7d5f7611a4a3b/aten/src/ATen/TensorIterator.h#L86
    # Ensure at least one worker is always returned to avoid division by zero.
    return max(1, min(parallelism_level(), ceildiv(problem_size, grain_size)))


@always_inline
fn _get_start_indices_of_nth_subvolume[
    rank: Int, //, subvolume_rank: Int = 1
](n: Int, shape: IndexList[rank, ...], out res: type_of(shape)):
    """Converts a flat index into the starting ND indices of the nth subvolume
    with rank `subvolume_rank`.

    For example:
        - `_get_start_indices_of_nth_subvolume[0](n, shape)` will return
        the starting indices of the nth element in shape.
        - `_get_start_indices_of_nth_subvolume[1](n, shape)` will return
        the starting indices of the nth row in shape.
        - `_get_start_indices_of_nth_subvolume[2](n, shape)` will return
        the starting indices of the nth horizontal slice in shape.

    The ND indices will iterate from right to left. I.E

        shape = (20, 5, 2, N)
        _get_start_indices_of_nth_subvolume[1](1, shape) = (0, 0, 1, 0)
        _get_start_indices_of_nth_subvolume[1](5, shape) = (0, 2, 1, 0)
        _get_start_indices_of_nth_subvolume[1](50, shape) = (5, 0, 0, 0)
        _get_start_indices_of_nth_subvolume[1](56, shape) = (5, 1, 1, 0)

    Parameters:
        rank: The rank of the ND index.
        subvolume_rank: The rank of the subvolume under consideration.

    Args:
        n: The flat index to convert (the nth subvolume to retrieve).
        shape: The shape of the ND space we are converting into.

    Returns:
        Constructed ND-index.
    """

    comptime assert (
        subvolume_rank <= rank
    ), "subvolume rank cannot be greater than indices rank"
    comptime assert subvolume_rank >= 0, "subvolume rank must be non-negative"

    # fast impls for common cases
    comptime if rank == 2 and subvolume_rank == 1:
        return {n, 0}

    comptime if rank - 1 == subvolume_rank:
        res = {0}
        res[0] = n
        return

    comptime if rank == subvolume_rank:
        return {0}

    res = {}
    var curr_index = n

    comptime for i in reversed(range(rank - subvolume_rank)):
        res[i] = curr_index._positive_rem(shape[i])
        curr_index = curr_index / shape[i]


# TODO(KERN-637) - optimize this algorithm for UInt rather than delegating
# to the Int overload.
@always_inline
fn _get_start_indices_of_nth_subvolume_uint[
    rank: Int,
    //,
    subvolume_rank: UInt = 1,
](n: UInt, shape: IndexList[rank, ...]) -> type_of(shape):
    """Converts a flat index into the starting ND indices of the nth subvolume
    with rank `subvolume_rank`.

    For example:
        - `_get_start_indices_of_nth_subvolume[0](n, shape)` will return
        the starting indices of the nth element in shape.
        - `_get_start_indices_of_nth_subvolume[1](n, shape)` will return
        the starting indices of the nth row in shape.
        - `_get_start_indices_of_nth_subvolume[2](n, shape)` will return
        the starting indices of the nth horizontal slice in shape.

    The ND indices will iterate from right to left. I.E

        shape = (20, 5, 2, N)
        _get_start_indices_of_nth_subvolume[1](1, shape) = (0, 0, 1, 0)
        _get_start_indices_of_nth_subvolume[1](5, shape) = (0, 2, 1, 0)
        _get_start_indices_of_nth_subvolume[1](50, shape) = (5, 0, 0, 0)
        _get_start_indices_of_nth_subvolume[1](56, shape) = (5, 1, 1, 0)

    Parameters:
        rank: The rank of the ND index.
        subvolume_rank: The rank of the subvolume under consideration.

    Args:
        n: The flat index to convert (the nth subvolume to retrieve).
        shape: The shape of the ND space we are converting into.

    Returns:
        Constructed ND-index.
    """
    return _get_start_indices_of_nth_subvolume[Int(subvolume_rank)](
        Int(n), shape
    )


# ===-----------------------------------------------------------------------===#
# Elementwise
# ===-----------------------------------------------------------------------===#


@always_inline
fn elementwise[
    func: fn[width: Int, rank: Int, alignment: Int = 1](
        IndexList[rank]
    ) capturing[_] -> None,
    simd_width: Int,
    *,
    use_blocking_impl: Bool = False,
    target: StaticString = "cpu",
    _trace_description: StaticString = "",
](shape: Int) raises:
    """Executes `func[width, rank](indices)`, possibly as sub-tasks, for a
    suitable combination of width and indices so as to cover shape. Returns when
    all sub-tasks have completed.

    Parameters:
        func: The body function.
        simd_width: The SIMD vector width to use.
        use_blocking_impl: Do not invoke the function using asynchronous calls.
        target: The target to run on.
        _trace_description: Description of the trace.

    Args:
        shape: The shape of the buffer.

    Raises:
        If the operation fails.
    """

    elementwise[
        func,
        simd_width=simd_width,
        use_blocking_impl=use_blocking_impl,
        target=target,
        _trace_description=_trace_description,
    ](Index(shape))


@always_inline
fn elementwise[
    rank: Int,
    //,
    func: fn[width: Int, rank: Int, alignment: Int = 1](
        IndexList[rank]
    ) capturing[_] -> None,
    simd_width: Int,
    *,
    use_blocking_impl: Bool = False,
    target: StaticString = "cpu",
    _trace_description: StaticString = "",
](shape: IndexList[rank, ...]) raises:
    """Executes `func[width, rank](indices)`, possibly as sub-tasks, for a
    suitable combination of width and indices so as to cover shape. Returns when
    all sub-tasks have completed.

    Parameters:
        rank: The rank of the buffer.
        func: The body function.
        simd_width: The SIMD vector width to use.
        use_blocking_impl: Do not invoke the function using asynchronous calls.
        target: The target to run on.
        _trace_description: Description of the trace.

    Args:
        shape: The shape of the buffer.

    Raises:
        If the operation fails.
    """

    comptime assert is_cpu[target](), (
        "the target must be CPU use the elementwise which takes the"
        " DeviceContext to be able to use the GPU version"
    )

    _elementwise_impl_cpu[
        func, simd_width, use_blocking_impl=use_blocking_impl
    ](shape)


@always_inline
fn elementwise[
    func: fn[width: Int, rank: Int, alignment: Int = 1](
        IndexList[rank]
    ) capturing[_] -> None,
    simd_width: Int,
    *,
    use_blocking_impl: Bool = False,
    target: StaticString = "cpu",
    _trace_description: StaticString = "",
](shape: Int, context: DeviceContext) raises:
    """Executes `func[width, rank](indices)`, possibly as sub-tasks, for a
    suitable combination of width and indices so as to cover shape. Returns when
    all sub-tasks have completed.

    Parameters:
        func: The body function.
        simd_width: The SIMD vector width to use.
        use_blocking_impl: Do not invoke the function using asynchronous calls.
        target: The target to run on.
        _trace_description: Description of the trace.

    Args:
        shape: The shape of the buffer.
        context: The device context to use.

    Raises:
        If the operation fails.
    """

    elementwise[
        func,
        simd_width=simd_width,
        use_blocking_impl=use_blocking_impl,
        target=target,
    ](Index(shape), context)


@always_inline
fn elementwise[
    rank: Int,
    //,
    func: fn[width: Int, rank: Int, alignment: Int = 1](
        IndexList[rank]
    ) capturing[_] -> None,
    simd_width: Int,
    *,
    use_blocking_impl: Bool = False,
    target: StaticString = "cpu",
    _trace_description: StaticString = "",
](shape: IndexList[rank, ...], context: DeviceContext) raises:
    """Executes `func[width, rank](indices)`, possibly as sub-tasks, for a
    suitable combination of width and indices so as to cover shape. Returns when
    all sub-tasks have completed.

    Parameters:
        rank: The rank of the buffer.
        func: The body function.
        simd_width: The SIMD vector width to use.
        use_blocking_impl: Do not invoke the function using asynchronous calls.
        target: The target to run on.
        _trace_description: Description of the trace.

    Args:
        shape: The shape of the buffer.
        context: The device context to use.

    Raises:
        If the operation fails.
    """

    _elementwise_impl[
        func, simd_width, use_blocking_impl=use_blocking_impl, target=target
    ](shape, context)


@always_inline
fn elementwise[
    rank: Int,
    //,
    func: fn[width: Int, rank: Int, alignment: Int = 1](
        IndexList[rank]
    ) capturing[_] -> None,
    simd_width: Int,
    *,
    use_blocking_impl: Bool = False,
    target: StaticString = "cpu",
    _trace_description: StaticString = "",
](shape: IndexList[rank, ...], context: DeviceContextPtr) raises:
    """Executes `func[width, rank](indices)`, possibly as sub-tasks, for a
    suitable combination of width and indices so as to cover shape. Returns when
    all sub-tasks have completed.

    Parameters:
        rank: The rank of the buffer.
        func: The body function.
        simd_width: The SIMD vector width to use.
        use_blocking_impl: Do not invoke the function using asynchronous calls.
        target: The target to run on.
        _trace_description: Description of the trace.

    Args:
        shape: The shape of the buffer.
        context: The device context to use.

    Raises:
        If the operation fails.
    """

    @always_inline
    @parameter
    fn description_fn() -> String:
        var shape_str = trace_arg("shape", shape)
        var vector_width_str = String(t"vector_width={simd_width}")

        return ";".join(Span([shape_str, vector_width_str]))

    # Intern the kind string as a static string so we don't allocate.
    comptime d = _trace_description
    comptime desc = String(t"({d})") if d else ""
    comptime kind = get_static_string["elementwise", desc]()

    with Trace[TraceLevel.OP, target=target](
        kind,
        Trace[TraceLevel.OP]._get_detail_str[description_fn](),
        task_id=get_safe_task_id(context),
    ):
        comptime if is_gpu[target]():
            _elementwise_impl_gpu[func, simd_width = UInt(simd_width)](
                shape, context[]
            )
        else:
            _elementwise_impl_cpu[
                func, simd_width, use_blocking_impl=use_blocking_impl
            ](shape)


@always_inline
fn _elementwise_impl[
    rank: Int,
    //,
    func: fn[width: Int, rank: Int, alignment: Int = 1](
        IndexList[rank]
    ) capturing[_] -> None,
    simd_width: Int,
    /,
    *,
    use_blocking_impl: Bool = False,
    target: StaticString = "cpu",
](shape: IndexList[rank, ...], context: DeviceContext) raises:
    comptime if is_cpu[target]():
        _elementwise_impl_cpu[
            func, simd_width, use_blocking_impl=use_blocking_impl
        ](shape)
    else:
        _elementwise_impl_gpu[func, UInt(simd_width)](
            shape,
            context,
        )


@always_inline
fn _elementwise_impl_cpu[
    rank: Int,
    //,
    func: fn[width: Int, rank: Int, alignment: Int = 1](
        IndexList[rank]
    ) capturing[_] -> None,
    simd_width: Int,
    /,
    *,
    use_blocking_impl: Bool = False,
](shape: IndexList[rank, ...]):
    comptime impl = _elementwise_impl_cpu_1d if rank == 1 else _elementwise_impl_cpu_nd
    impl[func, simd_width, use_blocking_impl=use_blocking_impl](shape)


@always_inline
fn _elementwise_impl_cpu_1d[
    rank: Int,
    //,
    func: fn[width: Int, rank: Int, alignment: Int = 1](
        IndexList[rank]
    ) capturing[_] -> None,
    simd_width: Int,
    *,
    use_blocking_impl: Bool,
](shape: IndexList[rank, ...]):
    """Executes `func[width, rank](indices)`, possibly using sub-tasks, for a
    suitable combination of width and indices so as to cover shape. Returns when
    all sub-tasks have completed.

    Parameters:
        rank: The rank of the buffer.
        func: The body function.
        simd_width: The SIMD vector width to use.
        use_blocking_impl: If true the functions execute without sub-tasks.

    Args:
        shape: The shape of the buffer.
    """
    comptime assert rank == 1, "Specialization for 1D"

    comptime unroll_factor = 8  # TODO: Comeup with a cost heuristic.

    var problem_size = shape.flattened_length()

    comptime if use_blocking_impl:

        @always_inline
        fn blocking_task_fun[simd_width: Int](idx: Int) unified {read}:
            func[simd_width, rank](IndexList[rank](idx))

        vectorize[simd_width, unroll_factor=unroll_factor](
            problem_size, blocking_task_fun
        )
        return

    var num_workers = _get_num_workers(problem_size)
    var chunk_size = ceildiv(problem_size, num_workers)

    @always_inline
    @parameter
    fn task_func(i: Int):
        var start_offset = i * chunk_size
        var end_offset = min((i + 1) * chunk_size, problem_size)
        var len = end_offset - start_offset

        @always_inline
        fn func_wrapper[simd_width: Int](idx: Int) unified {read start_offset}:
            var offset = start_offset + idx
            func[simd_width, rank](IndexList[rank](offset))

        vectorize[simd_width, unroll_factor=unroll_factor](len, func_wrapper)

    sync_parallelize[task_func](num_workers)


@always_inline
fn _elementwise_impl_cpu_nd[
    rank: Int,
    //,
    func: fn[width: Int, rank: Int, alignment: Int = 1](
        IndexList[rank]
    ) capturing[_] -> None,
    simd_width: Int,
    *,
    use_blocking_impl: Bool,
](shape: IndexList[rank, ...]):
    """Executes `func[width, rank](indices)`, possibly using sub-tasks, for a
    suitable combination of width and indices so as to cover shape. Returns
    when all sub-tasks have completed.

    Parameters:
        rank: The rank of the buffer.
        func: The body function.
        simd_width: The SIMD vector width to use.
        use_blocking_impl: If true this is a blocking op.

    Args:
        shape: The shape of the buffer.
    """
    comptime assert rank > 1, "Specialization for ND where N > 1"

    # If we know we won't do any work, return early
    if shape[rank - 1] == 0:
        return

    comptime unroll_factor = 8  # TODO: Comeup with a cost heuristic.

    # Strategy: we parallelize over all dimensions except the innermost and
    # vectorize over the innermost dimension. We unroll the innermost dimension
    # by a factor of unroll_factor.

    # Compute the number of workers to allocate based on ALL work, not just
    # the dimensions we split across.
    var total_size: Int = shape.flattened_length()

    comptime if use_blocking_impl:

        @always_inline
        @parameter
        fn blocking_task_fn(i: Int):
            var indices = _get_start_indices_of_nth_subvolume(i, shape)

            @always_inline
            fn func_wrapper[simd_width: Int](idx: Int) unified {mut indices}:
                # The inner most dimension is vectorized, so we set it
                # to the index offset.
                indices[rank - 1] = idx
                func[simd_width, rank](indices.canonicalize())

            # We vectorize over the innermost dimension.
            vectorize[simd_width, unroll_factor=unroll_factor](
                shape[rank - 1], func_wrapper
            )

        map[blocking_task_fn](total_size // shape[rank - 1])

        return

    var num_workers = _get_num_workers(total_size)
    var parallelism_size = total_size // shape[rank - 1]
    var chunk_size = ceildiv(parallelism_size, num_workers)

    @always_inline
    @parameter
    fn task_func(i: Int):
        var start_parallel_offset = i * chunk_size
        var end_parallel_offset = min((i + 1) * chunk_size, parallelism_size)

        var len = end_parallel_offset - start_parallel_offset
        if len <= 0:
            return

        for parallel_offset in range(
            start_parallel_offset, end_parallel_offset
        ):
            var indices = _get_start_indices_of_nth_subvolume(
                parallel_offset, shape
            )

            @always_inline
            fn func_wrapper[simd_width: Int](idx: Int) unified {mut indices}:
                # The inner most dimension is vectorized, so we set it
                # to the index offset.
                indices[rank - 1] = idx
                func[simd_width, rank](indices.canonicalize())

            # We vectorize over the innermost dimension.
            vectorize[simd_width, unroll_factor=unroll_factor](
                shape[rank - 1], func_wrapper
            )

    sync_parallelize[task_func](num_workers)


@always_inline
fn _elementwise_impl_gpu[
    rank: Int,
    //,
    func: fn[width: Int, rank: Int, alignment: Int = 1](
        IndexList[rank]
    ) capturing[_] -> None,
    simd_width: UInt,
](shape: IndexList[rank, ...], ctx: DeviceContext) raises:
    """Executes `func[width, rank](indices)` as sub-tasks for a suitable
    combination of width and indices so as to cover shape on the GPU.

    Parameters:
        rank: The rank of the buffer.
        func: The body function.
        simd_width: The SIMD vector width to use.

    Args:
        shape: The shape of the buffer.
        ctx: The pointer to DeviceContext.
    """

    # optimized implementation inspired by https://archive.md/Tye9y#selection-1101.2-1151.3

    comptime hw_info = ctx.default_device_info

    comptime registers_per_thread = 255
    comptime num_waves = 32
    comptime registers_per_block = hw_info.max_registers_per_block
    comptime sm_count = UInt(hw_info.sm_count)
    comptime threads_per_multiprocessor = UInt(
        hw_info.threads_per_multiprocessor
    )

    comptime assert (
        sm_count > 0 and threads_per_multiprocessor > 0
    ), "the sm_count and thread_count must be known"

    # split between packed and tail regions of input
    var length = UInt(shape.flattened_length())
    var num_packed_elems = length // simd_width
    var unpacked_tail_length = length % simd_width
    var packed_region_length = length - unpacked_tail_length

    if length == 0:
        return

    comptime block_size_unrounded = registers_per_block // registers_per_thread

    # when testing other elementwise kernels, they appear to also use 128 as the block size on blackwell specifically
    comptime block_size = 128 if ctx.default_device_info == B200 else block_size_unrounded - (
        block_size_unrounded % 2
    )

    var num_blocks = clamp(
        ceildiv(num_packed_elems, UInt(block_size)),
        1,
        sm_count * threads_per_multiprocessor // UInt(block_size) * num_waves,
    )

    @__copy_capture(
        num_packed_elems, unpacked_tail_length, packed_region_length
    )
    @parameter
    @__llvm_metadata(
        MAX_THREADS_PER_BLOCK_METADATA=StaticTuple[Int32, 1](Int32(block_size))
    )
    fn _elementwise_gpu_kernel[*, block_size: UInt, handle_uneven_simd: Bool]():
        # process the packed region
        var tid = thread_idx.x + block_size * block_idx.x

        comptime if PDLLevel() == PDLLevel.OVERLAP_AT_BEGINNING:
            launch_dependent_grids()

        comptime if PDLLevel() > PDLLevel.OFF:
            wait_on_dependent_grids()

        for idx in range(
            tid,
            num_packed_elems,
            block_size * grid_dim.x,
        ):
            var start_indices = _get_start_indices_of_nth_subvolume_uint[0](
                idx * simd_width, shape
            )

            comptime if handle_uneven_simd:
                if start_indices[rank - 1] + Int(simd_width) >= shape[rank - 1]:
                    comptime for off in range(Int(simd_width)):
                        func[1, rank](
                            _get_start_indices_of_nth_subvolume_uint[0](
                                idx * simd_width + UInt(off),
                                shape,
                            ).canonicalize()
                        )
                else:
                    func[Int(simd_width), rank](start_indices.canonicalize())
            else:
                # The alignment is by number of elements, which will be converted to
                # number of bytes by graph compiler.
                func[Int(simd_width), rank, Int(simd_width)](
                    start_indices.canonicalize()
                )

        # process the tail region
        if tid < unpacked_tail_length:
            var index_tup = _get_start_indices_of_nth_subvolume_uint[0](
                packed_region_length + tid, shape
            ).canonicalize()
            func[1, rank](index_tup)

        comptime if PDLLevel() == PDLLevel.OVERLAP_AT_END:
            launch_dependent_grids()

    if shape[rank - 1] % Int(simd_width) == 0:
        comptime kernel = _elementwise_gpu_kernel[
            block_size = UInt(block_size), handle_uneven_simd=False
        ]
        ctx.enqueue_function[kernel, kernel](
            grid_dim=Int(num_blocks),
            block_dim=block_size,
            attributes=pdl_launch_attributes(),
        )
    else:
        comptime kernel = _elementwise_gpu_kernel[
            block_size = UInt(block_size), handle_uneven_simd=True
        ]
        ctx.enqueue_function[kernel, kernel](
            grid_dim=Int(num_blocks),
            block_dim=block_size,
            attributes=pdl_launch_attributes(),
        )


# ===-----------------------------------------------------------------------===#
# parallelize_over_rows
# ===-----------------------------------------------------------------------===#


fn parallelize_over_rows[
    func: fn(Int, Int) capturing[_] -> None
](shape: IndexList, axis: Int, grain_size: Int):
    """Parallelize func over non-axis dims of shape.

    Parameters:
        func: Function to call on range of rows.

    Args:
        shape: Shape to parallelize over.
        axis: Rows are slices along the axis dimension of shape.
        grain_size: The minimum number of elements to warrant using an additional thread.
    """
    # If we know we will have no work, return early
    if shape[axis] == 0:
        return

    var total_size = shape.flattened_length()
    var num_rows = total_size // shape[axis]

    var num_workers = min(
        num_rows,
        _get_num_workers(total_size, grain_size),
    )
    var chunk_size = ceildiv(num_rows, num_workers)

    @always_inline
    @parameter
    fn task_func(task_id: Int):
        var start_row = task_id * chunk_size
        var end_row = min((task_id + 1) * chunk_size, num_rows)

        func(start_row, end_row)

    sync_parallelize[task_func](num_workers)


# ===-----------------------------------------------------------------------===#
# stencil
# ===-----------------------------------------------------------------------===#

comptime stencil = _stencil_impl_cpu
"""CPU implementation of stencil computation."""

comptime stencil_gpu = _stencil_impl_gpu
"""GPU implementation of stencil computation."""


fn _stencil_impl_cpu[
    shape_element_type: DType,
    input_shape_element_type: DType,
    //,
    rank: Int,
    stencil_rank: Int,
    stencil_axis: IndexList[stencil_rank, ...],
    simd_width: Int,
    dtype: DType,
    map_fn: fn(IndexList[stencil_rank, ...]) capturing[_] -> Tuple[
        IndexList[stencil_rank],
        IndexList[stencil_rank],
    ],
    map_strides: fn(dim: Int) capturing[_] -> Int,
    load_fn: fn[simd_width: Int, dtype: DType](IndexList[rank, ...]) capturing[
        _
    ] -> SIMD[dtype, simd_width],
    compute_init_fn: fn[simd_width: Int]() capturing[_] -> SIMD[
        dtype, simd_width
    ],
    compute_fn: fn[simd_width: Int](
        IndexList[rank, ...],
        SIMD[dtype, simd_width],
        SIMD[dtype, simd_width],
    ) capturing[_] -> SIMD[dtype, simd_width],
    compute_finalize_fn: fn[simd_width: Int](
        IndexList[rank, ...], SIMD[dtype, simd_width]
    ) capturing[_] -> None,
](
    shape: IndexList[rank, element_type=shape_element_type],
    input_shape: IndexList[rank, element_type=input_shape_element_type],
):
    """Computes stencil operation in parallel.

    Computes output as a function that processes input stencils, stencils are
    computed as a continuous region for each output point that is determined
    by map_fn : map_fn(y) -> lower_bound, upper_bound. The boundary conditions
    for regions that fail out of the input domain are handled by load_fn.


    Parameters:
        shape_element_type: The element dtype of the shape.
        input_shape_element_type: The element dtype of the input shape.
        rank: Input and output domain rank.
        stencil_rank: Rank of stencil subdomain slice.
        stencil_axis: Stencil subdomain axes.
        simd_width: The SIMD vector width to use.
        dtype: The input and output data dtype.
        map_fn: A function that a point in the output domain to the input co-domain.
        map_strides: A function that returns the stride for the dim.
        load_fn: A function that loads a vector of simd_width from input.
        compute_init_fn: A function that initializes vector compute over the stencil.
        compute_fn: A function the process the value computed for each point in the stencil.
        compute_finalize_fn: A function that finalizes the computation of a point in the output domain given a stencil.

    Args:
        shape: The shape of the output buffer.
        input_shape: The shape of the input buffer.
    """
    comptime assert rank == 4, "Only stencil of rank-4 supported"
    comptime assert (
        stencil_axis[0] == 1 and stencil_axis[1] == 2
    ), "Only stencil spatial axes [1, 2] are supported"

    # If we know we will have no work, return early
    if shape[rank - 1] == 0:
        return

    var total_size = shape.flattened_length()

    var num_workers = _get_num_workers(total_size)
    var parallelism_size = total_size // shape[rank - 1]
    var chunk_size = ceildiv(parallelism_size, num_workers)

    comptime unroll_factor = 8  # TODO: Comeup with a cost heuristic.

    @always_inline
    @parameter
    fn task_func(i: Int):
        var start_parallel_offset = i * chunk_size
        var end_parallel_offset = min((i + 1) * chunk_size, parallelism_size)

        var len = end_parallel_offset - start_parallel_offset
        if len <= 0:
            return

        for parallel_offset in range(
            start_parallel_offset, end_parallel_offset
        ):
            var indices = _get_start_indices_of_nth_subvolume(
                parallel_offset, shape
            )

            @always_inline
            fn func_wrapper[
                simd_width: Int
            ](idx: Int) unified {mut indices, read input_shape}:
                indices[rank - 1] = idx
                var stencil_indices = IndexList[
                    stencil_rank, element_type = stencil_axis.element_type
                ](indices[stencil_axis[0]], indices[stencil_axis[1]])
                var bounds = map_fn(stencil_indices)
                var lower_bound = bounds[0]
                var upper_bound = bounds[1]
                var step_i = map_strides(0)
                var step_j = map_strides(1)
                var result = compute_init_fn[simd_width]()
                var input_height = input_shape[1]
                var input_width = input_shape[2]

                # In the below loops, each part corresponds to either a padded
                # side (A, B, C, D), or the main part of the input buffer (X)
                # For the padded parts we do not need to load pad value
                # (e.g., neginf for max_pool, or 0 for avg_pool) because result
                # is initialized by compute_init_fn() to the appropriate value
                # (e.g., neginf or 0 in the max_pool/avg_pool cases).
                # Loading and calculating the result for boundary locations
                # (A-D) as in:
                #   var val = pad_value
                #   result = compute_fn[simd_width](point_idx, result, val)
                # would therefore make no difference to not doing it at
                # all.

                # NOTE: The above works when padding is constant and invariant
                # to input coordinates.

                #  AAAAAAAA
                #  AAAAAAAA
                #  BBXXXXDD
                #  BBXXXXDD
                #  BBXXXXDD
                #  BBXXXXDD
                #  CCCCCCCC
                #  CCCCCCCC

                # Calculation for lower_bound below takes into account dilation
                # across rows dimension.
                # Will be the zero if dilation is 1, or the closest point >0 if
                # dilation > 1
                if lower_bound[0] < 0:
                    var mul_i = ceildiv(-lower_bound[0], step_i)
                    lower_bound[0] = lower_bound[0] + mul_i * step_i
                if lower_bound[1] < 0:
                    var mul_j = ceildiv(-lower_bound[1], step_j)
                    lower_bound[1] = lower_bound[1] + mul_j * step_j

                # Part X (inner part)
                for i in range(
                    lower_bound[0],
                    min(input_height, upper_bound[0]),
                    step_i,
                ):
                    for j in range(
                        lower_bound[1],
                        min(input_width, upper_bound[1]),
                        step_j,
                    ):
                        var point_idx = IndexList[
                            rank, element_type=shape_element_type
                        ](indices[0], i, j, indices[3])

                        var val = load_fn[simd_width, dtype](point_idx)
                        result = compute_fn[simd_width](point_idx, result, val)

                compute_finalize_fn[simd_width](indices, result)

            vectorize[simd_width, unroll_factor=unroll_factor](
                shape[rank - 1], func_wrapper
            )

    sync_parallelize[task_func](num_workers)


fn _stencil_impl_gpu[
    shape_element_type: DType,
    input_shape_element_type: DType,
    //,
    rank: Int,
    stencil_rank: Int,
    stencil_axis: IndexList[stencil_rank, ...],
    simd_width: Int,
    dtype: DType,
    map_fn: fn(IndexList[stencil_rank, ...]) capturing[_] -> Tuple[
        IndexList[stencil_rank],
        IndexList[stencil_rank],
    ],
    map_strides: fn(dim: Int) capturing[_] -> Int,
    load_fn: fn[simd_width: Int, dtype: DType](IndexList[rank, ...]) capturing[
        _
    ] -> SIMD[dtype, simd_width],
    compute_init_fn: fn[simd_width: Int]() capturing[_] -> SIMD[
        dtype, simd_width
    ],
    compute_fn: fn[simd_width: Int](
        IndexList[rank, ...],
        SIMD[dtype, simd_width],
        SIMD[dtype, simd_width],
    ) capturing[_] -> SIMD[dtype, simd_width],
    compute_finalize_fn: fn[simd_width: Int](
        IndexList[rank, ...], SIMD[dtype, simd_width]
    ) capturing[_] -> None,
](
    ctx: DeviceContext,
    shape: IndexList[rank, element_type=shape_element_type],
    input_shape: IndexList[rank, element_type=input_shape_element_type],
) raises:
    """(Naive implementation) Computes stencil operation in parallel on GPU.

    Parameters:
        shape_element_type: The element dtype of the shape.
        input_shape_element_type: The element dtype of the input shape.
        rank: Input and output domain rank.
        stencil_rank: Rank of stencil subdomain slice.
        stencil_axis: Stencil subdomain axes.
        simd_width: The SIMD vector width to use.
        dtype: The input and output data dtype.
        map_fn: A function that a point in the output domain to the input co-domain.
        map_strides: A function that returns the stride for the dim.
        load_fn: A function that loads a vector of simd_width from input.
        compute_init_fn: A function that initializes vector compute over the stencil.
        compute_fn: A function the process the value computed for each point in the stencil.
        compute_finalize_fn: A function that finalizes the computation of a point in the output domain given a stencil.

    Args:
        ctx: The DeviceContext to use for GPU execution.
        shape: The shape of the output buffer.
        input_shape: The shape of the input buffer.
    """
    comptime assert rank == 4, "Only stencil of rank-4 supported"
    comptime assert (
        stencil_axis[0] == 1 and stencil_axis[1] == 2
    ), "Only stencil spatial axes [1, 2] are supported"

    # GPU kernel implementation
    @always_inline
    @parameter
    fn stencil_kernel():
        # Get thread indices
        var tid_x = thread_idx.x
        var tid_y = thread_idx.y
        var bid_x = block_idx.x
        var bid_y = block_idx.y
        var bid_z = block_idx.z

        # Calculate global indices
        var x = bid_x * block_dim.x + tid_x
        var y = bid_y * block_dim.y + tid_y

        # Calculate batch and channel from bid_z
        var batch_idx = bid_z // UInt(shape[3])
        var channel = bid_z % UInt(shape[3])

        # Early exit if outside bounds
        if x >= UInt(shape[2]) or y >= UInt(shape[1]):
            return

        # Create output point indices with computed batch and channel
        var indices = IndexList[rank, element_type=shape_element_type](
            Int(batch_idx), Int(y), Int(x), Int(channel)
        )

        # Process stencil for this point
        var stencil_indices = IndexList[
            stencil_rank, element_type = stencil_axis.element_type
        ](indices[stencil_axis[0]], indices[stencil_axis[1]])
        var bounds = map_fn(stencil_indices)
        var lower_bound = bounds[0]
        var upper_bound = bounds[1]
        var step_i = map_strides(0)
        var step_j = map_strides(1)
        var result = compute_init_fn[simd_width]()
        var input_height = input_shape[1]
        var input_width = input_shape[2]

        # Handle boundary conditions
        if lower_bound[0] < 0:
            var mul_i = ceildiv(-lower_bound[0], step_i)
            lower_bound[0] = lower_bound[0] + mul_i * step_i
        if lower_bound[1] < 0:
            var mul_j = ceildiv(-lower_bound[1], step_j)
            lower_bound[1] = lower_bound[1] + mul_j * step_j

        # Process stencil window
        for i in range(
            lower_bound[0],
            min(input_height, upper_bound[0]),
            step_i,
        ):
            for j in range(
                lower_bound[1],
                min(input_width, upper_bound[1]),
                step_j,
            ):
                var point_idx = IndexList[
                    rank, element_type=shape_element_type
                ](indices[0], i, j, indices[3])
                var val = load_fn[simd_width, dtype](point_idx)
                result = compute_fn[simd_width](point_idx, result, val)

        compute_finalize_fn[simd_width](indices, result)

    # Calculate grid and block dimensions
    var block_dim = (32, 32, 1)
    var grid_dim = (
        ceildiv(shape[2], block_dim[0]),  # width
        ceildiv(shape[1], block_dim[1]),  # height
        shape[0] * shape[3],  # batch_size * num_channels
    )

    # Compile and launch kernel
    ctx.enqueue_function[stencil_kernel, stencil_kernel](
        grid_dim=grid_dim, block_dim=block_dim
    )
