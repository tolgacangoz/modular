# ===----------------------------------------------------------------------=== #
# Copyright (c) 2026, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
"""OpenResponses API Pydantic schema definitions.

This module provides comprehensive Pydantic models for the OpenResponses API
specification, supporting request validation, response serialization, and
type-safe interactions with AI models.

The OpenResponses API is a standardized interface for AI model interactions
supporting text generation, tool calling, multi-turn conversations, and streaming.

Spec: https://www.openresponses.org/reference
"""

from __future__ import annotations

import base64
import json
import logging
import mimetypes
import time
from enum import Enum
from io import BytesIO
from typing import TYPE_CHECKING, Annotated, Any, Literal, Protocol
from urllib.parse import urlparse

import httpx
import numpy as np
import numpy.typing as npt
from max.interfaces.provider_options import (
    ImageProviderOptions,
    ProviderOptions,
)
from PIL import Image
from pydantic import BaseModel, ConfigDict, Field, model_validator

from .base import RequestID

logger = logging.getLogger(__name__)

if TYPE_CHECKING:
    from max.interfaces.generation import GenerationOutput

# ============================================================================
# Section 1: Enumerations
# ============================================================================


class ToolChoiceValueEnum(str, Enum):
    """Controls which (if any) tool is called by the model.

    - `none`: The model will not call any tool and instead generates a message.
    - `auto`: The model can pick between generating a message or calling one or
      more tools.
    - `required`: The model must call one or more tools.
    """

    none = "none"
    auto = "auto"
    required = "required"


class FunctionCallStatus(str, Enum):
    """The status of the function call.

    - `in_progress`: The function call is being generated.
    - `completed`: The function call generation is complete.
    - `incomplete`: The function call generation stopped before completion.
    """

    in_progress = "in_progress"
    completed = "completed"
    incomplete = "incomplete"


class MessageRole(str, Enum):
    """The role of the message author.

    - `user`: Messages from the end user.
    - `assistant`: Messages generated by the AI assistant.
    - `system`: System-level instructions that guide the assistant's behavior.
    - `developer`: Messages from the application developer.
    """

    user = "user"
    assistant = "assistant"
    system = "system"
    developer = "developer"


class MessageStatus(str, Enum):
    """The status of the message.

    - `in_progress`: The message is being generated.
    - `completed`: The message generation is complete.
    - `incomplete`: The message generation stopped before completion.
    """

    in_progress = "in_progress"
    completed = "completed"
    incomplete = "incomplete"


class ImageDetail(str, Enum):
    """Controls image processing detail level.

    - `low`: Low detail, faster processing.
    - `high`: High detail, more thorough analysis.
    - `auto`: Let the model decide based on context.
    """

    low = "low"
    high = "high"
    auto = "auto"


class ReasoningEffortEnum(str, Enum):
    """The effort level for extended reasoning.

    - `none`: No extended reasoning.
    - `low`: Minimal reasoning effort.
    - `medium`: Moderate reasoning effort.
    - `high`: Significant reasoning effort.
    - `xhigh`: Maximum reasoning effort.
    """

    none = "none"
    low = "low"
    medium = "medium"
    high = "high"
    xhigh = "xhigh"


class ReasoningSummaryEnum(str, Enum):
    """The level of detail for reasoning summaries.

    - `concise`: Brief summary of reasoning.
    - `detailed`: Comprehensive reasoning explanation.
    - `auto`: Let the model decide detail level.
    """

    concise = "concise"
    detailed = "detailed"
    auto = "auto"


class ServiceTierEnum(str, Enum):
    """The service tier for request processing.

    - `auto`: Automatically select appropriate tier.
    - `default`: Standard service tier.
    - `flex`: Flexible tier with variable performance.
    - `priority`: Prioritized processing.
    """

    auto = "auto"
    default = "default"
    flex = "flex"
    priority = "priority"


class TruncationEnum(str, Enum):
    """Controls message truncation behavior.

    - `auto`: Automatically truncate if needed.
    - `disabled`: Never truncate messages.
    """

    auto = "auto"
    disabled = "disabled"


class IncludeEnum(str, Enum):
    """Additional data to include in the response.

    - `reasoning.encrypted_content`: Include encrypted reasoning content.
    - `message.output_text.logprobs`: Include log probabilities for output text.
    """

    reasoning_encrypted_content = "reasoning.encrypted_content"
    message_output_text_logprobs = "message.output_text.logprobs"


class VerbosityEnum(str, Enum):
    """The verbosity level for responses.

    - `low`: Minimal verbosity.
    - `medium`: Moderate verbosity.
    - `high`: Maximum verbosity.
    """

    low = "low"
    medium = "medium"
    high = "high"


# ============================================================================
# Section 2: Content Types
# ============================================================================


class InputTextContent(BaseModel):
    """Text content in input messages."""

    model_config = ConfigDict(frozen=True)

    type: Literal["input_text"] = Field(
        "input_text", description="The type of content, always 'input_text'."
    )
    text: str = Field(..., description="The text content.")


class InputImageContent(BaseModel):
    """Image content in input messages."""

    model_config = ConfigDict(frozen=True)

    type: Literal["input_image"] = Field(
        "input_image", description="The type of content, always 'input_image'."
    )
    image_url: str = Field(..., description="The URL of the image.")
    detail: ImageDetail | None = Field(
        None,
        description="The detail level for image processing. Controls how the model "
        "processes the image.",
    )

    @model_validator(mode="after")
    def validate_data_uri_only(self) -> InputImageContent:
        """Validate that image_url is a data URI.

        Web URLs (http/https) should have been converted to base64 data URIs
        during request preprocessing. Only data URIs are accepted at validation time.

        Raises:
            ValueError: If image_url is not a data URI.

        Returns:
            The validated InputImageContent instance.
        """
        if not self.image_url.startswith("data:"):
            raise ValueError(
                f"Only data URIs are allowed in image_url field. "
                f"Expected format: 'data:image/[type];base64,[data]', "
                f"but got: {self.image_url[:50]}... "
                f"Web URLs are automatically converted during request processing."
            )
        return self


class InputFileContent(BaseModel):
    """File content in input messages."""

    model_config = ConfigDict(frozen=True)

    type: Literal["input_file"] = Field(
        "input_file", description="The type of content, always 'input_file'."
    )
    file_url: str | None = Field(None, description="The URL of the file.")
    file_data: str | None = Field(None, description="Base64-encoded file data.")
    filename: str | None = Field(None, description="The name of the file.")


class InputVideoContent(BaseModel):
    """Video content in input messages."""

    model_config = ConfigDict(frozen=True)

    type: Literal["input_video"] = Field(
        "input_video", description="The type of content, always 'input_video'."
    )
    video_url: str = Field(..., description="The URL of the video.")


class OutputVideoContent(BaseModel):
    """Video content generated by the model in output messages."""

    model_config = ConfigDict(frozen=True)

    type: Literal["output_video"] = Field(
        "output_video",
        description="The type of content, always 'output_video'.",
    )
    video_url: str | None = Field(
        None, description="The URL of the generated video."
    )
    video_data: str | None = Field(
        None, description="Base64-encoded video data."
    )
    format: str | None = Field(
        None,
        description="The format of the video (e.g., 'mp4', 'webm').",
    )

    @classmethod
    def from_numpy(
        cls,
        array: npt.NDArray[Any],
        fps: int = 24,
        format: str = "raw",
    ) -> OutputVideoContent:
        """Create an OutputVideoContent from a numpy array.

        Converts a numpy array containing video data to base64-encoded format.
        Currently supports raw tensor dumps. Future implementations may support
        video encoding (e.g. mp4) if dependencies allow.

        Args:
            array: A numpy array containing video data. Expected shape:
                   [frames, height, width, channels] or similar.
                   Values are typically float32 [0, 1] or uint8 [0, 255].
            fps: Frames per second metadata (currently unused by raw encoder).
            format: The format string. Defaults to "raw".

        Returns:
            An OutputVideoContent instance with base64-encoded data.
        """
        if format == "mp4":
            try:
                import io

                import av
                import numpy as _np

                # Expected shape: [frames, height, width, channels] float32 in [0, 1]
                frames_uint8 = (_np.clip(array, 0.0, 1.0) * 255).astype(
                    _np.uint8
                )
                num_frames_enc, h, w = (
                    frames_uint8.shape[0],
                    frames_uint8.shape[1],
                    frames_uint8.shape[2],
                )

                # Ensure even dimensions (required by most h264 encoders)
                h_enc = h - (h % 2)
                w_enc = w - (w % 2)
                if h_enc != h or w_enc != w:
                    frames_uint8 = frames_uint8[:, :h_enc, :w_enc, :]

                buf = io.BytesIO()
                container = av.open(buf, mode="w", format="mp4")
                stream = container.add_stream("h264", rate=fps)
                stream.width = w_enc
                stream.height = h_enc
                stream.pix_fmt = "yuv420p"
                stream.options = {"crf": "23"}
                for fi in range(num_frames_enc):
                    frame = av.VideoFrame.from_ndarray(
                        frames_uint8[fi], format="rgb24"
                    )
                    for packet in stream.encode(frame):
                        container.mux(packet)
                for packet in stream.encode():
                    container.mux(packet)
                container.close()

                video_bytes = buf.getvalue()
                base64_data = base64.b64encode(video_bytes).decode("utf-8")
                return cls(
                    type="output_video",
                    video_data=base64_data,
                    format="mp4",
                )
            except ImportError:
                # av not installed - fall through to raw dump
                format = "raw"

        # Fallback: raw bytes dump
        video_bytes = array.tobytes()
        base64_data = base64.b64encode(video_bytes).decode("utf-8")

        return cls(
            type="output_video",
            video_data=base64_data,
            format=format,
        )


class OutputAudioContent(BaseModel):
    """Audio content generated by the model in output messages."""

    model_config = ConfigDict(frozen=True)

    type: Literal["output_audio"] = Field(
        "output_audio",
        description="The type of content, always 'output_audio'.",
    )
    audio_url: str | None = Field(
        None, description="The URL of the generated audio."
    )
    audio_data: str | None = Field(
        None, description="Base64-encoded audio data."
    )
    format: str | None = Field(
        None,
        description="The format of the audio (e.g., 'wav', 'mp3').",
    )

    @classmethod
    def from_numpy(
        cls,
        array: npt.NDArray[Any],
        sample_rate: int = 16000,
        format: str = "wav",
    ) -> OutputAudioContent:
        """Create an OutputAudioContent from a numpy array.

        Converts a numpy array containing audio data to base64-encoded format.
        If format is 'wav', encodes with RIFF WAVE header.

        Args:
            array: A numpy array containing audio data. Expected shape:
                   [channels, samples] or [samples].
                   Values should be float32 [-1, 1] or int16.
            sample_rate: Sampling rate in Hz. Defaults to 16000.
            format: The format to encode to. Defaults to "wav".

        Returns:
            An OutputAudioContent instance with base64-encoded data.
        """
        import wave

        # Ensure array is in the right shape/format
        # Expected: [channels, samples] or [samples]
        if array.ndim == 1:
            # Mono
            channels = 1
            samples = array
        elif array.ndim == 2:
            # Check dimensions to guess channels vs samples
            # Usually samples >> channels
            if array.shape[0] < array.shape[1] and array.shape[0] <= 8:
                channels = array.shape[0]
                samples = array.T.flatten()  # Interleave channels
            else:
                # Assume [samples, channels]
                channels = array.shape[1]
                samples = array.flatten()
        else:
            raise ValueError(f"Unsupported audio array shape: {array.shape}")

        # Convert to int16 if float
        if array.dtype in (np.float32, np.float64):
            # Clip and scale
            audio_int16 = (np.clip(samples, -1.0, 1.0) * 32767).astype(np.int16)
        else:
            audio_int16 = samples.astype(np.int16)

        buffer = BytesIO()
        if format.lower() == "wav":
            with wave.open(buffer, "wb") as wav_file:
                wav_file.setnchannels(channels)
                wav_file.setsampwidth(2)  # 16-bit
                wav_file.setframerate(sample_rate)
                wav_file.writeframes(audio_int16.tobytes())
            audio_bytes = buffer.getvalue()
        else:
            # Fallback to raw bytes
            audio_bytes = audio_int16.tobytes()

        base64_data = base64.b64encode(audio_bytes).decode("utf-8")

        return cls(
            type="output_audio",
            audio_data=base64_data,
            format=format,
        )


class OutputTextContent(BaseModel):
    """Text content in output messages."""

    model_config = ConfigDict(frozen=True)

    type: Literal["output_text"] = Field(
        "output_text", description="The type of content, always 'output_text'."
    )
    text: str = Field(..., description="The text content.")
    annotations: list[int] | None = Field(
        None,
        description="Indices of citations or references for this text segment.",
    )
    logprobs: list[LogProb] | None = Field(
        None,
        description="Log probability information for the output tokens.",
    )


class RefusalContent(BaseModel):
    """Refusal content when the model declines to respond."""

    model_config = ConfigDict(frozen=True)

    type: Literal["refusal"] = Field(
        "refusal", description="The type of content, always 'refusal'."
    )
    refusal: str = Field(
        ...,
        description="The refusal message explaining why the model declined.",
    )


class ReasoningSummaryContent(BaseModel):
    """Summary of the model's reasoning process."""

    model_config = ConfigDict(frozen=True)

    type: Literal["reasoning_summary"] = Field(
        "reasoning_summary",
        description="The type of content, always 'reasoning_summary'.",
    )
    summary: str = Field(..., description="The reasoning summary text.")


class OutputImageContent(BaseModel):
    """Image content generated by the model in output messages."""

    model_config = ConfigDict(frozen=True)

    type: Literal["output_image"] = Field(
        "output_image",
        description="The type of content, always 'output_image'.",
    )
    image_url: str | None = Field(
        None, description="The URL of the generated image."
    )
    image_data: str | None = Field(
        None, description="Base64-encoded image data."
    )
    format: str | None = Field(
        None,
        description="The format of the image (e.g., 'png', 'jpeg', 'webp').",
    )
    detail: ImageDetail | None = Field(
        None,
        description="The detail level of the generated image.",
    )

    @classmethod
    def from_numpy(
        cls,
        array: npt.NDArray[np.float32],
        format: str = "png",
        detail: ImageDetail | None = None,
    ) -> OutputImageContent:
        """Create an OutputImageContent from a numpy array.

        Converts a numpy array containing image data to base64-encoded format
        suitable for the OpenResponses API.

        Args:
            array: A numpy array containing image data with dtype float32.
                   Expected shapes:
                   - [H, W, C] for color images (C=3 for RGB, C=4 for RGBA)
                   - [H, W] for grayscale images
                   Values should be in range [0, 1].
            format: The image format to use for encoding (e.g., 'png', 'jpeg', 'webp').
                    Defaults to 'png'.
            detail: Optional detail level for the generated image.

        Returns:
            An OutputImageContent instance with base64-encoded image data.

        Raises:
            ImportError: If PIL/Pillow is not available.
            ValueError: If the array shape or dtype is invalid.

        Example:
            >>> import numpy as np
            >>> from max.interfaces.request.open_responses import OutputImageContent
            >>> # Create a simple red image
            >>> img_array = np.ones((100, 100, 3), dtype=np.float32)
            >>> img_array[:, :, 1:] = 0  # Set green and blue to 0
            >>> output = OutputImageContent.from_numpy(img_array, format="png")
        """
        # Validate array shape
        if array.ndim not in (2, 3):
            raise ValueError(
                f"Expected 2D or 3D array, got shape {array.shape}. "
                "Valid shapes: [H, W] for grayscale or [H, W, C] for color."
            )

        # Convert to uint8 if needed
        if array.dtype == np.float32:
            # Assume values are in [0, 1] range
            array = np.clip(array * 255, 0, 255).astype(np.uint8)
        else:
            raise ValueError(
                f"Unsupported dtype {array.dtype}. Expected float32."
            )

        # Handle different array shapes
        if array.ndim == 2:
            # Grayscale image
            pil_image = Image.fromarray(array, mode="L")
        elif array.shape[2] == 1:
            # Single-channel image, squeeze to 2D
            pil_image = Image.fromarray(array.squeeze(), mode="L")
        elif array.shape[2] == 3:
            # RGB image
            pil_image = Image.fromarray(array, mode="RGB")
        elif array.shape[2] == 4:
            # RGBA image
            pil_image = Image.fromarray(array, mode="RGBA")
        else:
            raise ValueError(
                f"Unsupported number of channels: {array.shape[2]}. "
                "Expected 1 (grayscale), 3 (RGB), or 4 (RGBA)."
            )

        # Convert to bytes
        buffer = BytesIO()
        pil_image.save(buffer, format=format.upper())
        image_bytes = buffer.getvalue()

        # Encode as base64
        base64_data = base64.b64encode(image_bytes).decode("utf-8")

        return cls(
            type="output_image",
            image_data=base64_data,
            format=format.lower(),
            detail=detail,
        )


# Type alias for input content
InputContent = (
    InputTextContent | InputImageContent | InputFileContent | InputVideoContent
)

# Type alias for output content - using discriminated union for efficient deserialization
OutputContent = Annotated[
    OutputTextContent
    | RefusalContent
    | ReasoningSummaryContent
    | OutputImageContent
    | OutputVideoContent
    | OutputAudioContent,
    Field(discriminator="type"),
]


# ============================================================================
# Section 3: Message Types
# ============================================================================


class UserMessage(BaseModel):
    """A message from the user."""

    model_config = ConfigDict(frozen=True)

    role: Literal["user"] = Field(
        "user", description="The role of the message author, always 'user'."
    )
    content: str | list[InputContent] = Field(
        ...,
        description="The content of the message. Can be a simple string or a list "
        "of structured content items (text, images, files, videos).",
    )
    name: str | None = Field(
        None,
        description="An optional name for the participant. Provides the model "
        "information to differentiate between participants of the same role.",
    )


class AssistantMessage(BaseModel):
    """A message from the assistant."""

    model_config = ConfigDict(frozen=True)

    role: Literal["assistant"] = Field(
        "assistant",
        description="The role of the message author, always 'assistant'.",
    )
    content: str | list[OutputTextContent | RefusalContent] = Field(
        ...,
        description="The content of the message. Can be a simple string or a list "
        "of structured content items (text, refusal).",
    )
    name: str | None = Field(
        None,
        description="An optional name for the participant.",
    )
    tool_calls: list[FunctionCall] | None = Field(
        None,
        description="The tool calls generated by the model, such as function calls.",
    )


class SystemMessage(BaseModel):
    """A system message that guides the assistant's behavior."""

    model_config = ConfigDict(frozen=True)

    role: Literal["system"] = Field(
        "system", description="The role of the message author, always 'system'."
    )
    content: str | list[InputTextContent] = Field(
        ...,
        description="The content of the message. System messages can only contain "
        "text content.",
    )
    name: str | None = Field(
        None,
        description="An optional name for the participant.",
    )


class DeveloperMessage(BaseModel):
    """A message from the application developer."""

    model_config = ConfigDict(frozen=True)

    role: Literal["developer"] = Field(
        "developer",
        description="The role of the message author, always 'developer'.",
    )
    content: str | list[InputTextContent] = Field(
        ...,
        description="The content of the message. Developer messages can only "
        "contain text content.",
    )
    name: str | None = Field(
        None,
        description="An optional name for the participant.",
    )


class Message(BaseModel):
    """A generic message in the conversation."""

    model_config = ConfigDict(frozen=True)

    id: str = Field(..., description="The unique identifier for the message.")
    role: MessageRole = Field(
        ..., description="The role of the message author."
    )
    content: str | list[InputContent | OutputContent] = Field(
        ...,
        description="The content of the message.",
    )
    name: str | None = Field(
        None,
        description="An optional name for the participant.",
    )
    status: MessageStatus | None = Field(
        None,
        description="The status of the message.",
    )
    tool_calls: list[FunctionCall] | None = Field(
        None,
        description="The tool calls generated by the model.",
    )


# Type alias for input messages
InputMessage = UserMessage | AssistantMessage | SystemMessage | DeveloperMessage


# ============================================================================
# Section 4: Tool Types
# ============================================================================


class FunctionToolParam(BaseModel):
    """A function tool definition in the request."""

    model_config = ConfigDict(frozen=True)

    type: Literal["function"] = Field(
        "function", description="The type of tool, always 'function'."
    )
    name: str = Field(
        ...,
        description="The name of the function to be called. Must be a-z, A-Z, 0-9, "
        "or contain underscores and dashes, with a maximum length of 64.",
    )
    description: str | None = Field(
        None,
        description="A description of what the function does, used by the model "
        "to choose when and how to call the function.",
    )
    parameters: dict[str, Any] | None = Field(
        None,
        description="The parameters the function accepts, described as a JSON "
        "Schema object. Omit to indicate the function takes no parameters.",
    )
    strict: bool | None = Field(
        None,
        description="Whether to enable strict schema adherence when generating "
        "the function call. If true, the model will follow the exact schema "
        "provided in parameters.",
    )


class FunctionTool(BaseModel):
    """A function tool definition in the response."""

    model_config = ConfigDict(frozen=True)

    type: Literal["function"] = Field(
        "function", description="The type of tool, always 'function'."
    )
    name: str = Field(..., description="The name of the function.")
    description: str | None = Field(
        None, description="A description of what the function does."
    )
    parameters: dict[str, Any] | None = Field(
        None, description="The parameters the function accepts."
    )
    strict: bool | None = Field(
        None, description="Whether strict schema adherence is enabled."
    )


class FunctionCall(BaseModel):
    """A function call generated by the model."""

    model_config = ConfigDict(frozen=True)

    id: str = Field(
        ..., description="The unique identifier for the function call."
    )
    type: Literal["function"] = Field(
        "function", description="The type of tool call, always 'function'."
    )
    name: str = Field(..., description="The name of the function to call.")
    arguments: str = Field(
        ...,
        description="The arguments to call the function with, as a JSON string.",
    )
    status: FunctionCallStatus | None = Field(
        None, description="The status of the function call."
    )


class FunctionCallOutput(BaseModel):
    """The output from a function call."""

    model_config = ConfigDict(frozen=True)

    id: str = Field(
        ...,
        description="The ID of the function call this output corresponds to.",
    )
    type: Literal["function_call_output"] = Field(
        "function_call_output",
        description="The type, always 'function_call_output'.",
    )
    output: str = Field(
        ...,
        description="The output of the function call, as a JSON string.",
    )


class FunctionToolChoice(BaseModel):
    """Specifies a particular function to call."""

    model_config = ConfigDict(frozen=True)

    type: Literal["function"] = Field(
        "function", description="The type of tool choice, always 'function'."
    )
    name: str = Field(..., description="The name of the function to call.")


class AllowedToolChoice(BaseModel):
    """Specifies a subset of allowed tools and selection mode."""

    model_config = ConfigDict(frozen=True)

    type: Literal["allowed_tools"] = Field(
        "allowed_tools",
        description="The type of tool choice, always 'allowed_tools'.",
    )
    tools: list[FunctionToolChoice] = Field(
        ...,
        description="The list of specific functions the model is allowed to call.",
    )
    mode: ToolChoiceValueEnum = Field(
        ...,
        description="How the model should use the allowed tools (none, auto, or "
        "required).",
    )


# Type alias for tool choice
ToolChoice = ToolChoiceValueEnum | FunctionToolChoice | AllowedToolChoice


# ============================================================================
# Section 5: Reasoning Types
# ============================================================================


class ReasoningParam(BaseModel):
    """Configuration for extended reasoning."""

    model_config = ConfigDict(frozen=True)

    effort: ReasoningEffortEnum | None = Field(
        None,
        description="The effort level for extended reasoning. Higher effort may "
        "produce more thoughtful responses at the cost of increased latency.",
    )
    summary: ReasoningSummaryEnum | None = Field(
        None,
        description="The level of detail for reasoning summaries included in the "
        "response.",
    )


class ReasoningBody(BaseModel):
    """Reasoning information in the response."""

    model_config = ConfigDict(frozen=True)

    effort: ReasoningEffortEnum | None = Field(
        None, description="The effort level used for extended reasoning."
    )
    summary: ReasoningSummaryEnum | None = Field(
        None, description="The level of detail for reasoning summaries."
    )
    encrypted_content: str | None = Field(
        None,
        description="Encrypted reasoning content, only included if requested via "
        "the include parameter.",
    )


class ReasoningReferenceParam(BaseModel):
    """Reference to previous reasoning content."""

    model_config = ConfigDict(frozen=True)

    type: Literal["reasoning_reference"] = Field(
        "reasoning_reference",
        description="The type, always 'reasoning_reference'.",
    )
    reasoning_id: str = Field(
        ..., description="The ID of the reasoning to reference."
    )


class ReasoningReference(BaseModel):
    """Reference to reasoning in the response."""

    model_config = ConfigDict(frozen=True)

    type: Literal["reasoning_reference"] = Field(
        "reasoning_reference",
        description="The type, always 'reasoning_reference'.",
    )
    reasoning_id: str = Field(
        ..., description="The ID of the reasoning being referenced."
    )


# ============================================================================
# Section 6: Text & Response Formats
# ============================================================================


class TextParam(BaseModel):
    """Configuration for text generation."""

    model_config = ConfigDict(frozen=True)

    type: Literal["text"] = Field(
        "text", description="The type, always 'text'."
    )


class TextField(BaseModel):
    """Text response format."""

    model_config = ConfigDict(frozen=True)

    type: Literal["text"] = Field(
        "text", description="The type, always 'text'."
    )


class JsonObjectParam(BaseModel):
    """Configuration for JSON object responses."""

    model_config = ConfigDict(frozen=True)

    type: Literal["json_object"] = Field(
        "json_object", description="The type, always 'json_object'."
    )


class JsonObjectField(BaseModel):
    """JSON object response format."""

    model_config = ConfigDict(frozen=True)

    type: Literal["json_object"] = Field(
        "json_object", description="The type, always 'json_object'."
    )


class JsonSchemaParam(BaseModel):
    """Configuration for JSON schema-constrained responses."""

    model_config = ConfigDict(frozen=True)

    type: Literal["json_schema"] = Field(
        "json_schema", description="The type, always 'json_schema'."
    )
    json_schema: dict[str, Any] = Field(
        ...,
        description="The JSON schema that the model's output must conform to.",
    )
    name: str | None = Field(
        None,
        description="A name for the schema, used for identification.",
    )
    description: str | None = Field(
        None,
        description="A description of the schema.",
    )
    strict: bool | None = Field(
        None,
        description="Whether to enable strict schema adherence.",
    )


class JsonSchemaField(BaseModel):
    """JSON schema response format."""

    model_config = ConfigDict(frozen=True)

    type: Literal["json_schema"] = Field(
        "json_schema", description="The type, always 'json_schema'."
    )
    json_schema: dict[str, Any] = Field(
        ...,
        description="The JSON schema that the model's output conforms to.",
    )
    name: str | None = Field(None, description="The name of the schema.")
    description: str | None = Field(
        None, description="A description of the schema."
    )
    strict: bool | None = Field(
        None, description="Whether strict schema adherence was enabled."
    )


# Type aliases for response formats
ResponseFormatParam = TextParam | JsonObjectParam | JsonSchemaParam
ResponseFormat = TextField | JsonObjectField | JsonSchemaField


class StreamOptionsParam(BaseModel):
    """Options for streaming responses."""

    model_config = ConfigDict(frozen=True)

    include_usage: bool | None = Field(
        None,
        description="If true, include token usage information in the stream.",
    )


# ============================================================================
# Section 7: Supporting Types
# ============================================================================


class Error(BaseModel):
    """Error information."""

    model_config = ConfigDict(frozen=True)

    code: str = Field(..., description="The error code.")
    message: str = Field(..., description="The error message.")
    param: str | None = Field(
        None, description="The parameter that caused the error."
    )


class IncompleteDetails(BaseModel):
    """Details about why a generation was incomplete."""

    model_config = ConfigDict(frozen=True)

    reason: str = Field(
        ...,
        description="The reason why the generation was incomplete. Common reasons "
        "include 'max_tokens', 'stop_sequence', 'content_filter'.",
    )


class InputTokensDetails(BaseModel):
    """Detailed token count information for input."""

    model_config = ConfigDict(frozen=True)

    cached_tokens: int | None = Field(
        None,
        description="The number of cached tokens in the input.",
    )
    text_tokens: int | None = Field(
        None,
        description="The number of text tokens in the input.",
    )
    audio_tokens: int | None = Field(
        None,
        description="The number of audio tokens in the input.",
    )
    image_tokens: int | None = Field(
        None,
        description="The number of image tokens in the input.",
    )


class OutputTokensDetails(BaseModel):
    """Detailed token count information for output."""

    model_config = ConfigDict(frozen=True)

    text_tokens: int | None = Field(
        None,
        description="The number of text tokens in the output.",
    )
    audio_tokens: int | None = Field(
        None,
        description="The number of audio tokens in the output.",
    )
    reasoning_tokens: int | None = Field(
        None,
        description="The number of reasoning tokens in the output.",
    )


class Usage(BaseModel):
    """Token usage statistics."""

    model_config = ConfigDict(frozen=True)

    input_tokens: int = Field(
        ..., description="The total number of input tokens."
    )
    output_tokens: int = Field(
        ..., description="The total number of output tokens."
    )
    total_tokens: int = Field(
        ..., description="The total number of tokens (input + output)."
    )
    input_tokens_details: InputTokensDetails | None = Field(
        None,
        description="Detailed breakdown of input token counts.",
    )
    output_tokens_details: OutputTokensDetails | None = Field(
        None,
        description="Detailed breakdown of output token counts.",
    )


class TopLogProb(BaseModel):
    """Log probability information for a token."""

    model_config = ConfigDict(frozen=True)

    token: str = Field(..., description="The token.")
    logprob: float = Field(..., description="The log probability of the token.")
    bytes: list[int] | None = Field(
        None, description="The UTF-8 bytes of the token."
    )


class LogProb(BaseModel):
    """Log probability information for output tokens."""

    model_config = ConfigDict(frozen=True)

    token: str = Field(..., description="The token.")
    logprob: float = Field(..., description="The log probability of the token.")
    bytes: list[int] | None = Field(
        None, description="The UTF-8 bytes of the token."
    )
    top_logprobs: list[TopLogProb] | None = Field(
        None,
        description="List of the most likely tokens and their log probabilities "
        "at this position.",
    )


class UrlCitationParam(BaseModel):
    """URL citation in request."""

    model_config = ConfigDict(frozen=True)

    type: Literal["url"] = Field("url", description="The type, always 'url'.")
    url: str = Field(..., description="The URL being cited.")
    title: str | None = Field(None, description="The title of the URL.")


class UrlCitationBody(BaseModel):
    """URL citation in response."""

    model_config = ConfigDict(frozen=True)

    type: Literal["url"] = Field("url", description="The type, always 'url'.")
    url: str = Field(..., description="The URL being cited.")
    title: str | None = Field(None, description="The title of the URL.")


class ItemReferenceParam(BaseModel):
    """Reference to an item in the conversation."""

    model_config = ConfigDict(frozen=True)

    type: Literal["item_reference"] = Field(
        "item_reference", description="The type, always 'item_reference'."
    )
    item_id: str = Field(
        ..., description="The ID of the item being referenced."
    )


# ============================================================================
# Section 8: Main Request Type
# ============================================================================


class OpenResponsesRequestBody(BaseModel):
    """Request body for creating a response."""

    model_config = ConfigDict(frozen=True)

    model: str = Field(
        ...,
        description="ID of the model to use. See the model endpoint "
        "documentation for details on which models are available.",
    )
    input: str | list[InputMessage] = Field(
        ...,
        description="The input to generate a response for. Can be a simple string "
        "or a list of messages for multi-turn conversations.",
    )
    citations: list[UrlCitationParam | ItemReferenceParam] | None = Field(
        None,
        description="Citations and references to include in the request.",
    )
    frequency_penalty: float | None = Field(
        None,
        ge=-2.0,
        le=2.0,
        description="Number between -2.0 and 2.0. Positive values penalize new "
        "tokens based on their existing frequency in the text so far, decreasing "
        "the model's likelihood to repeat the same line verbatim.",
    )
    include: list[IncludeEnum] | None = Field(
        None,
        description="Additional data to include in the response, such as reasoning "
        "content or log probabilities.",
    )
    logit_bias: dict[str, float] | None = Field(
        None,
        description="Modify the likelihood of specified tokens appearing in the "
        "completion. Maps token IDs to bias values from -100 to 100.",
    )
    logprobs: bool | None = Field(
        None,
        description="Whether to return log probabilities of the output tokens.",
    )
    max_output_tokens: int | None = Field(
        None,
        ge=1,
        description="The maximum number of tokens that can be generated in the "
        "response.",
    )
    metadata: dict[str, str] | None = Field(
        None,
        description="Metadata to attach to the request. This can be useful for "
        "filtering and organization.",
    )
    modalities: list[str] | None = Field(
        None,
        description="The modalities to include in the response (e.g., text, audio).",
    )
    n: int | None = Field(
        None,
        ge=1,
        description="How many response choices to generate for each input message.",
    )
    parallel_tool_calls: bool | None = Field(
        None,
        description="Whether to enable parallel function calling during tool use.",
    )
    presence_penalty: float | None = Field(
        None,
        ge=-2.0,
        le=2.0,
        description="Number between -2.0 and 2.0. Positive values penalize new "
        "tokens based on whether they appear in the text so far, increasing the "
        "model's likelihood to talk about new topics.",
    )
    reasoning: ReasoningParam | None = Field(
        None,
        description="Configuration for extended reasoning.",
    )
    reasoning_reference: ReasoningReferenceParam | None = Field(
        None,
        description="Reference to previous reasoning to use for this request.",
    )
    response_format: ResponseFormatParam | None = Field(
        None,
        description="The format of the response. Can be text, json_object, or "
        "json_schema.",
    )
    seed: int | None = Field(
        None,
        description="If specified, the system will make a best effort to sample "
        "deterministically for this seed value.",
    )
    service_tier: ServiceTierEnum | None = Field(
        None,
        description="The service tier to use for this request.",
    )
    stop: str | list[str] | None = Field(
        None,
        description="Up to 4 sequences where the API will stop generating further "
        "tokens.",
    )
    store: bool | None = Field(
        None,
        description="Whether to store the output of this response for use in "
        "distillation or evals.",
    )
    stream: bool | None = Field(
        None,
        description="If true, stream back partial progress as server-sent events.",
    )
    stream_options: StreamOptionsParam | None = Field(
        None,
        description="Options for streaming responses.",
    )
    temperature: float | None = Field(
        None,
        ge=0.0,
        le=2.0,
        description="What sampling temperature to use, between 0 and 2. Higher "
        "values like 0.8 will make the output more random, while lower values "
        "like 0.2 will make it more focused and deterministic.",
    )
    tool_choice: ToolChoice | None = Field(
        None,
        description="Controls which (if any) tool is called by the model. Can be "
        "a simple value (none, auto, required), a specific function to call, or "
        "a list of allowed tools with a selection mode.",
    )
    tools: list[FunctionToolParam] | None = Field(
        None,
        description="A list of tools the model may call. Currently, only functions "
        "are supported as a tool.",
    )
    top_logprobs: int | None = Field(
        None,
        ge=0,
        le=20,
        description="An integer between 0 and 20 specifying the number of most "
        "likely tokens to return at each token position, each with an associated "
        "log probability.",
    )
    top_p: float | None = Field(
        None,
        ge=0.0,
        le=1.0,
        description="An alternative to sampling with temperature, called nucleus "
        "sampling, where the model considers the results of the tokens with "
        "top_p probability mass.",
    )
    truncation: TruncationEnum | None = Field(
        None,
        description="Controls message truncation behavior.",
    )
    user: str | None = Field(
        None,
        description="A unique identifier representing your end-user, which can "
        "help the provider to monitor and detect abuse.",
    )
    verbosity: VerbosityEnum | None = Field(
        None,
        description="The verbosity level for responses.",
    )
    provider_options: ProviderOptions = Field(
        default_factory=lambda: ProviderOptions(image=ImageProviderOptions()),
        description=(
            "Provider-specific options for MAX platform and modalities. "
            "Structure: 'max' for universal MAX options (target_endpoint, etc.), "
            "and modality-specific fields like 'image' for image generation or 'video' for video generation. "
            "Example: {'max': {'target_endpoint': 'instance-123'}, 'image': {'width': 1024, 'height': 768}}. "
            "Defaults to empty ProviderOptions with default ImageProviderOptions."
        ),
    )

    @model_validator(mode="after")
    def validate_streaming_not_supported(self) -> OpenResponsesRequestBody:
        """Validate that streaming is not requested.

        Streaming is not currently supported in the MAX implementation of the
        OpenResponses API. This validator ensures that requests with stream=True
        are rejected during validation.

        Raises:
            ValueError: If stream is set to True.

        Returns:
            The validated OpenResponsesRequestBody instance.
        """
        if self.stream is True:
            raise ValueError(
                "Streaming is not currently supported. "
                "Please set 'stream' to false or omit it from the request."
            )
        return self

    @model_validator(mode="after")
    def validate_single_message_only(self) -> OpenResponsesRequestBody:
        """Validate that only a single message is provided when using list input.

        The current implementation only supports single-turn requests with one
        message. Multi-turn conversations with multiple messages are not yet
        supported for pixel generation pipelines.

        Raises:
            ValueError: If input is a list with more than one message.

        Returns:
            The validated OpenResponsesRequestBody instance.
        """
        if isinstance(self.input, list) and len(self.input) > 1:
            raise ValueError(
                f"Only single-message input is currently supported. "
                f"Received {len(self.input)} messages. "
                f"Please provide exactly one message in the input list."
            )
        return self


# ============================================================================
# Section 9: Main Response Type
# ============================================================================


class ResponseResource(BaseModel):
    """The response from the model."""

    model_config = ConfigDict(frozen=True)

    id: str = Field(..., description="The unique identifier for the response.")
    object: Literal["response"] = Field(
        "response", description="The object type, always 'response'."
    )
    created_at: int = Field(
        ...,
        description="The Unix timestamp (in seconds) of when the response was created.",
    )
    status: str = Field(
        ...,
        description="The status of the response. Can be 'in_progress', 'completed', "
        "'incomplete', 'failed', or 'cancelled'.",
    )
    model: str = Field(
        ..., description="The model used to generate the response."
    )
    citations: list[UrlCitationBody | ItemReferenceParam] | None = Field(
        None,
        description="Citations and references included in the response.",
    )
    error: Error | None = Field(
        None,
        description="Error information if the response failed.",
    )
    frequency_penalty: float | None = Field(
        None,
        description="The frequency penalty used for this response.",
    )
    include: list[IncludeEnum] | None = Field(
        None,
        description="Additional data included in the response.",
    )
    incomplete_details: IncompleteDetails | None = Field(
        None,
        description="Details about why the response was incomplete, if applicable.",
    )
    logit_bias: dict[str, float] | None = Field(
        None,
        description="The logit bias used for this response.",
    )
    logprobs: bool | None = Field(
        None,
        description="Whether log probabilities were returned.",
    )
    max_output_tokens: int | None = Field(
        None,
        description="The maximum number of output tokens allowed.",
    )
    metadata: dict[str, str] | None = Field(
        None,
        description="Metadata attached to the request.",
    )
    modalities: list[str] | None = Field(
        None,
        description="The modalities included in the response.",
    )
    n: int | None = Field(
        None,
        description="The number of response choices generated.",
    )
    output: list[Message] | None = Field(
        None,
        description="The messages generated by the model.",
    )
    parallel_tool_calls: bool | None = Field(
        None,
        description="Whether parallel function calling was enabled.",
    )
    presence_penalty: float | None = Field(
        None,
        description="The presence penalty used for this response.",
    )
    reasoning: ReasoningBody | None = Field(
        None,
        description="Reasoning information for this response.",
    )
    reasoning_reference: ReasoningReference | None = Field(
        None,
        description="Reference to reasoning used for this response.",
    )
    response_format: ResponseFormat | None = Field(
        None,
        description="The format of the response.",
    )
    seed: int | None = Field(
        None,
        description="The seed used for deterministic sampling.",
    )
    service_tier: ServiceTierEnum | None = Field(
        None,
        description="The service tier used for this request.",
    )
    stop: str | list[str] | None = Field(
        None,
        description="The stop sequences used.",
    )
    store: bool | None = Field(
        None,
        description="Whether the output was stored.",
    )
    stream: bool | None = Field(
        None,
        description="Whether streaming was enabled.",
    )
    stream_options: StreamOptionsParam | None = Field(
        None,
        description="Streaming options used.",
    )
    temperature: float | None = Field(
        None,
        description="The sampling temperature used.",
    )
    tool_choice: ToolChoice | None = Field(
        None,
        description="The tool choice configuration used.",
    )
    tools: list[FunctionTool] | None = Field(
        None,
        description="The tools that were available for the model to call.",
    )
    top_logprobs: int | None = Field(
        None,
        description="The number of top log probabilities returned.",
    )
    top_p: float | None = Field(
        None,
        description="The nucleus sampling parameter used.",
    )
    truncation: TruncationEnum | None = Field(
        None,
        description="The truncation behavior used.",
    )
    usage: Usage | None = Field(
        None,
        description="Token usage statistics for this response.",
    )
    user: str | None = Field(
        None,
        description="The user identifier provided in the request.",
    )
    verbosity: VerbosityEnum | None = Field(
        None,
        description="The verbosity level used.",
    )

    @classmethod
    def from_generation_output(
        cls,
        generation_output: GenerationOutput,
        model: str,
    ) -> ResponseResource:
        """Create a ResponseResource from a GenerationOutput.

        Converts pipeline generation output to the OpenResponses API response format.

        Args:
            generation_output: The generation output from the pipeline, containing
                the generated content (images, etc.) and request metadata.
            model: The model name used for generation.

        Returns:
            A ResponseResource instance with the generated content formatted as
            an assistant message in the output field.

        Example:
            >>> from max.interfaces.generation import GenerationOutput
            >>> from max.interfaces.request import RequestID
            >>> from max.interfaces.request.open_responses import OutputImageContent
            >>> from max.interfaces.status import GenerationStatus
            >>> import numpy as np
            >>>
            >>> # Create generation output
            >>> img_array = np.random.rand(512, 512, 3).astype(np.float32)
            >>> gen_output = GenerationOutput(
            ...     request_id=RequestID(value="req-123"),
            ...     final_status=GenerationStatus.END_OF_SEQUENCE,
            ...     output=[OutputImageContent.from_numpy(img_array, format="png")]
            ... )
            >>>
            >>> # Convert to ResponseResource
            >>> response = ResponseResource.from_generation_output(
            ...     gen_output, model="flux-1-schnell"
            ... )
        """
        # Create a message from the generation output
        # Message IDs follow the format: msg_<request_id>_<index>
        message_index = 0
        message = Message(
            id=f"msg_{generation_output.request_id.value}_{message_index}",
            role=MessageRole.assistant,
            content=list(generation_output.output),
            status=MessageStatus.completed,
        )

        return cls(
            id=f"resp_{generation_output.request_id.value}",
            object="response",
            created_at=int(time.time()),
            status="completed",
            model=model,
            output=[message],
            usage=None,  # TODO: Populate token usage if available from generation_output
        )


# ============================================================================
# Section 10: Type Aliases
# ============================================================================

# Already defined inline above:
# - InputContent
# - OutputContent
# - InputMessage
# - ToolChoice
# - ResponseFormatParam
# - ResponseFormat


# ============================================================================
# Section 11: OpenResponses Request Container
# ============================================================================


class _RequestState(Protocol):
    """Protocol for request state containing request_id."""

    request_id: str


class FastAPIRequestProtocol(Protocol):
    """Minimal protocol for FastAPI/Starlette Request objects.

    This protocol defines the minimal interface needed from a FastAPI Request
    without requiring the fastapi dependency in the interfaces library.
    """

    state: _RequestState

    async def body(self) -> bytes:
        """Return the request body as bytes."""
        ...


def _is_web_url(url: str) -> bool:
    """Check if a URL is a web URL (http or https).

    Args:
        url: The URL to check.

    Returns:
        True if the URL starts with http:// or https://, False otherwise.
    """
    parsed = urlparse(url)
    return parsed.scheme in ("http", "https")


async def _download_and_encode_image(url: str) -> str:
    """Download an image from a URL and encode it as a base64 data URI.

    Args:
        url: The URL of the image to download.

    Returns:
        A data URI string containing the base64-encoded image.

    Raises:
        httpx.HTTPError: If the download fails due to HTTP errors.
        httpx.RequestError: If the download fails due to network errors.
    """
    async with httpx.AsyncClient() as client:
        response = await client.get(url, timeout=30.0, follow_redirects=True)
        response.raise_for_status()

        # Get image bytes
        image_bytes = response.content

        # Determine MIME type from Content-Type header or URL extension
        content_type = response.headers.get("content-type")
        if not content_type or not content_type.startswith("image/"):
            # Fall back to guessing from URL
            guessed_type, _ = mimetypes.guess_type(url)
            content_type = guessed_type or "image/png"

        # Encode as base64
        base64_data = base64.b64encode(image_bytes).decode("utf-8")

        # Return as data URI
        return f"data:{content_type};base64,{base64_data}"


async def _process_image_urls_in_dict(
    body_dict: dict[str, Any],
) -> dict[str, Any]:
    """Process image URLs in raw dict before Pydantic validation.

    Downloads images from web URLs (http/https) and converts them to
    base64-encoded data URIs. This ensures downstream processors don't
    need to download images individually.

    Args:
        body_dict: Raw request body as dict.

    Returns:
        Modified dict with web URLs replaced by base64 data URIs.

    Raises:
        ValueError: If any image URL cannot be downloaded, with details
            about which URL failed and why.
    """
    # Only process if input is a list
    input_value = body_dict.get("input")
    if not isinstance(input_value, list):
        return body_dict

    for message in input_value:
        # Only process user messages with list content
        if message.get("role") != "user":
            continue
        content = message.get("content")
        if not isinstance(content, list):
            continue

        for content_item in content:
            # Only process input_image types
            if content_item.get("type") != "input_image":
                continue

            image_url = content_item.get("image_url")
            if image_url and _is_web_url(image_url):
                logger.info(f"Downloading image from URL: {image_url}")
                try:
                    # Download and convert to data URI
                    data_uri = await _download_and_encode_image(image_url)
                    content_item["image_url"] = data_uri
                    logger.info(
                        "Successfully converted image URL to base64 data URI"
                    )
                except httpx.HTTPStatusError as e:
                    raise ValueError(
                        f"Failed to download image from '{image_url}': "
                        f"HTTP {e.response.status_code} {e.response.reason_phrase}"
                    ) from e
                except httpx.RequestError as e:
                    raise ValueError(
                        f"Failed to download image from '{image_url}': "
                        f"Network error - {str(e)}"
                    ) from e
                except Exception as e:
                    raise ValueError(
                        f"Failed to download image from '{image_url}': {str(e)}"
                    ) from e

    return body_dict


class OpenResponsesRequest(BaseModel):
    """General request container for OpenResponses API requests.

    This class wraps an OpenResponsesRequestBody and adheres to the Request schema.
    All request fields are accessed directly from the body.
    """

    model_config = ConfigDict(frozen=True)

    request_id: RequestID
    """A unique identifier for the request."""

    body: OpenResponsesRequestBody
    """The complete OpenResponses request body."""

    def __str__(self) -> str:
        return str(self.request_id)

    @classmethod
    async def from_fastapi_request(
        cls,
        request: FastAPIRequestProtocol,
    ) -> OpenResponsesRequest:
        """Create an OpenResponsesRequest from a FastAPI/Starlette Request.

        Extracts the request_id from request.state.request_id and parses the
        request body as an OpenResponsesRequestBody. If the request contains
        image URLs (http/https), they will be downloaded and converted to
        base64 data URIs before validation.

        Args:
            request: A request object with state.request_id and body() method.
                Compatible with FastAPI/Starlette Request objects.

        Returns:
            An OpenResponsesRequest instance with all web image URLs converted
            to base64 data URIs.

        Raises:
            ValueError: If request.state.request_id is not set, or if any
                image URL cannot be downloaded.
            pydantic.ValidationError: If the request body is invalid.
        """
        if not hasattr(request.state, "request_id"):
            raise ValueError(
                "request.state.request_id not found. "
                "Ensure the request ID middleware is properly configured."
            )

        request_id = RequestID(value=request.state.request_id)

        # Get raw JSON bytes
        raw_body = await request.body()

        # Parse to dict (not validated yet)
        body_dict = json.loads(raw_body)

        # Process image URLs in the dict BEFORE validation (async)
        # This downloads images and converts to base64 data URIs
        body_dict = await _process_image_urls_in_dict(body_dict)

        # NOW validate and create immutable Pydantic model
        body = OpenResponsesRequestBody.model_validate(body_dict)

        return cls(
            request_id=request_id,
            body=body,
        )
