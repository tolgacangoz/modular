# ===----------------------------------------------------------------------=== #
# Copyright (c) 2025, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

"""Standardized configuration for Pipeline Inference."""

from __future__ import annotations

import importlib
import json
import logging
import os
import sys
from dataclasses import MISSING, dataclass, field, fields
from enum import Enum
from pathlib import Path
from typing import Any, get_type_hints

from max.config import MAXConfig
from max.driver import DeviceSpec, load_devices
from max.engine import InferenceSession
from max.graph.quantization import QuantizationEncoding
from max.serve.queue.zmq_queue import generate_zmq_ipc_path

from .config_enums import PipelineRole, PixelGenerationType
from .diffusers_config import DiffusersConfig
from .kv_cache_config import KVCacheConfig
from .lora_config import LoRAConfig
from .memory_estimation import MemoryEstimator, to_human_readable_bytes
from .model_config import MAXModelConfig
from .profiling_config import ProfilingConfig
from .registry import (
    PIPELINE_REGISTRY,
    SupportedArchitecture,
    get_pipeline_for_task,
)
from .sampling import SamplingConfig
from .speculative_config import SpeculativeConfig

logger = logging.getLogger("max.pipelines")

# Default prefill chunk size for chunked prefill and memory estimation.
DEFAULT_PREFILL_CHUNK_SIZE = 8192


@dataclass(frozen=False)
class PipelineConfig(MAXConfig):
    """Configuration for a pipeline.

    WIP - Once a PipelineConfig is fully initialized, it should be as immutable
    as possible (frozen=True). All underlying dataclass fields should have been
    initialized to their default values, be it user specified via some CLI
    flag, config file, environment variable, or internally set to a reasonable
    default.
    """

    max_length: int | None = None
    """Maximum sequence length of the model."""

    pipeline_role: PipelineRole = PipelineRole.PrefillAndDecode
    """Whether the pipeline should serve both a prefill or decode role or both."""

    max_batch_size: int | None = None
    """Maximum batch size to execute with the model.
    When not specified (None), we determine this value dynamically. For users
    launching in a server scenario, the expectation is that this value should be
    set higher based on server capacity.
    """

    max_queue_size_tg: int | None = None
    """Maximum number of requests in decode queue. By default, this is max-batch-size."""

    min_batch_size_tg: int | None = None
    """Specifies a soft floor on the decode batch size.

    If the TG batch size is larger than this value, the scheduler will continue to
    run TG batches. If it falls below, the scheduler will prioritize CE. Note that
    this is NOT a strict minimum! By default, this is max-queue-size-tg.

    This is an experimental flag solely for the TTS scheduler. Do not use unless
    you know what you are doing.
    """

    ep_size: int = 1
    """The expert parallelism size. Needs to be 1 (no expert parallelism) or the
    total number of GPUs across nodes."""

    ce_delay_ms: float = 0.0
    """Duration of scheduler sleep prior to starting a prefill batch.

    This is an experimental flag solely for the TTS scheduler. Do not use unless
    you know what you are doing.
    """

    enable_prioritize_first_decode: bool = False
    """When enabled, the scheduler will always run a TG batch immediately after a CE batch,
    with the same requests. This may be useful for decreasing time-to-first-chunk latency.

    This is an experimental flag solely for the TTS scheduler. Do not use unless
    you know what you are doing.
    """

    experimental_background_queue: bool = False
    """When enabled, offloads queue draining to a background thread for improved performance.

    This is an experimental flag. Use with caution.
    """

    enable_chunked_prefill: bool = True
    """Enable chunked prefill to split context encoding requests into multiple chunks
    based on 'prefill_chunk_size'."""

    enable_in_flight_batching: bool = False
    """When enabled, prioritizes token generation by batching it with context
    encoding requests."""

    max_num_steps: int = -1
    """The number of steps to run for multi-step scheduling. -1 specifies a default value based on
    configuration and platform. Ignored for models which are not auto-regressive (e.g. embedding
    models)."""

    prefill_chunk_size: int = DEFAULT_PREFILL_CHUNK_SIZE
    """The target number of un-encoded tokens to include in each batch.
    This value is used for chunked prefill and memory estimation."""

    enable_echo: bool = False
    """Whether the model should be built with echo capabilities."""

    pool_embeddings: bool = True
    """Whether to pool embedding outputs."""

    chat_template: Path | None = None
    """Optional custom chat template to override the one shipped with the
    HuggingFace model config. Can be either:
    - A Path pointing to a file containing the template

    If a Path is provided, the file will be read during config resolution and
    the content will be stored as a string. This allows customizing the prompt
    formatting for different use cases. If None, the model's default chat
    template will be used."""

    use_experimental_kernels: str = os.environ.get(
        "USE_EXPERIMENTAL_KERNELS", "false"
    )
    """Enables using experimental mojo kernels with max serve.
    The kernels could be unstable, incorrect, or otherwise have issues.
    """

    use_vendor_blas: str = os.environ.get("MAX_SERVE_USE_VENDOR_BLAS", "false")
    """Enables using the vendor blas libraries (cublas/hipblas/etc) with max serve.
    Currently, this just replaces matmul calls, but it could replace other numeric functions in the future.
    """

    pdl_level: str = os.environ.get("PDL_LEVEL", "0")
    """Level of overlap of kernel launch via programmatic dependent grid control."""

    custom_architectures: list[str] = field(default_factory=list)
    """A list of custom architecture implementations to register.
    Each input can either be a raw module name or an import path followed by a colon and the module name.
    Ex:
    - `my_module`
    - `folder/path/to/import:my_module`

    Each module must expose an `ARCHITECTURES` list of architectures to register.
    """

    zmq_endpoint_base: str = field(default_factory=generate_zmq_ipc_path)
    """The prefix for the ZMQ endpoints used for IPC. This prefix ensures that the
    ZMQ endpoints are unique across multiple MAX Serve instances running on the
    same host. This should be randomly generated when the PipelineConfig is created.
    Ex:
    - lora_request_zmq_endpoint: f"{zmq_endpoint_base}-lora_request"
    - lora_response_zmq_endpoint: f"{zmq_endpoint_base}-lora_response"
    """

    execute_empty_batches: bool = False
    """Whether the scheduler should execute empty batches."""

    max_batch_context_length: int | None = None
    """Ensures that the sum of the context length in a batch does not exceed max_batch_context_length.

    If None, the sum of the context length in batch is not limited.
    """

    force: bool = field(default=False)
    """Skip validation of user provided flags against the architecture's required arguments."""

    kvcache_ce_watermark: float = 0.95
    """Projected cache usage threshold for scheduling CE requests, considers current + incoming
    request. CE is scheduled if either projected usage stays below this threshold OR no active
    requests exist. Greater KVCache utilization (as controlled by this parameter) was
    found to cause more preemptions.
    """

    use_module_v3: bool = False
    """Whether to use the ModuleV3 architecture if it exists."""

    _model_config: MAXModelConfig = field(default_factory=MAXModelConfig)
    """The model config."""

    _draft_model_config: MAXModelConfig | None = None
    """The draft model config."""

    _sampling_config: SamplingConfig = field(default_factory=SamplingConfig)
    """The sampling config."""

    _profiling_config: ProfilingConfig = field(default_factory=ProfilingConfig)
    """The profiling config."""

    _lora_config: LoRAConfig | None = None
    """The LoRA config."""

    _speculative_config: SpeculativeConfig | None = None
    """The SpeculativeConfig."""

    _config_file_section_name: str = "pipeline_config"
    """The section name to use when loading this config from a MAXConfig file.
    This is used to differentiate between different config sections in a single
    MAXConfig file."""

    def configure_session(self, session: InferenceSession) -> None:
        """Configure an InferenceSession with standard pipeline settings."""
        session.gpu_profiling(self.profiling_config.gpu_profiling)
        session._use_experimental_kernels(self.use_experimental_kernels)
        session._use_vendor_blas(self.use_vendor_blas)
        session._pdl_level(self.pdl_level)

    @staticmethod
    def _extract_kwargs_for_config(
        kwargs: dict[str, Any],
        config_class: type[MAXConfig],
        key_prefix: str = "",
        strip_prefix: bool = False,
    ) -> dict[str, Any]:
        """
        Extract kwargs that match a config class's fields.

        Args:
            kwargs: Source kwargs dictionary (modified in place)
            config_class: The MAXConfig dataclass to match fields against
            key_prefix: Optional prefix to filter keys (e.g., "draft_")
            strip_prefix: Whether to strip the prefix from extracted keys

        Returns:
            Dictionary of extracted kwargs
        """
        extracted = {}
        keys_to_remove = []

        for key, value in kwargs.items():
            # Check if key matches the prefix filter
            if key_prefix and not key.startswith(key_prefix):
                continue

            # Determine the field name to check
            field_name = key.replace(key_prefix, "") if strip_prefix else key

            # Check if this field exists in the config class
            if field_name in config_class.__dataclass_fields__:
                # Use original key or stripped key as specified
                extracted_key = field_name if strip_prefix else key
                extracted[extracted_key] = value
                keys_to_remove.append(key)

        # Remove extracted keys from original kwargs
        for key in keys_to_remove:
            del kwargs[key]

        return extracted

    def _create_lora_config_if_needed(self, kwargs: dict[str, Any]) -> None:
        """Extract LoRA kwargs and create valid LoRAConfig if enable_lora provided."""
        lora_kwargs = PipelineConfig._extract_kwargs_for_config(
            kwargs, LoRAConfig
        )

        if lora_kwargs.get("enable_lora", False):
            self._lora_config = LoRAConfig(**lora_kwargs)
        # TODO: We should add an elif to check / error out if other LoRA params
        # are provided, but enable_lora is not. We can't do this today as our
        # click PipelineConfig autogenerates defaults for all fields, including
        # required ones.

    # TODO: It might be cleaner to have the draft model be a part of the SpeculativeConfig
    def _create_draft_model_config_if_needed(
        self, kwargs: dict[str, Any]
    ) -> None:
        """Extract draft model kwargs and create MAXModelConfig if model_path provided."""
        draft_kwargs = PipelineConfig._extract_kwargs_for_config(
            kwargs, MAXModelConfig, key_prefix="draft_", strip_prefix=True
        )

        if draft_kwargs.get("model_path", "") != "":
            self._draft_model_config = MAXModelConfig(**draft_kwargs)
        # TODO: We should add an elif to check / error out if other draft model
        # params are provided, but model_path is not. We can't do this today
        # as our click PipelineConfig autogenerates defaults for all fields,
        # including required ones.

    def _create_speculative_config_if_needed(
        self, kwargs: dict[str, Any]
    ) -> None:
        """Extract speculative config kwargs and create SpeculativeConfig if any speculative parameters provided."""
        speculative_kwargs = PipelineConfig._extract_kwargs_for_config(
            kwargs, SpeculativeConfig
        )
        # Only create speculative config if speculative_method is explicitly set
        if (
            speculative_kwargs
            and speculative_kwargs.get("speculative_method") is not None
        ):
            # Remove None values to use defaults
            filtered_kwargs = {
                k: v for k, v in speculative_kwargs.items() if v is not None
            }
            if filtered_kwargs:
                self._speculative_config = SpeculativeConfig(**filtered_kwargs)
                assert self._draft_model_config is not None
                # We need to set the architecture to EagleLlamaForCausalLM for Eagle speculative decoding
                if self._speculative_config.is_eagle():
                    assert (
                        len(
                            self._draft_model_config.huggingface_config.architectures
                        )
                        == 1
                    )
                    hf_arch = self._draft_model_config.huggingface_config.architectures[
                        0
                    ]
                    if hf_arch == "LlamaForCausalLM":
                        self._draft_model_config.huggingface_config.architectures[
                            0
                        ] = "EagleLlamaForCausalLM"

    def _process_remaining_config_classes(
        self, unmatched_kwargs: dict[str, Any]
    ) -> None:
        """
        Process remaining kwargs for other config classes.

        Args:
            unmatched_kwargs: Dictionary of kwargs that haven't been matched yet
        """
        # TODO(zheng): Make this more efficient by using MaxConfig instance
        # instead of hardcoding the config names.
        config_mappings = [
            # NOTE: _model_config must come before _sampling_config so that
            # SamplingConfig can use generation_config from the model
            "_model_config",
            "_sampling_config",
            "_profiling_config",
        ]

        for config_name in config_mappings:
            config_class = get_type_hints(self.__class__)[config_name]
            matched_kwargs = {}
            kv_cache_kwargs = {}

            for key, value in unmatched_kwargs.items():
                if key in config_class.__dataclass_fields__:
                    matched_kwargs[key] = value
                # Check if this is a KVCache config param
                elif (
                    config_name == "_model_config"
                    and key in KVCacheConfig.__dataclass_fields__
                ):
                    kv_cache_kwargs[key] = value

            if matched_kwargs:
                self._create_and_set_config(
                    config_name, config_class, matched_kwargs, kv_cache_kwargs
                )

                # Remove matched kwargs
                for key in matched_kwargs:
                    _ = unmatched_kwargs.pop(key, None)
                for key in kv_cache_kwargs:
                    _ = unmatched_kwargs.pop(key, None)

    def _create_and_set_config(
        self,
        config_name: str,
        config_class: type,
        matched_kwargs: dict[str, Any],
        kv_cache_kwargs: dict[str, Any],
    ) -> None:
        """
        Create and set a config object with special handling for different config types.

        Args:
            config_name: Name of the config attribute (e.g., "_model_config")
            config_class: The config class to instantiate
            matched_kwargs: kwargs that matched the config class fields
            kv_cache_kwargs: kwargs for KVCache config (model config only)
        """
        if config_name == "_model_config" and kv_cache_kwargs:
            # Create new model config with updated KVCache config
            model_config = config_class(**matched_kwargs)

            if self._draft_model_config:
                memory_util = kv_cache_kwargs.get(
                    "device_memory_utilization", 0.9
                )
                main_model_util = memory_util * 0.7
                draft_model_util = memory_util - main_model_util

                kv_cache_kwargs["device_memory_utilization"] = main_model_util

            model_config._kv_cache_config = KVCacheConfig(**kv_cache_kwargs)
            setattr(self, config_name, model_config)

            if self._draft_model_config:
                kv_cache_kwargs["device_memory_utilization"] = draft_model_util
                self._draft_model_config._kv_cache_config = KVCacheConfig(
                    **kv_cache_kwargs
                )

        elif config_name == "_sampling_config":
            if hasattr(self, "_model_config") and self._model_config:
                assert isinstance(self._model_config, MAXModelConfig)
                assert hasattr(
                    config_class, "from_generation_config_sampling_defaults"
                )
                sampling_config = config_class.from_generation_config_sampling_defaults(
                    sampling_params_defaults=self._model_config.sampling_params_defaults,
                    **matched_kwargs,
                )
            else:
                sampling_config = config_class(**matched_kwargs)

            if self.enable_echo or self._draft_model_config:
                sampling_config.enable_variable_logits = True
            setattr(self, config_name, sampling_config)
        else:
            setattr(self, config_name, config_class(**matched_kwargs))

    def __init__(self, **kwargs: Any) -> None:
        # Initialize all fields with their defaults first
        for curr_field in fields(self.__class__):
            if curr_field.default is not MISSING:
                setattr(self, curr_field.name, curr_field.default)
            elif curr_field.default_factory is not MISSING:
                setattr(self, curr_field.name, curr_field.default_factory())

        # Process specialized config creation
        self._create_lora_config_if_needed(kwargs)
        self._create_draft_model_config_if_needed(kwargs)
        self._create_speculative_config_if_needed(kwargs)

        # Check if any kwargs are meant for other MAXConfig classes
        unmatched_kwargs: dict[str, Any] = {}
        # Then process kwargs which override defaults
        for key, value in list(kwargs.items()):
            if key in self.__dataclass_fields__:
                setattr(self, key, value)
                del kwargs[key]
            else:
                unmatched_kwargs[key] = value

        # Process remaining config classes
        if unmatched_kwargs:
            self._process_remaining_config_classes(unmatched_kwargs)

        # NOTE: Do not use this directly after instantiating PipelineConfig. We
        # only keep this here to support backward compatibility of the draft_model
        # field entrypoint. This will be removed entirely soon. I purposefully
        # set this to an empty string than None, to ensure that we catch any
        # inadvertent use of draft_model.
        self.draft_model = ""
        if unmatched_kwargs:
            raise ValueError(f"Unmatched kwargs: {unmatched_kwargs}")

        self.resolve()

    def retrieve_chat_template(self) -> str | None:
        # Read the file content
        if self.chat_template is None:
            return None

        try:
            with open(self.chat_template, encoding="utf-8") as f:
                template_content = f.read()

            # Try to parse as JSON and extract chat_template if present
            try:
                template_json = json.loads(template_content)
                if (
                    isinstance(template_json, dict)
                    and "chat_template" in template_json
                ):
                    logger.info(
                        f"Successfully loaded chat_template from JSON in {self.chat_template} "
                        f"({len(template_json['chat_template'])} characters)"
                    )
                    return template_json["chat_template"]
                else:
                    # JSON but no chat_template key, use entire content
                    logger.info(
                        f"Successfully loaded custom prompt template from {self.chat_template} "
                        f"({len(template_content)} characters, JSON without chat_template key)"
                    )
                    return template_content
            except json.JSONDecodeError:
                # Not valid JSON, use entire content as template
                logger.info(
                    f"Successfully loaded custom prompt template from {self.chat_template} "
                    f"({len(template_content)} characters)"
                )
                return template_content

        except (OSError, UnicodeDecodeError) as e:
            raise ValueError(
                f"Failed to read prompt template file {self.chat_template}: {str(e)}. "
                f"Please ensure the file is readable and contains valid UTF-8 text."
            ) from e

    def _resolve_chat_template(self) -> None:
        """
        Resolve chat_template if it's a Path object by reading the file content.

        This method handles the case where chat_template is a Path object,
        validates that the file exists, reads its content, and stores the content
        as a string in the chat_template field.

        Raises:
            FileNotFoundError: If the specified template file does not exist
            ValueError: If there's an error reading the template file
        """
        if self.chat_template is None:
            return

        # Expand user home directory if present (e.g., ~/templates/custom.jinja)
        self.chat_template = self.chat_template.expanduser()

        # Convert relative paths to absolute paths
        if not self.chat_template.is_absolute():
            self.chat_template = Path.cwd() / self.chat_template

        # Verify the file exists
        if not self.chat_template.exists():
            raise ValueError(
                f"--chat-template path ({self.chat_template}) does not exist."
            )

        if not self.chat_template.is_file():
            raise ValueError(
                f"Prompt template path is not a file: {self.chat_template}. "
                f"Please provide a path to a valid template file."
            )

    def _import_custom_architectures(self) -> None:
        """
        Import custom model modules to add them to the registry.
        """
        for module_spec in self.custom_architectures:
            module_parts = module_spec.split(":")
            if len(module_parts) > 2:
                raise ValueError(
                    f"Custom module spec contains too many colons: {module_spec}"
                )
            elif len(module_parts) == 2:
                module_path, module_name = module_parts
            else:
                module_path = os.path.dirname(module_parts[0])
                module_name = os.path.basename(module_parts[0])
            sys.path.append(module_path)
            try:
                module = importlib.import_module(module_name)
            except Exception as e:
                raise ValueError(
                    f"Failed to import custom model from: {module_spec}"
                ) from e

            if not module.ARCHITECTURES or not isinstance(
                module.ARCHITECTURES, list
            ):
                raise ValueError(
                    f"Custom model imported, but did not expose an `ARCHITECTURES` list. Module: {module_spec}"
                )

            for arch in module.ARCHITECTURES:
                PIPELINE_REGISTRY.register(arch, allow_override=True)

    def _validate_required_arguments_against_architecture(
        self, architecture: SupportedArchitecture
    ) -> None:
        """
        Validates and overrides config settings based on required_arguments from SupportedArchitecture.

        This method checks the required_arguments dictionary from the architecture
        and automatically overrides any config values that don't match, logging warnings
        when changes are made.

        Args:
            architecture: The SupportedArchitecture containing required_arguments dictionary
        """
        if not architecture.required_arguments:
            return

        config_objects = [
            ("PipelineConfig", self),
            ("MAXModelConfig", self._model_config),
            ("SamplingConfig", self._sampling_config),
            ("KVCacheConfig", self._model_config._kv_cache_config),
        ]

        # Add draft model configurations if present
        if self._draft_model_config is not None:
            config_objects.extend(
                [
                    ("Draft_MAXModelConfig", self._draft_model_config),
                    (
                        "Draft_KVCacheConfig",
                        self._draft_model_config._kv_cache_config,
                    ),
                ]
            )

        for arg_name, required_value in architecture.required_arguments.items():
            # Check each config object for the required argument
            for config_name, config_obj in config_objects:
                current_value = getattr(config_obj, arg_name, required_value)
                if current_value != required_value:
                    logger.warning(
                        f"Architecture '{architecture.name}' requires {config_name}.{arg_name}={required_value}, "
                        f"overriding current value {current_value}"
                    )
                    setattr(config_obj, arg_name, required_value)
                # We should be able to override this value for all config objects.
                continue

    def resolve(self) -> None:
        """
        Validates and resolves the config.

        This method is called after the config is initialized, to ensure that all
        config fields have been initialized to a valid state.
        """
        # Before anything else, import custom model modules to add them to the registry.
        self._import_custom_architectures()

        # Resolve chat_template if it's a Path
        self._resolve_chat_template()

        self.model_config.resolve()

        # Validate if a provided max_length is non-negative.
        if self.max_length is not None and self.max_length < 0:
            raise ValueError("max_length must be non-negative.")

        self._validate_and_resolve_max_num_steps()

        if (
            self.sampling_config.enable_structured_output
            and self.model_config.default_device_spec.device_type == "cpu"
        ):
            raise ValueError(
                "enable_structured_output is not currently supported on CPU."
            )

        if self.sampling_config.enable_penalties and self.draft_model_config:
            logger.warning(
                "frequency_penalty, presence_penalty and repetition_penalty are not currently supported with speculative decoding."
            )
            self.sampling_config.enable_penalties = False

        # Validate LoRA compatibility with model configuration
        if self._lora_config and self._lora_config.enable_lora:
            self.model_config.validate_lora_compatibility()

        # By this point, we should have a valid model_path.

        # Run Baseline Validation
        self._validate_and_resolve_remaining_pipeline_config(
            model_config=self.model_config
        )

        # Run Additional Checks for Speculative Decoding
        if self.draft_model_config:
            self._validate_and_resolve_remaining_pipeline_config(
                model_config=self.draft_model_config
            )

            self._validate_pipeline_config_for_speculative_decoding()

    def _validate_and_resolve_max_num_steps(self) -> None:
        """
        Validate and resolve the max_num_steps field. These are platform-specific.
        """
        if self.max_num_steps < 0:
            if self.model_config.default_device_spec == DeviceSpec.cpu():
                self.max_num_steps = 1
            else:
                self.max_num_steps = 10

    def _validate_pipeline_config_for_speculative_decoding(self) -> None:
        """
        Validate the pipeline configs when used in speculative decoding mode.
        """
        assert self.draft_model_config is not None
        assert self._speculative_config is not None

        # Validate that both the `draft_model` and target model `model_path` have the same
        # architecture
        draft_arch = PIPELINE_REGISTRY.retrieve_architecture(
            huggingface_repo=self.draft_model_config.huggingface_model_repo,
            use_module_v3=self.use_module_v3,
        )

        if not draft_arch:
            raise ValueError(
                "MAX-Optimized architecture not found for `draft_model`"
            )

        target_arch = PIPELINE_REGISTRY.retrieve_architecture(
            huggingface_repo=self.model_config.huggingface_model_repo,
            use_module_v3=self.use_module_v3,
        )
        if not target_arch:
            raise ValueError(
                "MAX-Optimized architecture not found for target model (`model_path`)"
            )

        # Validate that their tokenizers are identical.
        if self._speculative_config.is_standalone():
            if draft_arch != target_arch:
                raise ValueError(
                    f"architecture for the draft_model ({draft_arch.name}) does not match the architecture retrieved for the target model ({target_arch.name})"
                )

            draft_tokenizer = PIPELINE_REGISTRY.get_active_tokenizer(
                huggingface_repo=self.draft_model_config.huggingface_model_repo
            )
            target_tokenizer = PIPELINE_REGISTRY.get_active_tokenizer(
                huggingface_repo=self.model_config.huggingface_model_repo
            )

            # Compare Vocabularies
            if draft_tokenizer.get_vocab() != target_tokenizer.get_vocab():
                raise ValueError(
                    f"tokenizer for draft_model ({self.draft_model_config.model_path}) does not match the vocabulary of the tokenizer for the target model ({self.model_config.model_path})"
                )

            # Compare Tokenizer Configuration
            if hasattr(draft_tokenizer, "_tokenizer") and hasattr(
                target_tokenizer, "_tokenizer"
            ):
                if (
                    draft_tokenizer._tokenizer.__dict__
                    != target_tokenizer._tokenizer.__dict__
                ):
                    raise ValueError(
                        f"tokenizer for draft_model ({self.draft_model_config.model_path}) does not match the configuration of the tokenizer for the target model ({self.model_config.model_path})"
                    )
            else:
                if draft_tokenizer.__dict__ != target_tokenizer.__dict__:
                    raise ValueError(
                        f"tokenizer for draft_model ({self.draft_model_config.model_path}) does not match the configuration of the tokenizer for the target model ({self.model_config.model_path})"
                    )

        if self.enable_echo:
            raise ValueError(
                "enable_echo not currently supported with speculative decoding enabled"
            )

        if self.sampling_config.enable_structured_output:
            raise ValueError(
                "structured outputs not currently supported with speculative decoding enabled"
            )

        if (
            self.model_config.kv_cache_config.enable_prefix_caching
            and not self.force
        ):
            logging.warning(
                "Prefix caching is not supported with speculative decoding. "
                "Overriding user setting to False. Pass --force to bypass this "
                "validation, though this may result in unexpected behavior or errors."
            )
            self.model_config.kv_cache_config.enable_prefix_caching = False
            self.draft_model_config.kv_cache_config.enable_prefix_caching = (
                False
            )

    def _validate_and_resolve_remaining_pipeline_config(
        self, model_config: MAXModelConfig
    ) -> None:
        """Update remaining pipeline config fields with appropriate values
        if not provided. If invalid config is provided, error out with detailed
        reason."""
        # Retrieve the architecture
        arch = PIPELINE_REGISTRY.retrieve_architecture(
            huggingface_repo=model_config.huggingface_model_repo,
            use_module_v3=self.use_module_v3,
        )

        # If nothing is provided, we should not update any more params.
        if not arch:
            raise ValueError(
                f"MAX-optimized architecture not available for '{model_config.model_path}'. "
                "Please file a request at https://modul.ar/request to add this model architecture to MAX."
            )

        # Validate required arguments
        if not self.force:
            self._validate_required_arguments_against_architecture(arch)

        # Validate that model supports empty batches, if being requested.
        if self.execute_empty_batches and not arch.supports_empty_batches:
            raise ValueError(
                f"Architecture '{arch.name}' does not support empty batches. "
                "Please set `execute_empty_batches` to False."
            )

        devices = load_devices(model_config.device_specs)

        # Validate LoRA support - currently only Llama3 models support LoRA
        if self._lora_config and self._lora_config.enable_lora:
            # Check if the architecture is Llama3 (LlamaForCausalLM)
            if arch.name != "LlamaForCausalLM":
                raise ValueError(
                    f"LoRA is not currently supported for architecture '{arch.name}'. "
                    f"LoRA support is currently only available for Llama-3.x models (LlamaForCausalLM architecture). "
                    f"Model '{model_config.model_path}' uses the '{arch.name}' architecture."
                )
            # Currently, LoRA supported on only 1 device.
            if len(devices) > 1:
                raise ValueError(
                    "LoRA is currently not supported with the number of devices > 1."
                )

        # TODO(E2EOPT-28): remove this constraint.
        # Gemma has a MHA head size of 256.
        # This requires a kv cache page size of at least 256.
        if "Gemma3" in arch.name:
            model_config._kv_cache_config.kv_cache_page_size = max(
                model_config._kv_cache_config.kv_cache_page_size, 256
            )

        model_config.validate_multi_gpu_supported(
            multi_gpu_supported=arch.multi_gpu_supported
        )

        # We have now made sure that we have a valid SupportedArchitecture.
        # We should then validate the details of the existing architecture and
        # fallback to HuggingFace if needed.
        model_config.validate_and_resolve_quantization_encoding_weight_path(
            default_encoding=arch.default_encoding
        )

        model_config.validate_and_resolve_rope_type(
            arch_rope_type=arch.rope_type
        )

        # by this point, the quantization_encoding must be provided. verify it is supported.
        if model_config.quantization_encoding not in arch.supported_encodings:
            raise ValueError(
                f"quantization_encoding of '{model_config.quantization_encoding}' not supported by MAX engine."
            )
        model_config.validate_and_resolve_with_resolved_quantization_encoding(
            supported_encodings=arch.supported_encodings,
            default_weights_format=arch.default_weights_format,
        )

        # Resolve final pipeline-specific changes to the config before doing
        # memory estimations.
        arch.pipeline_model.finalize_pipeline_config(self)

        MemoryEstimator.estimate_memory_footprint(
            self,
            arch.pipeline_model,
            model_config,
            devices,
        )

        if clamped_max_seq_len := MemoryEstimator.max_supported_sequence_length(
            arch.pipeline_model, self, model_config, devices
        ):
            if self.max_length is None:
                self.max_length = clamped_max_seq_len
            elif self.max_length > clamped_max_seq_len:
                logging.warning(
                    f"Clamping max_length from {self.max_length} to {clamped_max_seq_len} due to capacity of KV Cache"
                )
                self.max_length = clamped_max_seq_len

        # Validate whether the architecture requires a max batch context length to be specified.
        # This needs to be done after max_length is resolved.
        if (
            arch.requires_max_batch_context_length
            and self.max_batch_context_length is None
        ):
            logger.warning(
                f"Architecture '{arch.name}' requires max-batch-context-length to be specified but found None. "
                f"Defaulting to the max sequence length of the model: {self.max_length}"
            )
            self.max_batch_context_length = self.max_length

    def __getstate__(self) -> dict[str, Any]:
        """Override `__getstate__` to exclude the Hugging Face config."""
        state = self.__dict__.copy()
        return state

    @property
    def graph_quantization_encoding(self) -> QuantizationEncoding | None:
        """Converts the CLI encoding to a MAX graph quantization encoding.

        Returns:
            The graph quantization encoding corresponding to the CLI encoding.
        """
        return self._model_config.graph_quantization_encoding

    def log_pipeline_info(self) -> None:
        """Log comprehensive pipeline and KVCache configuration information.

        Retrieves all necessary information from self and the PIPELINE_REGISTRY.
        Raises an error if architecture is not found (which should not happen after config resolution).
        """

        # Retrieve architecture - this should always exist after config resolution
        arch = PIPELINE_REGISTRY.retrieve_architecture(
            huggingface_repo=self.model_config.huggingface_model_repo,
            use_module_v3=self.use_module_v3,
        )

        if arch is None:
            raise ValueError(
                f"No architecture found for {self.model_config.huggingface_model_repo.repo_id}. "
                "This should not happen after config resolution."
            )

        # Get pipeline task and class information
        task = PIPELINE_REGISTRY.retrieve_pipeline_task(self)
        pipeline_class = get_pipeline_for_task(task, self)

        weights_repo_str = (
            f"\n            weights_repo_id        : {self.model_config._weights_repo_id}"
            if self.model_config._weights_repo_id
            else ""
        )

        devices_str = ", ".join(
            f"{d.device_type}[{d.id}]" for d in self.model_config.device_specs
        )

        quantization_encoding_str = str(self.model_config.quantization_encoding)
        if self.model_config._applied_dtype_cast_from:
            quantization_encoding_str = f"{quantization_encoding_str} (cast from {self.model_config._applied_dtype_cast_from})"

        # Helper function to log kvcache config details
        def _log_kvcache_details(
            config: KVCacheConfig, indent: str = "    "
        ) -> None:
            logger.info(
                f"{indent}cache_strategy         : {config.cache_strategy}"
            )
            logger.info(
                f"{indent}page_size              : {config.kv_cache_page_size} tokens"
            )
            logger.info(
                f"{indent}prefix_caching         : {config.enable_prefix_caching}"
            )
            logger.info(
                f"{indent}host_swapping          : {config.enable_kvcache_swapping_to_host}"
            )
            if config.enable_kvcache_swapping_to_host:
                logger.info(
                    f"{indent}host_swap_space        : {config.host_kvcache_swap_space_gb} GB"
                )
            logger.info(
                f"{indent}memory_utilization     : {config.device_memory_utilization:.1%}"
            )

            if config._available_cache_memory is None:
                raise ValueError(
                    "KVCache config is not available after config resolution."
                )
            logger.info(
                f"{indent}available_cache_memory : {to_human_readable_bytes(config._available_cache_memory)}"
            )

        # Log Pipeline and Model Information
        logger.info("")
        logger.info("Model Information")
        logger.info("=" * 60)
        logger.info(f"    architecture           : {arch.name}")
        logger.info(f"    pipeline_class         : {pipeline_class.__name__}")
        logger.info(
            f"    pipeline_model         : {arch.pipeline_model.__name__}"
        )
        logger.info(
            f"    tokenizer              : {arch.tokenizer_cls.__name__}"
        )
        logger.info(f"    devices                : {devices_str}")
        logger.info(
            f"    model_path             : {self.model_config.model_path}{weights_repo_str}"
        )
        logger.info(
            f"    huggingface_revision   : {self.model_config.huggingface_model_revision}"
        )
        logger.info(f"    quantization_encoding  : {quantization_encoding_str}")

        if len(self.model_config.weight_path) == 1:
            # Single weight path - format inline
            logger.info(
                f"    weight_path            : {self.model_config.weight_path[0]}"
            )
        elif len(self.model_config.weight_path) > 5:
            # Many weight paths - replace middle with "..."
            logger.info("    weight_path            : [")
            for path in (
                self.model_config.weight_path[:3]
                + ["..."]
                + [self.model_config.weight_path[-1]]
            ):
                logger.info(f"                              {path}")
            logger.info("                            ]")
        else:
            # Few weight paths - print all of them
            logger.info("    weight_path            : [")
            for path in self.model_config.weight_path:
                logger.info(f"                              {path}")
            logger.info("                            ]")

        logger.info("")
        logger.info("Pipeline Config")
        logger.info("=" * 60)
        logger.info(f"    max_seq_len            : {self.max_length}")
        logger.info(f"    max_batch_size         : {self.max_batch_size}")
        logger.info(
            f"    chunked_prefill        : {self.enable_chunked_prefill}"
        )
        logger.info(f"    prefill_chunk_size     : {self.prefill_chunk_size}")
        logger.info(
            f"    in_flight_batching     : {self.enable_in_flight_batching}"
        )
        logger.info("")

        # KVCache Configuration Summary
        logger.info("KVCache Config")
        logger.info("=" * 60)

        # Primary model kvcache config
        kv_config = self.model_config._kv_cache_config
        _log_kvcache_details(kv_config)

        # Draft model kvcache config (if using speculative decoding)
        if self.draft_model_config is not None:
            logger.info("")
            logger.info("Draft Model KVCache Configuration:")
            logger.info("-" * 40)
            draft_kv_config = self.draft_model_config._kv_cache_config
            _log_kvcache_details(draft_kv_config)

        logger.info("")

    def log_basic_config(self) -> None:
        """Log minimal pipeline configuration information.

        Logs basic PipelineConfig options including model name, pipeline task,
        weight path, max_batch_size, max_seq_len, and reserved memory.
        """
        # Retrieve architecture - this should always exist after config resolution
        arch = PIPELINE_REGISTRY.retrieve_architecture(
            huggingface_repo=self.model_config.huggingface_model_repo,
            use_module_v3=self.use_module_v3,
        )

        if arch is None:
            raise ValueError(
                f"No architecture found for {self.model_config.huggingface_model_repo.repo_id}. "
                "This should not happen after config resolution."
            )

        # Get pipeline task
        arch = PIPELINE_REGISTRY.retrieve_architecture(
            huggingface_repo=self.model_config.huggingface_model_repo,
            use_module_v3=self.use_module_v3,
        )
        if arch is None:
            raise ValueError(
                f"No architecture found for {self.model_config.huggingface_model_repo.repo_id}. "
                "This should not happen after config resolution."
            )

        task = PIPELINE_REGISTRY.retrieve_pipeline_task(self)
        pipeline_class = get_pipeline_for_task(task, self)

        # Get reserved memory info from KVCache config
        kv_config = self.model_config._kv_cache_config
        if kv_config._available_cache_memory is None:
            raise ValueError(
                "KVCache config is not available after config resolution."
            )
        memory_str = to_human_readable_bytes(kv_config._available_cache_memory)

        devices_str = ", ".join(
            f"{d.device_type}[{d.id}]" for d in self.model_config.device_specs
        )

        # Log basic configuration
        logger.info("")
        logger.info("=" * 60)
        logger.info(
            "Pipeline Configuration (use --pretty-print-config to print full config)"
        )
        logger.info("=" * 60)
        logger.info(f"    model              : {self.model_config.model_path}")
        logger.info(f"    architecture       : {arch.name}")
        logger.info(f"    pipeline           : {pipeline_class.__name__}")
        logger.info(f"    devices            : {devices_str}")
        logger.info(f"    max_batch_size     : {self.max_batch_size}")
        logger.info(f"    max_seq_len        : {self.max_length}")
        logger.info(f"    cache_memory       : {memory_str}")
        logger.info("")

    @staticmethod
    def help() -> dict[str, str]:
        return {
            "max_length": "Set the maximum sequence length for input data processed by the model. This must be less than the value specified in the Hugging Face configuration file. The default is derived from the Hugging Face configuration value. Larger values may consume more memory.",
            "pipeline_role": "Whether the pipeline should serve both a prefill or decode role or both.",
            "max_batch_size": "Define the maximum batch size to execute with the model. When not specified (None), we determine this value dynamically. For users launching in a server scenario, the expectation is that this value should be set higher based on server capacity.",
            "max_queue_size_tg": "Maximum number of requests in decode queue. By default, this is max-batch-size.",
            "min_batch_size_tg": "Specifies a soft floor on the decode batch size. If the TG batch size is larger than this value, the scheduler will continue to run TG batches. If it falls below, the scheduler will prioritize CE. This is an experimental flag solely for the TTS scheduler.",
            "ce_delay_ms": "Duration of scheduler sleep prior to starting a prefill batch. This is an experimental flag solely for the TTS scheduler. Default is 0.0.",
            "enable_prioritize_first_decode": "When enabled, the scheduler will always run a TG batch immediately after a CE batch, with the same requests. This may be useful for decreasing time-to-first-chunk latency. This is an experimental flag solely for the TTS scheduler. Default is false.",
            "experimental_background_queue": "When enabled, offloads queue draining to a background thread for improved performance. This is an experimental flag. Default is false.",
            "enable_chunked_prefill": "Enable chunked prefill to split context encoding requests into multiple chunks based on `prefill-chunk-size`. Default is true.",
            "enable_in_flight_batching": "When enabled, prioritizes token generation by batching it with context encoding requests. Default is false.",
            "max_num_steps": "Specify the number of steps to run for multi-step scheduling during inference. Default is -1 which specifies a default value based on configuration and platform. Ignored for models which are not auto-regressive (e.g. embedding models).",
            "prefill_chunk_size": "The target number of un-encoded tokens to include in each batch. This value is used for chunked prefill and memory estimation. Default is 8192.",
            "enable_echo": "Whether the model should be built with echo capabilities. This defaults to false.",
            "pool_embeddings": "Whether to pool embedding outputs. Default is true.",
            "use_experimental_kernels": "Whether to use experimental kernels. Default is false.",
            "max_batch_context_length": "Ensures that the sum of the context length in a batch does not exceed max_batch_context_length. If None, the sum of the context length in batch is not limited.",
            "pdl_level": "Level of overlap of kernel launch via programmatic dependent grid control. Default is 0.",
            "custom_architectures": "A list of custom architecture implementations to register. Each input can either be a raw module name or an import path followed by a colon and the module name.",
            "kvcache_ce_watermark": "Projected cache usage threshold for scheduling CE requests, considers current + incoming request. CE is scheduled if either projected usage stays below this threshold OR no active requests exist. Greater KVCache utilization (as controlled by this parameter) was found to cause more preemptions. Default watermark value is 0.95.",
        }

    @property
    def model_config(self) -> MAXModelConfig:
        return self._model_config

    @property
    def draft_model_config(self) -> MAXModelConfig | None:
        return self._draft_model_config

    @property
    def sampling_config(self) -> SamplingConfig:
        return self._sampling_config

    @property
    def profiling_config(self) -> ProfilingConfig:
        return self._profiling_config

    @property
    def lora_config(self) -> LoRAConfig | None:
        return self._lora_config


def _parse_flag_bool(value: str, flag_name: str) -> bool:
    if value.lower() == "true":
        return True
    elif value.lower() == "false":
        return False
    else:
        raise ValueError(
            f"Invalid boolean value: {value} for flag: {flag_name}"
        )


def _parse_flag_int(value: str, flag_name: str) -> int:
    try:
        return int(value)
    except ValueError as exc:
        raise ValueError(
            f"Invalid integer value: {value} for flag: {flag_name}"
        ) from exc


class PrependPromptSpeechTokens(str, Enum):
    NEVER = "never"
    """Never prepend the prompt speech tokens sent to the audio decoder."""

    ONCE = "once"
    """Prepend the prompt speech tokens to the first block of the audio decoder."""

    ROLLING = "rolling"
    """Prepend the prompt speech tokens to the first block of the audio decoder,
    and to later blocks to reach the requested buffer size."""


class PrometheusMetricsMode(str, Enum):
    INSTRUMENT_ONLY = "instrument_only"
    """Instrument metrics through the Prometheus client library, relying on the application to handle the metrics server."""

    LAUNCH_SERVER = "launch_server"
    """Launch a Prometheus server to handle metrics requests."""

    LAUNCH_MULTIPROC_SERVER = "launch_multiproc_server"
    """Launch a Prometheus server in multiprocess mode to report metrics."""


@dataclass
class AudioGenerationConfig(PipelineConfig):
    # TODO: Make these flags more discoverable.
    audio_decoder: str = ""
    """The name of the audio decoder model architecture."""

    audio_decoder_weights: str = ""
    """The path to the audio decoder weights file."""

    chunk_size: list[int] | None = None
    """The chunk sizes to use for streaming.
    If this is an int, then fixed-size chunks of the given size are used
    If this is a list, then variable chunk sizes are used."""

    buffer: int = 0
    """The number of previous speech tokens to pass to the audio decoder on
    each generation step."""

    block_causal: bool = False
    """Whether prior buffered tokens should attend to tokens in the current block.
    Has no effect if buffer is not set."""

    prepend_prompt_speech_tokens: PrependPromptSpeechTokens = (
        PrependPromptSpeechTokens.ONCE
    )
    """Whether the prompt speech tokens should be forwarded to the audio decoder.
    If "never", the prompt tokens are not forwarded.
    If "once", the prompt tokens are only forwarded on the first block.
    If "always", the prompt tokens are forwarded on all blocks.
    """

    prepend_prompt_speech_tokens_causal: bool = False
    """Whether the prompt speech tokens should attend to tokens in the currently
    generated audio block.
    Has no effect if prepend_prompt_speech_tokens is "never".
    If False (default), the prompt tokens do not attend to the current block.
    If True, the prompt tokens attend to the current block.
    """

    audio_decoder_config: dict[str, Any] = field(default_factory=dict)
    """Parameters to pass to the audio decoder model."""

    _run_model_test_mode: bool = False
    """Test-only flag that indicates that test parameters have been passed to
    the model, such as leaving the audio decoder weights empty or using a
    dummy speech language model."""

    prometheus_metrics_mode: PrometheusMetricsMode = (
        PrometheusMetricsMode.INSTRUMENT_ONLY
    )
    """The mode to use for Prometheus metrics."""

    def __init__(
        self,
        audio_decoder: str,
        audio_decoder_weights: str = "",
        chunk_size: list[int] | None = None,
        buffer: int = 0,
        block_causal: bool = False,
        prepend_prompt_speech_tokens: PrependPromptSpeechTokens = PrependPromptSpeechTokens.NEVER,
        prepend_prompt_speech_tokens_causal: bool = False,
        run_model_test_mode: bool = False,
        prometheus_metrics_mode: PrometheusMetricsMode = PrometheusMetricsMode.INSTRUMENT_ONLY,
        **kwargs: Any,
    ) -> None:
        # Must call the superclass's __init__ first, otherwise PipelineConfig's
        # init will override values defined in the AudioGenerationConfig.
        PipelineConfig.__init__(self, **kwargs)
        if block_causal:
            raise NotImplementedError("Causal generation is not implemented")
        if prepend_prompt_speech_tokens_causal:
            raise NotImplementedError(
                "Prepend prompt speech tokens causal is not implemented"
            )

        self.audio_decoder = audio_decoder
        self.audio_decoder_weights = audio_decoder_weights
        self.chunk_size = chunk_size
        self.buffer = buffer
        self.block_causal = block_causal
        self.prepend_prompt_speech_tokens = prepend_prompt_speech_tokens
        self.prepend_prompt_speech_tokens_causal = (
            prepend_prompt_speech_tokens_causal
        )
        self._run_model_test_mode = run_model_test_mode
        self.prometheus_metrics_mode = prometheus_metrics_mode

    @staticmethod
    def help() -> dict[str, str]:
        # Get the parent class help first
        audio_help = PipelineConfig.help()

        # Add AudioGenerationConfig-specific fields
        audio_specific_help = {
            "audio_decoder": "The name of the audio decoder model architecture.",
            "audio_decoder_weights": "The path to the audio decoder weights file.",
            "chunk_size": "The chunk sizes to use for streaming. If this is an int, then fixed-size chunks of the given size are used. If this is a list, then variable chunk sizes are used.",
            "buffer": "The number of previous speech tokens to pass to the audio decoder on each generation step. Default is 0.",
            "block_causal": "Whether prior buffered tokens should attend to tokens in the current block. Has no effect if buffer is not set. Default is false.",
            "prepend_prompt_speech_tokens": "Whether the prompt speech tokens should be forwarded to the audio decoder. Options: 'never', 'once', 'rolling'. Default is 'once'.",
            "prepend_prompt_speech_tokens_causal": "Whether the prompt speech tokens should attend to tokens in the currently generated audio block. Has no effect if prepend_prompt_speech_tokens is 'never'. Default is false.",
            "audio_decoder_config": "Parameters to pass to the audio decoder model.",
            "prometheus_metrics_mode": "The mode to use for Prometheus metrics. Default is 'instrument_only'.",
        }

        # Check for conflicts
        for key in audio_specific_help:
            if key in audio_help:
                raise ValueError(
                    f"Duplicate help key '{key}' found in AudioGenerationConfig"
                )

        # Merge the help dictionaries
        audio_help.update(audio_specific_help)
        return audio_help

    @classmethod
    def from_flags(
        cls, audio_flags: dict[str, str], **config_flags: Any
    ) -> AudioGenerationConfig:
        audio_decoder = audio_flags.pop("audio_decoder", "")
        if not audio_decoder:
            raise ValueError(
                "When running the audio generation task, --audio-decoder must be specified"
            )
        audio_decoder_weights = audio_flags.pop("audio_decoder_weights", "")

        # Configuration for audio generation streaming.
        chunk_size_str = audio_flags.pop("chunk_size", "")
        if not chunk_size_str:
            chunk_size = None
        else:
            chunk_size = [int(size) for size in chunk_size_str.split(",")]

        buffer = _parse_flag_int(audio_flags.pop("buffer", "0"), "buffer")

        block_causal = _parse_flag_bool(
            audio_flags.pop("block_causal", "false"), "block_causal"
        )

        prepend_prompt_speech_tokens = PrependPromptSpeechTokens(
            audio_flags.pop("prepend_prompt_speech_tokens", "never")
        )

        prepend_prompt_speech_tokens_causal = _parse_flag_bool(
            audio_flags.pop("prepend_prompt_speech_tokens_causal", "false"),
            "prepend_prompt_speech_tokens_causal",
        )

        run_model_test_mode = _parse_flag_bool(
            audio_flags.pop("run_model_test_mode", "false"),
            "run_model_test_mode",
        )

        prometheus_metrics_mode = PrometheusMetricsMode(
            audio_flags.pop("prometheus_metrics_mode", "instrument_only"),
        )

        if audio_flags:
            raise ValueError(
                f"Unknown audio generation option(s): {audio_flags}"
            )

        return cls(
            audio_decoder=audio_decoder,
            audio_decoder_weights=audio_decoder_weights,
            chunk_size=chunk_size,
            buffer=buffer,
            block_causal=block_causal,
            prepend_prompt_speech_tokens=prepend_prompt_speech_tokens,
            prepend_prompt_speech_tokens_causal=prepend_prompt_speech_tokens_causal,
            run_model_test_mode=run_model_test_mode,
            prometheus_metrics_mode=prometheus_metrics_mode,
            **config_flags,
        )


@dataclass
class PixelGenerationConfig(PipelineConfig):
    """Configuration for image and video generation pipelines.

    This config extends PipelineConfig to support diffusers-style pipelines
    with multi-folder repository structures and diffusion-specific parameters.
    """

    # Generation mode
    generation_type: PixelGenerationType = PixelGenerationType.TEXT_TO_IMAGE
    """The type of pixel generation to perform."""

    # Diffusion-specific parameters
    num_inference_steps: int = 50
    """Number of denoising steps."""

    guidance_scale: float = 7.5
    """Classifier-free guidance scale. Higher values produce outputs more
    aligned with the prompt but less diverse."""

    negative_prompt: str | None = None
    """Optional negative prompt for classifier-free guidance."""

    # Spatial dimensions
    height: int = 1024
    """Output image/video height in pixels."""

    width: int = 1024
    """Output image/video width in pixels."""

    # Video-specific parameters (only used when generation_type.outputs_video)
    num_frames: int | None = None
    """Number of frames for video generation. None = image mode (single frame)."""

    fps: int | None = None
    """Frames per second for video output. Only used for video generation."""

    num_videos_per_prompt: int = 1
    """Number of videos to generate per prompt (for video modes)."""

    motion_bucket_id: int | None = None
    """Motion strength for SVD-style video generation."""

    guidance_scale_2: float | None = None
    """Secondary guidance scale for two-stage models (e.g., Wan 2.2)."""

    use_dynamic_cfg: bool = False
    """Enable dynamic CFG for CogVideoX-style models."""

    # I2V-specific parameters (image-to-video)
    last_image: bool = False
    """Whether to use a last frame image for I2V generation (Wan I2V)."""

    # V2V-specific parameters (video-to-video)
    video_strength: float | None = None
    """Denoising strength for V2V. Higher = more change from input video."""

    # Image editing parameters (QwenImageEdit, etc.)
    true_cfg_scale: float | None = None
    """True CFG scale for image editing models (e.g., QwenImageEdit uses 4.0)."""

    # Conditioning parameters (img2img, inpainting)
    strength: float = 0.8
    """Denoising strength for img2img/inpainting. 1.0 = full denoise."""

    # Image-specific parameters
    num_images_per_prompt: int = 1
    """Number of images to generate per prompt."""

    # Scheduler
    scheduler_type: str | None = None
    """Optional scheduler override. Uses model default if not specified."""

    # Parsed diffusers repository config
    _diffusers_config: DiffusersConfig | None = None
    """Parsed model_index.json from a diffusers-style repository."""

    def __init__(
        self,
        generation_type: PixelGenerationType = PixelGenerationType.TEXT_TO_IMAGE,
        num_inference_steps: int = 50,
        guidance_scale: float = 7.5,
        negative_prompt: str | None = None,
        height: int = 1024,
        width: int = 1024,
        # Video params (optional)
        num_frames: int | None = None,
        fps: int | None = None,
        num_videos_per_prompt: int = 1,
        motion_bucket_id: int | None = None,
        guidance_scale_2: float | None = None,
        use_dynamic_cfg: bool = False,
        # I2V params
        last_image: bool = False,
        # V2V params
        video_strength: float | None = None,
        # Image editing params
        true_cfg_scale: float | None = None,
        # Conditioning params
        strength: float = 0.8,
        # Image params
        num_images_per_prompt: int = 1,
        scheduler_type: str | None = None,
        **kwargs: Any,
    ) -> None:
        # Must call the superclass's __init__ first
        PipelineConfig.__init__(self, **kwargs)

        self.generation_type = generation_type
        self.num_inference_steps = num_inference_steps
        self.guidance_scale = guidance_scale
        self.negative_prompt = negative_prompt
        self.height = height
        self.width = width
        self.num_frames = num_frames
        self.fps = fps
        self.num_videos_per_prompt = num_videos_per_prompt
        self.motion_bucket_id = motion_bucket_id
        self.guidance_scale_2 = guidance_scale_2
        self.use_dynamic_cfg = use_dynamic_cfg
        self.last_image = last_image
        self.video_strength = video_strength
        self.true_cfg_scale = true_cfg_scale
        self.strength = strength
        self.num_images_per_prompt = num_images_per_prompt
        self.scheduler_type = scheduler_type

        # Try to parse diffusers repo config if model_path points to one
        self._parse_diffusers_repo_if_needed()
        self._validate_pixel_config()

    def _parse_diffusers_repo_if_needed(self) -> None:
        """Parse model_index.json if the model_path is a diffusers-style repo."""
        if not self._model_config or not self._model_config.model_path:
            return

        model_path = Path(self._model_config.model_path)

        # Check for local diffusers repo
        if model_path.exists() and (model_path / "model_index.json").exists():
            try:
                self._diffusers_config = DiffusersConfig.from_model_path(
                    model_path
                )
                logger.info(
                    f"Parsed diffusers repo: {self._diffusers_config.pipeline_class} "
                    f"with components: {self._diffusers_config.component_names}"
                )
            except Exception as e:
                logger.warning(f"Failed to parse diffusers repo: {e}")
        # Check if it's a HuggingFace repo ID
        elif "/" in self._model_config.model_path and not model_path.exists():
            try:
                self._diffusers_config = DiffusersConfig.from_huggingface_repo(
                    self._model_config.model_path
                )
                logger.info(
                    f"Parsed diffusers repo from HF: {self._diffusers_config.pipeline_class} "
                    f"with components: {self._diffusers_config.component_names}"
                )
            except Exception as e:
                logger.debug(f"Model path is not a diffusers-style repo: {e}")

    def _validate_pixel_config(self) -> None:
        """Validate configuration based on generation type."""
        # Video generation requires num_frames to be set and > 1
        if self.generation_type.outputs_video:
            if self.num_frames is None or self.num_frames < 2:
                raise ValueError(
                    f"{self.generation_type.value} requires num_frames >= 2, "
                    f"got {self.num_frames}"
                )

        # V2V requires video_strength
        if self.generation_type == PixelGenerationType.VIDEO_TO_VIDEO:
            if self.video_strength is None:
                self.video_strength = 0.8  # Set default for V2V
            if not (0.0 < self.video_strength <= 1.0):
                raise ValueError(
                    f"video_strength must be in (0.0, 1.0] for V2V, "
                    f"got {self.video_strength}"
                )

        # Conditioning modes require valid strength
        if self.generation_type.requires_input_image:
            if not (0.0 < self.strength <= 1.0):
                raise ValueError(
                    f"strength must be in (0.0, 1.0] for {self.generation_type.value}, "
                    f"got {self.strength}"
                )

        # Image editing may use true_cfg_scale
        if self.generation_type == PixelGenerationType.IMAGE_EDITING:
            if self.true_cfg_scale is None:
                self.true_cfg_scale = 4.0  # QwenImageEdit default

        # Guidance scale validation
        if self.guidance_scale < 1.0:
            logger.warning(
                f"guidance_scale < 1.0 disables classifier-free guidance. "
                f"Current value: {self.guidance_scale}"
            )

    @property
    def is_video(self) -> bool:
        """Whether this config produces video output."""
        return self.generation_type.outputs_video

    @property
    def requires_input_image(self) -> bool:
        """Whether this config requires an input image."""
        return self.generation_type.requires_input_image

    @property
    def diffusers_repo_config(self) -> DiffusersConfig | None:
        """Get the parsed diffusers repository config."""
        return self._diffusers_config

    @staticmethod
    def help() -> dict[str, str]:
        # Get the parent class help first
        pixel_help = PipelineConfig.help()

        # Add PixelGenerationConfig-specific fields
        pixel_specific_help = {
            "generation_type": "The type of pixel generation: text_to_image, text_to_video, image_to_image, image_to_video, image_editing, video_to_video, inpainting, outpainting, or controlnet.",
            "num_inference_steps": "Number of denoising steps. More steps generally produce better quality but take longer. Default is 50.",
            "guidance_scale": "Classifier-free guidance scale. Higher values (7-15) produce outputs more aligned with the prompt. Default is 7.5.",
            "negative_prompt": "Optional negative prompt to guide generation away from certain concepts.",
            "height": "Output image/video height in pixels. Default is 512.",
            "width": "Output image/video width in pixels. Default is 512.",
            # Video params
            "num_frames": "Number of frames for video generation. Not used for image modes.",
            "fps": "Frames per second for video output. Only used for video modes.",
            "num_videos_per_prompt": "Number of videos to generate per prompt. Default is 1.",
            "motion_bucket_id": "Motion strength for SVD-style video generation.",
            "guidance_scale_2": "Secondary guidance scale for two-stage models (e.g., Wan 2.2).",
            "use_dynamic_cfg": "Enable dynamic CFG for CogVideoX-style models.",
            # I2V params
            "last_image": "Whether to use a last frame image for I2V generation.",
            # V2V params
            "video_strength": "Denoising strength for V2V. Higher = more change from input video.",
            # Image editing params
            "true_cfg_scale": "True CFG scale for image editing models (e.g., QwenImageEdit uses 4.0).",
            # Conditioning params
            "strength": "Denoising strength for img2img/inpainting. 1.0 means full denoise. Default is 0.8.",
            # Image params
            "num_images_per_prompt": "Number of images to generate per prompt. Default is 1.",
            "scheduler_type": "Optional scheduler override (e.g., 'ddim', 'euler'). Uses model default if not specified.",
        }

        # Check for conflicts
        for key in pixel_specific_help:
            if key in pixel_help:
                raise ValueError(
                    f"Duplicate help key '{key}' found in PixelGenerationConfig"
                )

        # Merge the help dictionaries
        pixel_help.update(pixel_specific_help)
        return pixel_help

    @classmethod
    def from_flags(
        cls, pixel_flags: dict[str, str], **config_flags: Any
    ) -> PixelGenerationConfig:
        """Create a PixelGenerationConfig from CLI flags."""
        generation_type = PixelGenerationType(
            pixel_flags.pop("generation_type", "text_to_image")
        )

        num_inference_steps = _parse_flag_int(
            pixel_flags.pop("num_inference_steps", "50"), "num_inference_steps"
        )

        guidance_scale = float(pixel_flags.pop("guidance_scale", "7.5"))

        negative_prompt = pixel_flags.pop("negative_prompt", None)
        if negative_prompt == "":
            negative_prompt = None

        height = _parse_flag_int(pixel_flags.pop("height", "512"), "height")
        width = _parse_flag_int(pixel_flags.pop("width", "512"), "width")

        # Video params (optional)
        num_frames_str = pixel_flags.pop("num_frames", None)
        num_frames = (
            _parse_flag_int(num_frames_str, "num_frames")
            if num_frames_str
            else None
        )

        fps_str = pixel_flags.pop("fps", None)
        fps = _parse_flag_int(fps_str, "fps") if fps_str else None

        num_videos_per_prompt = _parse_flag_int(
            pixel_flags.pop("num_videos_per_prompt", "1"),
            "num_videos_per_prompt",
        )

        motion_bucket_id_str = pixel_flags.pop("motion_bucket_id", None)
        motion_bucket_id = (
            _parse_flag_int(motion_bucket_id_str, "motion_bucket_id")
            if motion_bucket_id_str
            else None
        )

        guidance_scale_2_str = pixel_flags.pop("guidance_scale_2", None)
        guidance_scale_2 = (
            float(guidance_scale_2_str) if guidance_scale_2_str else None
        )

        use_dynamic_cfg = (
            pixel_flags.pop("use_dynamic_cfg", "false").lower() == "true"
        )

        # I2V params
        last_image = pixel_flags.pop("last_image", "false").lower() == "true"

        # V2V params
        video_strength_str = pixel_flags.pop("video_strength", None)
        video_strength = (
            float(video_strength_str) if video_strength_str else None
        )

        # Image editing params
        true_cfg_scale_str = pixel_flags.pop("true_cfg_scale", None)
        true_cfg_scale = (
            float(true_cfg_scale_str) if true_cfg_scale_str else None
        )

        # Conditioning params
        strength = float(pixel_flags.pop("strength", "0.8"))

        # Image params
        num_images_per_prompt = _parse_flag_int(
            pixel_flags.pop("num_images_per_prompt", "1"),
            "num_images_per_prompt",
        )

        scheduler_type = pixel_flags.pop("scheduler_type", None)
        if scheduler_type == "":
            scheduler_type = None

        if pixel_flags:
            raise ValueError(
                f"Unknown pixel generation option(s): {pixel_flags}"
            )

        return cls(
            generation_type=generation_type,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
            negative_prompt=negative_prompt,
            height=height,
            width=width,
            num_frames=num_frames,
            fps=fps,
            num_videos_per_prompt=num_videos_per_prompt,
            motion_bucket_id=motion_bucket_id,
            guidance_scale_2=guidance_scale_2,
            use_dynamic_cfg=use_dynamic_cfg,
            last_image=last_image,
            video_strength=video_strength,
            true_cfg_scale=true_cfg_scale,
            strength=strength,
            num_images_per_prompt=num_images_per_prompt,
            scheduler_type=scheduler_type,
            **config_flags,
        )
