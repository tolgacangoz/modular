# ===----------------------------------------------------------------------=== #
# Copyright (c) 2026, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
# mypy: disable-error-code="import-not-found"
"""Pixel generation tokenizer implementation."""

from __future__ import annotations

import asyncio
import base64
import logging
from collections.abc import Callable
from enum import Enum
from io import BytesIO
from typing import TYPE_CHECKING, Any

import numpy as np
import numpy.typing as npt
import PIL.Image
from max.interfaces import (
    PipelineTokenizer,
    TokenBuffer,
)
from max.interfaces.request import OpenResponsesRequest
from max.interfaces.request.open_responses import (
    InputImageContent,
    InputTextContent,
)
from max.pipelines.core import PixelContext
from transformers import AutoTokenizer

from .diffusion_schedulers import SchedulerFactory

if TYPE_CHECKING:
    import PIL.Image
    from max.pipelines.lib.config import PipelineConfig

logger = logging.getLogger("max.pipelines")


async def run_with_default_executor(
    fn: Callable[..., Any], *args: Any, **kwargs: Any
) -> Any:
    """Runs a callable in the default thread pool executor.

    Args:
        fn: Callable to run.
        *args: Positional arguments for ``fn``.
        **kwargs: Keyword arguments for ``fn``.

    Returns:
        The result of ``fn(*args, **kwargs)``.
    """
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(None, fn, *args, **kwargs)


class PipelineClassName(str, Enum):
    FLUX = "FluxPipeline"
    FLUX2 = "Flux2Pipeline"
    ZIMAGE = "ZImagePipeline"
    LTX2 = "LTX2Pipeline"

    @classmethod
    def from_diffusers_config(
        cls, diffusers_config: dict[str, Any]
    ) -> PipelineClassName:
        """Resolve a PipelineClassName from a diffusers config dict."""
        raw = diffusers_config.get("_class_name")
        if raw is None:
            raise KeyError(
                "diffusers_config is missing required key '_class_name'."
            )
        try:
            return cls(raw)
        except ValueError as e:
            allowed = ", ".join([m.value for m in cls])
            raise ValueError(
                f"Unsupported _class_name={raw!r}. Allowed: {allowed}"
            ) from e


class PixelGenerationTokenizer(
    PipelineTokenizer[
        PixelContext,
        tuple[npt.NDArray[np.int64], npt.NDArray[np.bool_]],
        OpenResponsesRequest,
    ]
):
    """Encapsulates creation of PixelContext and specific token encode/decode logic.

    Args:
        model_path: Path to the model/tokenizer.
        pipeline_config: Pipeline configuration (must include diffusers_config).
        subfolder: Subfolder within the model path for the primary tokenizer.
        subfolder_2: Optional subfolder for a second tokenizer (e.g. text encoder).
        revision: Git revision/branch to use.
        max_length: Maximum sequence length for the primary tokenizer.
        secondary_max_length: Maximum sequence length for the secondary tokenizer, if used.
        trust_remote_code: Whether to trust remote code from the model.
        context_validators: Optional list of validators to run on PixelContext.
    """

    def __init__(
        self,
        model_path: str,
        pipeline_config: PipelineConfig,
        subfolder: str,
        *,
        subfolder_2: str | None = None,
        revision: str | None = None,
        max_length: int | None = None,
        secondary_max_length: int | None = None,
        trust_remote_code: bool = False,
        context_validators: list[Callable[[PixelContext], None]] | None = None,
        **unused_kwargs,
    ) -> None:
        self.model_path = model_path

        if max_length is None:
            raise ValueError(
                "diffusion models frequently have an unbounded max length. Please provide a max length"
            )

        self.max_length = max_length

        if secondary_max_length is None and subfolder_2 is not None:
            raise ValueError(
                "diffusion models frequently have an unbounded max length. Please provide a max length"
            )

        self.secondary_max_length = secondary_max_length

        try:
            self.delegate = AutoTokenizer.from_pretrained(
                model_path,
                revision=revision,
                trust_remote_code=trust_remote_code,
                model_max_length=self.max_length,
                subfolder=subfolder,
            )

            # Gemma expects left padding for chat-style prompts
            self.delegate.padding_side = "left"
            if self.delegate.pad_token is None:
                self.delegate.pad_token = self.delegate.eos_token

            if subfolder_2 is not None:
                self.delegate_2 = AutoTokenizer.from_pretrained(
                    model_path,
                    revision=revision,
                    trust_remote_code=trust_remote_code,
                    model_max_length=self.secondary_max_length,
                    subfolder=subfolder_2,
                )
            else:
                self.delegate_2 = None
        except Exception as e:
            raise ValueError(
                f"Failed to load tokenizer from {model_path}. "
                "This can happen if:\n"
                "- The model is not fully supported by the transformers python package\n"
                "- Required configuration files are missing\n"
                "- The model path is incorrect\n"
                "- '--trust-remote-code' is needed but not set\n"
            ) from e

        self._context_validators = (
            context_validators if context_validators else []
        )

        # Extract diffusers_config
        if not pipeline_config or not hasattr(
            pipeline_config.model, "diffusers_config"
        ):
            raise ValueError(
                "pipeline_config.model.diffusers_config is required for PixelGenerationTokenizer. "
                "Please provide a pipeline_config with a valid diffusers_config."
            )
        if pipeline_config.model.diffusers_config is None:
            raise ValueError(
                "pipeline_config.model.diffusers_config cannot be None. "
                "Please provide a valid diffusers_config."
            )
        self.diffusers_config = pipeline_config.model.diffusers_config

        # Store the pipeline class name for model-specific behavior
        self._pipeline_class_name = PipelineClassName.from_diffusers_config(
            self.diffusers_config
        )

        # Extract static config values once during initialization
        components = self.diffusers_config.get("components", {})
        vae_config = components.get("vae", {}).get("config_dict", {})
        transformer_config = components.get("transformer", {}).get(
            "config_dict", {}
        )

        # Compute static VAE scale factor
        block_out_channels = vae_config.get("block_out_channels", None)
        self._vae_scale_factor = (
            2 ** (len(block_out_channels) - 1) if block_out_channels else 8
        )
        self._vae_temporal_compression_ratio = vae_config.get(
            "temporal_compression_ratio"
        )

        # Store static model dimensions
        self._default_sample_size = 128
        self._num_channels_latents = transformer_config["in_channels"] // 4

        # Store guidance embeds flag
        self._use_guidance_embeds = transformer_config.get(
            "guidance_embeds", False
        )

        # LTX2-specific constants (video+audio) used when _pipeline_class_name == "LTX2Pipeline".
        # These match the current MAX LTX2 pipeline implementation.
        self._is_ltx2 = self._pipeline_class_name == "LTX2Pipeline"
        if self._is_ltx2:
            # LTX-2 uses CausalVideoAutoencoder with no standard block_out_channels;
            # read spatial/temporal compression ratios directly from the VAE config.
            self._vae_scale_factor = vae_config.get(
                "spatial_compression_ratio", 32
            )
            # LTX-2 uses patch_size=1 (no spatial packing), so VAE latent
            # channels == transformer in_channels (128), not in_channels // 4.
            self._num_channels_latents = transformer_config["in_channels"]
            # VAE temporal downsample factor (frames per latent frame).
            # Audio configuration: read from audio_vae config with safe fallbacks.
            audio_vae_config = components.get("audio_vae", {}).get(
                "config_dict", {}
            )
            self._ltx2_audio_sampling_rate = audio_vae_config.get(
                "sample_rate", 16_000
            )
            self._ltx2_audio_hop_length = audio_vae_config.get(
                "mel_hop_length", 160
            )
            self._ltx2_num_mel_bins = audio_vae_config.get("mel_bins", 64)
            # Mel compression = 2^(num_downsampling_stages); ch_mult has one entry per
            # resolution level, so num_downsampling_stages = len(ch_mult) - 1.
            ch_mult = audio_vae_config.get("ch_mult", [1, 2, 4])
            self._ltx2_audio_mel_compression_ratio = 2 ** (len(ch_mult) - 1)

        # Create scheduler
        scheduler_class_name = components.get("scheduler", {}).get(
            "class_name", None
        )
        scheduler_cfg = components.get("scheduler", {}).get("config_dict", {})
        scheduler_cfg["use_empirical_mu"] = (
            self._pipeline_class_name == PipelineClassName.FLUX2
        )
        self._scheduler = SchedulerFactory.create(
            class_name=scheduler_class_name,
            config_dict=scheduler_cfg,
        )

        self._max_pixel_size = None
        if self._pipeline_class_name == PipelineClassName.FLUX2:
            self._max_pixel_size = 1024 * 1024

    def _prepare_ltx2_video_coords(
        self,
        batch_size: int,
        latent_num_frames: int,
        latent_height: int,
        latent_width: int,
        frame_rate: float,
    ) -> npt.NDArray[np.float32]:
        """Pure-numpy equivalent of LTX2AudioVideoRotaryPosEmbed.prepare_video_coords.

        Returns float32 array of shape [batch_size, 3, num_patches, 2] containing
        per-patch pixel-space [start, end) boundaries for the (frame, height, width)
        dimensions, scaled to seconds on the temporal axis.
        """
        # LTX2 constants (patch sizes are always 1 for this model).
        patch_size_t: int = 1
        patch_size: int = 1
        # scale_factors converts latent → pixel space: (temporal, height, width).
        scale_f: int = self._vae_temporal_compression_ratio  # 8
        scale_h: int = self._vae_scale_factor  # 32
        scale_w: int = self._vae_scale_factor  # 32
        causal_offset: int = 1

        # 1. 1-D grids for each spatial/temporal dimension.
        grid_f = np.arange(0, latent_num_frames, patch_size_t, dtype=np.float32)
        grid_h = np.arange(0, latent_height, patch_size, dtype=np.float32)
        grid_w = np.arange(0, latent_width, patch_size, dtype=np.float32)

        # 2. Broadcast to 3-D grid [N_F, N_H, N_W] then stack → [3, N_F, N_H, N_W].
        grid_f_3d = np.broadcast_to(
            grid_f[:, None, None], (len(grid_f), len(grid_h), len(grid_w))
        )
        grid_h_3d = np.broadcast_to(
            grid_h[None, :, None], (len(grid_f), len(grid_h), len(grid_w))
        )
        grid_w_3d = np.broadcast_to(
            grid_w[None, None, :], (len(grid_f), len(grid_h), len(grid_w))
        )
        grid = np.stack(
            [grid_f_3d, grid_h_3d, grid_w_3d], axis=0
        )  # [3, N_F, N_H, N_W]

        # 3. Patch [start, end) boundaries → [3, N_F, N_H, N_W, 2] → [3, num_patches, 2].
        patch_delta = np.array(
            [patch_size_t, patch_size, patch_size], dtype=np.float32
        ).reshape(3, 1, 1, 1)
        patch_ends = grid + patch_delta
        latent_coords = np.stack(
            [grid, patch_ends], axis=-1
        )  # [3, N_F, N_H, N_W, 2]
        latent_coords = latent_coords.reshape(3, -1, 2)  # [3, num_patches, 2]

        # 4. Tile for batch → [B, 3, num_patches, 2].
        latent_coords = np.tile(latent_coords[None], (batch_size, 1, 1, 1))

        # 5. Convert latent → pixel-space coordinates.
        scale = np.array([scale_f, scale_h, scale_w], dtype=np.float32).reshape(
            1, 3, 1, 1
        )
        pixel_coords = latent_coords * scale

        # 6. Causal temporal fix-up: the first latent frame covers fewer pixel frames.
        pixel_coords[:, 0] = np.clip(
            pixel_coords[:, 0] + causal_offset - scale_f, 0, None
        )

        # 7. Convert pixel frames → seconds.
        pixel_coords[:, 0] /= frame_rate

        return pixel_coords  # float32, [B, 3, num_patches, 2]

    def _prepare_ltx2_audio_coords(
        self,
        batch_size: int,
        audio_num_frames: int,
    ) -> npt.NDArray[np.float32]:
        """Pure-numpy equivalent of LTX2AudioVideoRotaryPosEmbed.prepare_audio_coords.

        Returns float32 array of shape [batch_size, 1, num_patches, 2] containing
        per-patch [start, end) timestamps in seconds for the temporal dimension.
        """
        audio_scale_factor: int = self._ltx2_audio_mel_compression_ratio  # 4
        audio_patch_size_t: int = 1
        causal_offset: int = 1

        grid_f = np.arange(
            0, audio_num_frames, audio_patch_size_t, dtype=np.float32
        )

        # Start timestamps in seconds.
        grid_start_mel = np.clip(
            grid_f * audio_scale_factor + causal_offset - audio_scale_factor,
            0,
            None,
        )
        grid_start_s = (
            grid_start_mel
            * self._ltx2_audio_hop_length
            / self._ltx2_audio_sampling_rate
        )

        # End timestamps in seconds.
        grid_end_mel = np.clip(
            (grid_f + audio_patch_size_t) * audio_scale_factor
            + causal_offset
            - audio_scale_factor,
            0,
            None,
        )
        grid_end_s = (
            grid_end_mel
            * self._ltx2_audio_hop_length
            / self._ltx2_audio_sampling_rate
        )

        audio_coords = np.stack(
            [grid_start_s, grid_end_s], axis=-1
        )  # [num_patches, 2]
        # Tile for batch and add modality dimension → [B, 1, num_patches, 2].
        audio_coords = np.tile(audio_coords[None, None], (batch_size, 1, 1, 1))

        return audio_coords  # float32, [B, 1, num_patches, 2]

    def _prepare_latent_image_ids(
        self, height: int, width: int, batch_size: int = 1
    ) -> npt.NDArray[np.float32]:
        if self._pipeline_class_name == PipelineClassName.FLUX2:
            # Create 4D coordinates using numpy (T=0, H, W, L=0)
            t_coords, h_coords, w_coords, l_coords = np.meshgrid(
                np.array([0]),  # T dimension
                np.arange(height),  # H dimension
                np.arange(width),  # W dimension
                np.array([0]),  # L dimension
                indexing="ij",
            )
            latent_image_ids = np.stack(
                [t_coords, h_coords, w_coords, l_coords], axis=-1
            )
            latent_image_ids = latent_image_ids.reshape(-1, 4)

            latent_image_ids = np.tile(
                latent_image_ids[np.newaxis, :, :], (batch_size, 1, 1)
            )
            return latent_image_ids
        else:
            latent_image_ids = np.zeros((height, width, 3))
            latent_image_ids[..., 1] = (
                latent_image_ids[..., 1] + np.arange(height)[:, None]
            )
            latent_image_ids[..., 2] = (
                latent_image_ids[..., 2] + np.arange(width)[None, :]
            )
            return latent_image_ids.reshape(
                -1, latent_image_ids.shape[-1]
            ).astype(np.float32)

    def _randn_tensor(
        self,
        shape: tuple[int, ...],
        seed: int | None,
    ) -> npt.NDArray[np.float32]:
        rng = np.random.RandomState(seed)
        return rng.standard_normal(shape).astype(np.float32)

    def _preprocess_input_image(
        self,
        image: PIL.Image.Image | npt.NDArray[np.uint8],
        target_height: int | None = None,
        target_width: int | None = None,
    ) -> PIL.Image.Image:
        """Preprocess input image for image-to-image generation.

        This method preprocesses images for condition-based image-to-image generation.
        Matching diffusers behavior: resizes large images, ensures dimensions are multiples
        of vae_scale_factor * 2, and optionally resizes to target dimensions.

        Note: This is a simplified version compared to pipeline_flux2.py which uses
        image_processor.preprocess. This tokenizer-level preprocessing is sufficient
        for the Max framework's condition-based approach.

        Args:
            image: PIL Image or numpy array (uint8) to preprocess.
            target_height: Target height for the image. If None, uses image's height.
            target_width: Target width for the image. If None, uses image's width.

        Returns:
            Preprocessed PIL Image with adjusted dimensions.
        """
        import PIL.Image

        if isinstance(image, np.ndarray):
            image = PIL.Image.fromarray(image.astype(np.uint8))

        image_width, image_height = image.size
        multiple_of = self._vae_scale_factor * 2

        if self._max_pixel_size is not None:
            if image_width * image_height > self._max_pixel_size:
                scale = (
                    self._max_pixel_size / (image_width * image_height)
                ) ** 0.5
                new_width = int(image_width * scale)
                new_height = int(image_height * scale)
                image = image.resize(
                    (new_width, new_height), PIL.Image.Resampling.LANCZOS
                )
                image_width, image_height = image.size

        image_width = (image_width // multiple_of) * multiple_of
        image_height = (image_height // multiple_of) * multiple_of

        if target_height is not None:
            image_height = (target_height // multiple_of) * multiple_of
        if target_width is not None:
            image_width = (target_width // multiple_of) * multiple_of

        if image.size != (image_width, image_height):
            image = image.resize(
                (image_width, image_height), PIL.Image.Resampling.LANCZOS
            )

        return image

    def _prepare_latents(
        self,
        batch_size: int,
        num_channels_latents: int,
        latent_height: int,
        latent_width: int,
        seed: int | None,
        num_frames: int | None = None,
    ) -> tuple[npt.NDArray[np.float32], npt.NDArray[np.float32]]:
        shape = (batch_size, num_channels_latents, latent_height, latent_width)
        if num_frames is not None:
            num_latent_frames = (
                num_frames - 1
            ) // self._vae_temporal_compression_ratio + 1
            shape = (
                batch_size,
                num_channels_latents,
                num_latent_frames,
                latent_height,
                latent_width,
            )
        latents = self._randn_tensor(shape, seed)
        latent_image_ids = self._prepare_latent_image_ids(
            latent_height // 2, latent_width // 2, batch_size
        )

        return latents, latent_image_ids

    async def _generate_tokens_ids(
        self,
        prompt: str,
        prompt_2: str | None = None,
        negative_prompt: str | None = None,
        negative_prompt_2: str | None = None,
        do_true_cfg: bool = False,
        images: list[PIL.Image.Image] | None = None,
    ) -> tuple[
        npt.NDArray[np.int64],
        npt.NDArray[np.bool_],
        npt.NDArray[np.int64] | None,
        npt.NDArray[np.int64] | None,
        npt.NDArray[np.int64] | None,
    ]:
        """Tokenize prompt(s) with encoder model(s).

        Args:
            prompt: Primary prompt to tokenize.
            prompt_2: Secondary prompt (optional).
            negative_prompt: Negative prompt (optional).
            negative_prompt_2: Secondary negative prompt (optional).
            do_true_cfg: Whether to use true classifier-free guidance.
            images: Optional list of images for image-to-image generation (Flux2 only).

        Returns:
            Tuple of (token_ids, attn_mask, token_ids_2, negative_token_ids, negative_token_ids_2).
            token_ids_2 and negative_token_ids_2 are None if no secondary tokenizer is configured.
        """
        token_ids, attn_mask = await self.encode(prompt, images=images)

        token_ids_2: npt.NDArray[np.int64] | None = None
        if self.delegate_2 is not None:
            token_ids_2, _attn_mask_2 = await self.encode(
                prompt_2 or prompt,
                use_secondary=True,
            )

        negative_token_ids: npt.NDArray[np.int64] | None = None
        negative_token_ids_2: npt.NDArray[np.int64] | None = None
        attn_mask_neg: npt.NDArray[np.bool_] | None = None
        if do_true_cfg:
            negative_token_ids, attn_mask_neg = await self.encode(
                negative_prompt or ""
            )
            if self.delegate_2 is not None:
                negative_token_ids_2, _attn_mask_neg_2 = await self.encode(
                    negative_prompt_2 or negative_prompt or "",
                    use_secondary=True,
                )

        return (
            token_ids,
            attn_mask,
            attn_mask_neg,
            token_ids_2,
            negative_token_ids,
            negative_token_ids_2,
        )

    @property
    def eos(self) -> int:
        """Returns the end-of-sequence token ID."""
        return self.delegate.eos_token_id

    @property
    def expects_content_wrapping(self) -> bool:
        """Returns whether this tokenizer expects content wrapping."""
        return False

    async def encode(
        self,
        prompt: str,
        add_special_tokens: bool = True,
        *,
        use_secondary: bool = False,
        images: list[PIL.Image.Image] | None = None,
    ) -> tuple[npt.NDArray[np.int64], npt.NDArray[np.bool_]]:
        """Transforms the provided prompt into a token array."""
        delegate = self.delegate_2 if use_secondary else self.delegate
        max_sequence_length = (
            self.secondary_max_length if use_secondary else self.max_length
        )

        tokenizer_output: Any

        # Check if this is Flux2 pipeline (uses Mistral3Tokenizer with chat_template)
        # Flux2 requires apply_chat_template for proper tokenization

        def _encode_fn(prompt_str: str) -> Any:
            assert delegate is not None

            # For Flux2, use apply_chat_template with format_input
            if self._pipeline_class_name == PipelineClassName.FLUX2:
                from max.pipelines.architectures.flux2.system_messages import (
                    SYSTEM_MESSAGE,
                    format_input,
                )

                messages_batch = format_input(
                    prompts=[prompt_str],
                    system_message=SYSTEM_MESSAGE,
                    images=None,
                )

                return delegate.apply_chat_template(
                    messages_batch[0],
                    add_generation_prompt=False,
                    tokenize=True,
                    return_dict=True,
                    padding="max_length",
                    truncation=True,
                    max_length=max_sequence_length,
                    return_length=False,
                    return_overflowing_tokens=False,
                )
            else:
                return delegate(
                    prompt_str,
                    padding="max_length",
                    max_length=max_sequence_length,
                    truncation=True,
                    add_special_tokens=add_special_tokens,
                )

        # Note: the underlying tokenizer may not be thread safe in some cases, see https://github.com/huggingface/tokenizers/issues/537
        # Add a standard (non-async) lock in the executor thread if needed.
        tokenizer_output = await run_with_default_executor(_encode_fn, prompt)

        # Extract input_ids and attention_mask
        if isinstance(tokenizer_output, dict):
            # apply_chat_template returns a dict
            input_ids = tokenizer_output["input_ids"]
            attention_mask = tokenizer_output.get("attention_mask", None)
            if attention_mask is None:
                attention_mask = [1] * len(input_ids)

            # Extract real tokens only (using attention mask) for Flux2
            if self._pipeline_class_name == PipelineClassName.FLUX2:
                # Filter to keep only real tokens (where mask == 1)
                real_token_ids = [
                    token_id
                    for token_id, mask in zip(
                        input_ids[0], attention_mask[0], strict=False
                    )
                    if mask == 1
                ]
                input_ids = [real_token_ids]
                attention_mask = [[1] * len(real_token_ids)]
        else:
            # Standard tokenizer output
            input_ids = tokenizer_output.input_ids
            attention_mask = tokenizer_output.attention_mask

        if max_sequence_length and len(input_ids) > max_sequence_length:
            raise ValueError(
                f"Input string is larger than tokenizer's max length ({len(input_ids)} > {max_sequence_length})."
            )

        encoded_prompt = np.array(input_ids)
        attention_mask_array = np.array(attention_mask).astype(np.bool_)

        return encoded_prompt, attention_mask_array

    async def decode(
        self,
        encoded: tuple[npt.NDArray[np.int64], npt.NDArray[np.bool_]],
        **kwargs,
    ) -> str:
        """Decodes token arrays to text (not implemented for this tokenizer)."""
        raise NotImplementedError(
            "Decoding is not implemented for this tokenizer."
        )

    async def postprocess(
        self,
        output: Any,
    ) -> Any:
        """Post-process pipeline output.

        Accepts either a raw numpy array or a GenerationOutput.
        For raw numpy arrays, denormalizes from [-1, 1] to [0, 1].
        For GenerationOutput, returns as-is (denormalization is handled
        in the pipeline variant before encoding to OutputImageContent).
        """
        from max.interfaces.generation import GenerationOutput

        if isinstance(output, GenerationOutput):
            return output

        # Raw numpy path
        pixel_data = (output * 0.5 + 0.5).clip(min=0.0, max=1.0)
        return pixel_data

    @staticmethod
    def _retrieve_prompt(request: OpenResponsesRequest) -> str:
        """Retrieve the text prompt from an OpenResponsesRequest.

        Supports three input formats:
        1. input is a string - use directly as prompt
        2. input is a list of messages where first message content is a string - use as prompt
        3. input is a list of messages where first message content is a list - extract InputTextContent.text

        Args:
            request: The OpenResponsesRequest to extract the prompt from.

        Returns:
            The extracted text prompt.

        Raises:
            ValueError: If no valid prompt can be extracted from the request.
        """
        # Case 1: input is a string
        if isinstance(request.body.input, str):
            return request.body.input

        # Cases 2 & 3: input is a list of messages
        if isinstance(request.body.input, list):
            if not request.body.input:
                raise ValueError("Input message list cannot be empty.")

            first_message = request.body.input[0]

            # Case 2: message.content is a string
            if isinstance(first_message.content, str):
                return first_message.content

            # Case 3: message.content is a list
            if isinstance(first_message.content, list):
                # Extract text from all InputTextContent items
                text_parts = [
                    item.text
                    for item in first_message.content
                    if isinstance(item, InputTextContent)
                ]
                if not text_parts:
                    raise ValueError(
                        "No text content found in message. Please include at least one "
                        "InputTextContent item with a text prompt."
                    )
                return " ".join(text_parts)

            raise ValueError(
                f"Unexpected message content type: {type(first_message.content).__name__}"
            )

        raise ValueError(
            f"Input must be a string or list of messages, got {type(request.body.input).__name__}"
        )

    @staticmethod
    def _retrieve_image(
        request: OpenResponsesRequest,
    ) -> PIL.Image.Image | None:
        """Retrieve the input image from an OpenResponsesRequest.

        Extracts InputImageContent from the first message's content list and converts
        the data URI to a PIL Image.

        Args:
            request: The OpenResponsesRequest to extract the image from.

        Returns:
            PIL Image if found, None otherwise.
        """
        # Only check list inputs
        if not isinstance(request.body.input, list):
            return None

        if not request.body.input:
            return None

        first_message = request.body.input[0]

        # Only check list content
        if not isinstance(first_message.content, list):
            return None

        # Find first InputImageContent item
        for item in first_message.content:
            if isinstance(item, InputImageContent):
                # Parse data URI and convert to PIL Image
                image_url = item.image_url
                if image_url.startswith("data:"):
                    # Extract base64 data from data URI
                    # Format: data:image/png;base64,<base64_data>
                    _, base64_data = image_url.split(",", 1)
                    image_bytes = base64.b64decode(base64_data)
                    return PIL.Image.open(BytesIO(image_bytes))

        return None

    async def new_context(
        self,
        request: OpenResponsesRequest,
        input_image: PIL.Image.Image | None = None,
    ) -> PixelContext:
        """Create a new PixelContext object, leveraging necessary information from OpenResponsesRequest."""
        # Extract prompt from request using the helper method
        prompt = self._retrieve_prompt(request)
        if not prompt:
            raise ValueError("Prompt must be a non-empty string.")

        # Extract input image from request content (takes precedence over input_image parameter)
        input_image = self._retrieve_image(request) or input_image

        # Extract image provider options (always available via defaults)
        image_options = request.body.provider_options.image
        video_options = request.body.provider_options.video

        # For LTX-2 and other video models, we should consolidate options from both image and video
        # Prioritize video options if present, then image options.
        neg_prompt = (
            video_options.negative_prompt if video_options else None
        ) or (image_options.negative_prompt if image_options else None)
        sec_prompt = image_options.secondary_prompt if image_options else None
        sec_neg_prompt = (
            image_options.secondary_negative_prompt if image_options else None
        )
        guidance_scale = (
            video_options.guidance_scale
            if video_options and video_options.guidance_scale is not None
            else None
        ) or (image_options.guidance_scale if image_options else 3.5)
        true_cfg_scale = (
            video_options.true_cfg_scale
            if video_options and video_options.true_cfg_scale is not None
            else None
        ) or (image_options.true_cfg_scale if image_options else 1.0)
        height = (
            video_options.height
            if video_options and video_options.height is not None
            else None
        ) or (image_options.height if image_options else None)
        width = (
            video_options.width
            if video_options and video_options.width is not None
            else None
        ) or (image_options.width if image_options else None)
        steps = (
            video_options.steps
            if video_options and video_options.steps is not None
            else None
        ) or (image_options.steps if image_options else 50)
        num_images = image_options.num_images if image_options else 1

        if guidance_scale < 1.0 or true_cfg_scale < 1.0:
            logger.warning(
                f"Guidance scales < 1.0 detected (guidance_scale={guidance_scale}, "
                f"true_cfg_scale={true_cfg_scale}). This is mathematically possible"
                " but may produce lower quality or unexpected results."
            )

        if true_cfg_scale > 1.0 and neg_prompt is None:
            logger.warning(
                f"true_cfg_scale={true_cfg_scale} is set, but no negative_prompt "
                "is provided. True classifier-free guidance requires a negative prompt; "
                "falling back to standard generation."
            )

        do_true_cfg = true_cfg_scale > 1.0 and neg_prompt is not None
        do_cfg = guidance_scale > 1.0

        import PIL.Image

        # 1. Tokenize prompts
        # Convert input_image to list format for _generate_tokens_ids
        images_for_tokenization: list[PIL.Image.Image] | None = None
        if input_image is not None:
            input_img: PIL.Image.Image
            if isinstance(input_image, np.ndarray):
                input_img = PIL.Image.fromarray(input_image.astype(np.uint8))
            else:
                input_img = input_image
            images_for_tokenization = [input_img]

        (
            token_ids,
            attn_mask,
            attn_mask_neg,
            token_ids_2,
            negative_token_ids,
            negative_token_ids_2,
        ) = await self._generate_tokens_ids(
            prompt,
            sec_prompt,
            neg_prompt,
            sec_neg_prompt,
            do_true_cfg or (self._is_ltx2 and do_cfg),
            images=images_for_tokenization,
        )

        token_buffer = TokenBuffer(
            array=token_ids.astype(np.int64, copy=False),
        )
        token_buffer_2 = None
        if token_ids_2 is not None:
            token_buffer_2 = TokenBuffer(
                array=token_ids_2.astype(np.int64, copy=False),
            )
        negative_token_buffer = None
        if negative_token_ids is not None:
            negative_token_buffer = TokenBuffer(
                array=negative_token_ids.astype(np.int64, copy=False),
            )
        negative_token_buffer_2 = None
        if negative_token_ids_2 is not None:
            negative_token_buffer_2 = TokenBuffer(
                array=negative_token_ids_2.astype(np.int64, copy=False),
            )

        default_sample_size = self._default_sample_size
        vae_scale_factor = self._vae_scale_factor

        height = height or default_sample_size * vae_scale_factor
        width = width or default_sample_size * vae_scale_factor

        # 2. Preprocess input image if provided
        preprocessed_image_array = None
        if input_image is not None:
            preprocessed_image = self._preprocess_input_image(
                input_image, height, width
            )
            height = preprocessed_image.height
            width = preprocessed_image.width
            # Convert PIL.Image to numpy array for serialization
            # Use .copy() to ensure no references to the PIL.Image object are retained
            preprocessed_image_array = np.array(
                preprocessed_image, dtype=np.uint8
            ).copy()

        # 3. Resolve image dimensions using cached static values
        latent_height = 2 * (int(height) // (self._vae_scale_factor * 2))
        latent_width = 2 * (int(width) // (self._vae_scale_factor * 2))
        image_seq_len = (latent_height // 2) * (latent_width // 2)

        num_inference_steps = steps
        timesteps, sigmas = self._scheduler.retrieve_timesteps_and_sigmas(
            image_seq_len, num_inference_steps
        )

        num_warmup_steps: int = max(
            len(timesteps) - steps * self._scheduler.order, 0
        )

        video_options = request.body.provider_options.video
        num_frames = (
            video_options.num_frames if video_options is not None else None
        )
        latents, latent_image_ids = self._prepare_latents(
            num_images,
            self._num_channels_latents,
            latent_height,
            latent_width,
            request.body.seed,
            num_frames,
        )

        guidance: npt.NDArray[np.float32] | None = None
        if self._use_guidance_embeds:
            guidance = np.array([guidance_scale], dtype=np.float32)

        extra_params: dict[str, npt.NDArray[Any]] = {}
        if self._is_ltx2:
            frame_rate = (
                video_options.frames_per_second
                if video_options is not None
                else None
            )

            if num_frames is None or num_frames <= 0:
                num_frames = 1
            if frame_rate is None or frame_rate <= 0:
                frame_rate = 24

            # Audio latents: [B, 8, L, M]. Match the MAX LTX2 pipeline's defaults.
            num_mel_bins = self._ltx2_num_mel_bins
            latent_mel_bins = (
                num_mel_bins // self._ltx2_audio_mel_compression_ratio
            )
            duration_s = float(num_frames) / float(frame_rate)
            audio_latents_per_second = (
                self._ltx2_audio_sampling_rate
                / float(self._ltx2_audio_hop_length)
                / float(self._ltx2_audio_mel_compression_ratio)
            )
            audio_num_frames = round(duration_s * audio_latents_per_second)
            if audio_num_frames <= 0:
                audio_num_frames = 1

            audio_shape = (
                num_images,
                8,
                audio_num_frames,
                latent_mel_bins,
            )
            audio_latents = self._randn_tensor(audio_shape, request.body.seed)
            extra_params["ltx2_audio_latents"] = audio_latents

            # Pre-compute positional embedding coordinates on CPU so the
            # pipeline doesn't need to run tensor ops at inference time.
            # These are deterministic functions of the resolution/duration
            # and can be computed once here, avoiding repeated compilation.
            # Store scalar latent dimensions so the pipeline can skip
            # recomputing them from scratch.
            latent_num_frames = (
                num_frames - 1
            ) // self._vae_temporal_compression_ratio + 1
            extra_params["ltx2_latent_num_frames"] = np.array(
                latent_num_frames, dtype=np.int64
            )
            extra_params["ltx2_latent_height"] = np.array(
                latent_height, dtype=np.int64
            )
            extra_params["ltx2_latent_width"] = np.array(
                latent_width, dtype=np.int64
            )
            extra_params["ltx2_audio_num_frames"] = np.array(
                audio_num_frames, dtype=np.int64
            )
            extra_params["ltx2_latent_mel_bins"] = np.array(
                latent_mel_bins, dtype=np.int64
            )

            # Pre-compute positional embedding coordinates on CPU so the
            # pipeline doesn't need to run tensor ops at inference time.
            # These are deterministic functions of the resolution/duration
            # and can be computed once here, avoiding repeated compilation.

            video_coords = self._prepare_ltx2_video_coords(
                batch_size=num_images,
                latent_num_frames=latent_num_frames,
                latent_height=latent_height,
                latent_width=latent_width,
                frame_rate=float(frame_rate),
            )
            audio_coords = self._prepare_ltx2_audio_coords(
                batch_size=num_images,
                audio_num_frames=audio_num_frames,
            )

            # Pre-double for CFG on CPU (guidance_scale is already known here),
            # avoiding an eager F.concat tensor compilation in the pipeline.
            if do_cfg:
                video_coords = np.concatenate(
                    [video_coords, video_coords], axis=0
                )
                audio_coords = np.concatenate(
                    [audio_coords, audio_coords], axis=0
                )
            extra_params["ltx2_video_coords"] = video_coords
            extra_params["ltx2_audio_coords"] = audio_coords
            extra_params["ltx2_coords_cfg_doubled"] = np.array(do_cfg)

            # Number of real (non-padding) text tokens per batch item (uint32).
            # Pre-doubled for CFG here on CPU, mirroring the coords treatment
            # above, so the pipeline can wrap it in a Tensor without any
            # further mask arithmetic.
            valid_length_np = np.atleast_2d(
                np.array(attn_mask.sum(axis=-1), dtype=np.uint32)
            )

            if do_cfg:
                extra_params["ltx2_attn_mask_neg"] = attn_mask_neg
                valid_length_neg_np = np.atleast_2d(
                    np.array(attn_mask_neg.sum(axis=-1), dtype=np.uint32)
                )
                valid_length_np = np.concatenate(
                    [valid_length_neg_np, valid_length_np], axis=0
                )
            extra_params["ltx2_valid_length"] = valid_length_np

        # 5. Build the context
        context = PixelContext(
            request_id=request.request_id,
            tokens=token_buffer,
            mask=attn_mask,
            tokens_2=token_buffer_2,
            negative_tokens=negative_token_buffer,
            negative_tokens_2=negative_token_buffer_2,
            timesteps=timesteps,
            sigmas=sigmas,
            latents=latents,
            latent_image_ids=latent_image_ids,
            height=height,
            width=width,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
            guidance=guidance,
            true_cfg_scale=true_cfg_scale,
            num_visuals_per_prompt=num_images,
            num_frames=(
                video_options.num_frames
                if self._is_ltx2 and video_options is not None
                else None
            ),
            frame_rate=(
                video_options.frames_per_second
                if self._is_ltx2 and video_options is not None
                else None
            ),
            num_warmup_steps=num_warmup_steps,
            model_name=request.body.model,
            input_image=preprocessed_image_array,  # Pass numpy array instead of PIL.Image
            extra_params=extra_params,
        )

        for validator in self._context_validators:
            validator(context)

        return context
