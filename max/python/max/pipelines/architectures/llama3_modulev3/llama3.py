# ===----------------------------------------------------------------------=== #
# Copyright (c) 2026, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

"""Implements the Llama3 model using the ModuleV3 API."""

from __future__ import annotations

import functools
from collections.abc import Callable

from max.dtype import DType
from max.experimental import functional as F
from max.experimental.tensor import Tensor
from max.graph import TensorValue, ops
from max.nn.kv_cache import (
    KVCacheParamInterface,
    PagedCacheValues,
    unflatten_ragged_mha_decode_inputs,
)
from max.nn.module_v3 import Module
from max.nn.module_v3.embedding import Embedding
from max.nn.module_v3.linear import Linear
from max.nn.module_v3.norm import LayerNorm, RMSNorm
from max.nn.module_v3.sequential import ModuleList
from max.nn.transformer import ReturnHiddenStates, ReturnLogits

from ..common_layers.attention import AttentionWithRope
from ..common_layers.mlp import MLP
from .layers.mlp import LlamaStackedMLP
from .layers.rotary_embedding import (
    Llama3RotaryEmbedding,
    LongRoPERotaryEmbedding,
)
from .layers.transformer_block import LlamaTransformerBlock
from .model_config import Llama3Config


class Llama3TextModel(
    Module[[Tensor, PagedCacheValues, Tensor, Tensor], tuple[Tensor, ...]]
):
    """The Llama3 language model.

    Decoder-only Transformer with SwiGLU MLP, rotary embeddings,
    and grouped-query attention.
    """

    def __init__(self, config: Llama3Config) -> None:
        super().__init__()
        self.devices = config.devices

        # Create RoPE embedding.
        rope: Llama3RotaryEmbedding | LongRoPERotaryEmbedding
        if config.longrope_scaling_params is not None:
            rope = LongRoPERotaryEmbedding(
                dim=config.hidden_size,
                n_heads=config.num_attention_heads,
                theta=config.rope_theta,
                max_seq_len=config.max_seq_len,
                device=config.devices[0].to_device(),
                head_dim=Llama3Config.get_head_dim_from_config(config),
                interleaved=config.interleaved_rope_weights,
                scaling_params=config.longrope_scaling_params,
            )
        else:
            rope = Llama3RotaryEmbedding(
                dim=config.hidden_size,
                n_heads=config.num_attention_heads,
                theta=config.rope_theta,
                max_seq_len=config.max_seq_len,
                device=config.devices[0].to_device(),
                head_dim=Llama3Config.get_head_dim_from_config(config),
                interleaved=config.interleaved_rope_weights,
                scaling_params=config.rope_scaling_params,
            )

        # Select norm type.
        create_norm: Callable[..., Module[[Tensor], Tensor]]
        if config.norm_method == "rms_norm":
            if config.rms_norm_eps is None:
                raise ValueError(
                    "rms_norm_eps cannot be None for model that uses RMSNorm."
                )
            create_norm = functools.partial(
                RMSNorm, config.hidden_size, eps=config.rms_norm_eps
            )
        else:
            create_norm = functools.partial(
                LayerNorm,
                config.hidden_size,
                elementwise_affine=config.norm_elementwise_affine,
            )

        self.embed_tokens = Embedding(
            config.vocab_size,
            dim=config.hidden_size,
        )

        self.norm = create_norm()

        self.tie_word_embeddings = config.tie_word_embeddings
        if config.tie_word_embeddings:
            self.lm_head = None
        else:
            self.lm_head = Linear(
                in_dim=config.hidden_size,
                out_dim=config.vocab_size,
                bias=False,
            )

        # Build transformer layers.
        layers = []
        for i in range(config.num_hidden_layers):
            mlp: MLP | LlamaStackedMLP
            if config.stacked_mlp:
                mlp = LlamaStackedMLP(
                    hidden_dim=config.hidden_size,
                    feed_forward_length=config.intermediate_size,
                )
            else:
                mlp = MLP(
                    hidden_dim=config.hidden_size,
                    feed_forward_length=config.intermediate_size,
                )

            layers.append(
                LlamaTransformerBlock(
                    attention=AttentionWithRope(
                        rope=rope,
                        num_attention_heads=config.num_attention_heads,
                        num_key_value_heads=config.num_key_value_heads,
                        hidden_size=config.hidden_size,
                        kv_params=config.kv_params,
                        layer_idx=i,
                        scale=config.attention_multiplier,
                        has_bias=config.attention_bias,
                        stacked_qkv=config.stacked_qkv,
                        clip_qkv=config.clip_qkv,
                    ),
                    mlp=mlp,
                    input_layernorm=create_norm(),
                    post_attention_layernorm=create_norm(),
                    residual_multiplier=config.residual_multiplier,
                )
            )

        self.dim = config.hidden_size
        self.n_heads = config.num_attention_heads
        self.layers = ModuleList(layers)
        self.kv_params = config.kv_params
        self.return_logits = config.return_logits
        self.return_hidden_states = config.return_hidden_states
        self.embedding_multiplier = config.embedding_multiplier
        self.logits_scaling = config.logits_scaling

    def _compute_logits(self, h: Tensor) -> Tensor:
        """Compute logits from hidden states, handling weight tying."""
        if self.tie_word_embeddings:
            return F.cast(h @ self.embed_tokens.weight.T, DType.float32)
        assert self.lm_head is not None
        return F.cast(self.lm_head(h), DType.float32)

    def forward(
        self,
        tokens: Tensor,
        kv_collection: PagedCacheValues,
        return_n_logits: Tensor,
        input_row_offsets: Tensor,
    ) -> tuple[Tensor, ...]:
        h = self.embed_tokens(tokens)

        if self.embedding_multiplier != 1.0:
            h = h * F.constant(
                self.embedding_multiplier, h.dtype, device=h.device
            )

        # Run through transformer layers.
        for idx, layer in enumerate(self.layers):
            layer_idx_tensor = F.constant(idx, DType.uint32, device=h.device)
            h = layer(
                layer_idx_tensor,
                h,
                kv_collection,
                input_row_offsets=input_row_offsets,
            )

        # Compute logits based on return mode.
        last_h = F.gather(h, input_row_offsets[1:] - 1, axis=0)
        last_logits = self._compute_logits(self.norm(last_h))
        logits = None
        offsets = None

        if self.return_logits == ReturnLogits.VARIABLE:
            return_n_logits_range = ops.range(
                return_n_logits[0],
                0,
                -1,
                out_dim="return_n_logits_range",
                device=h.device,
                dtype=DType.int64,
            )
            offsets = (
                F.unsqueeze(input_row_offsets[1:], -1) - return_n_logits_range
            )
            last_indices = F.reshape(offsets, shape=(-1,))
            last_tokens = F.gather(h, last_indices, axis=0)
            logits = self._compute_logits(self.norm(last_tokens))
            offsets = ops.range(
                0,
                TensorValue(last_indices.shape[0]) + return_n_logits[0],
                return_n_logits[0],
                out_dim="logit_offsets",
                device=h.device,
                dtype=DType.int64,
            )
        elif self.return_logits == ReturnLogits.ALL:
            logits = self._compute_logits(self.norm(h))
            offsets = input_row_offsets

        if self.logits_scaling != 1.0:
            last_logits = last_logits / self.logits_scaling
            if logits is not None:
                logits = logits / self.logits_scaling

        ret_val: tuple[Tensor, ...] = (last_logits,)
        if offsets is not None:
            assert logits is not None
            ret_val += (logits, offsets)

        if self.return_hidden_states == ReturnHiddenStates.ALL:
            ret_val += (h,)
        elif self.return_hidden_states == ReturnHiddenStates.LAST:
            ret_val += (last_h,)
        elif self.return_hidden_states == ReturnHiddenStates.ALL_NORMALIZED:
            ret_val += (self.norm(h),)
        elif self.return_hidden_states == ReturnHiddenStates.LAST_NORMALIZED:
            ret_val += (self.norm(last_h),)

        return ret_val


class Llama3(Module[..., tuple[Tensor, ...]]):
    """The Llama3 model.

    Top-level wrapper that unflattens the variadic KV cache arguments
    and delegates to :class:`Llama3TextModel`.
    """

    def __init__(
        self,
        config: Llama3Config,
        kv_params: KVCacheParamInterface,
    ) -> None:
        super().__init__()
        self.language_model = Llama3TextModel(config)
        self.config = config
        self.kv_params = kv_params

    def forward(
        self,
        tokens: Tensor,
        return_n_logits: Tensor,
        input_row_offsets: Tensor,
        *variadic_args,
    ) -> tuple[Tensor, ...]:
        kv_collections = unflatten_ragged_mha_decode_inputs(
            variadic_args, n_devices=self.kv_params.n_devices
        )
        return self.language_model(
            tokens, kv_collections[0], return_n_logits, input_row_offsets
        )
