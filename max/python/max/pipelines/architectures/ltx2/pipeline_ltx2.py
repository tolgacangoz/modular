# ===----------------------------------------------------------------------=== #
# Copyright (c) 2025, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import logging
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, cast

import max.functional as F
from max._core.engine import Model
from max.driver import Buffer
from max.dtype import DType
from max.engine.api import InferenceSession
from max.graph import DeviceRef, Graph, TensorType
from max.graph.weights import (
    SafetensorWeights,
    WeightData,
    Weights,
)
from max.nn import Module
from max.pipelines import PixelContext
from max.pipelines.lib import (
    ModelInputs,
    ModelOutputs,
)
from max.pipelines.lib.interfaces import (
    DiffusionPipeline,
    PixelModelInputs,
)
from max.tensor import Tensor
from tqdm.auto import tqdm

from ..autoencoders import (
    AutoencoderKLLTX2AudioModel,
    AutoencoderKLLTX2VideoModel,
)
from .ltx2 import LTX2
from .model_config import LTX2Config
from .nn.transformer_ltx2 import LTX2Transformer2DModel

logger = logging.getLogger("max.pipelines")


@dataclass(kw_only=True)
class LTX2ModelInputs(PixelModelInputs):
    """A class representing inputs for the LTX2 model.

    This class encapsulates the input tensors required for the LTX2 model execution,
    including both text and vision inputs. Vision inputs are optional and can be None
    for text-only processing."""

    height: int | None = None
    """ The height in pixels of the generated image."""

    width: int | None = None
    """ The width in pixels of the generated image."""

    num_inference_steps: int = 50
    """ The number of denoising steps. More denoising steps usually lead to a higher quality image at the
    expense of slower inference."""

    num_warmup_steps: int = 0
    """ The number of warmup steps."""

    guidance_scale: float = 7.5
    """ Guidance scale as defined in Classifier-Free Diffusion Guidance.
    Higher values encourage images closely linked to the prompt, usually at the expense of lower image quality."""

    cfg_normalization: bool = False
    """ Whether to apply configuration normalization."""

    cfg_truncation: float = 1.0
    """ The truncation value for configuration."""

    token_ids: Buffer
    """ Tokenized prompt IDs from PixelGenerationTokenizer. Used for text encoder input."""

    secondary_token_ids: Buffer | None = None
    """ Secondary tokenized prompt IDs from PixelGenerationTokenizer. Used for text encoder input."""

    negative_token_ids: Buffer | None = None
    """ Negative prompt token IDs from PixelGenerationTokenizer. Used for text encoder input."""

    secondary_negative_token_ids: Buffer | None = None
    """ Negative secondary tokenized prompt IDs from PixelGenerationTokenizer. Used for text encoder input."""

    mask: Buffer | None = None
    """ Mask for the input tokens."""

    timesteps: Buffer | None = None
    """ Timesteps for the denoising process."""

    sigmas: list[Buffer] | None = None
    """ Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in
    their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed
    will be used."""

    num_videos_per_prompt: int | None = 1
    """ The number of videos to generate per prompt."""

    num_frames: int = 1
    """ The number of frames to generate for video output."""

    frame_rate: int = 24
    """ The frame rate for generated video."""

    seed: int | None = 0
    """ Seed for the random number generator to make generation deterministic.
    If None, random seed is used. Equivalent to torch.Generator().manual_seed(seed).
    """

    latents: Buffer | None = None
    """ Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image
    generation. Can be used to tweak the same generation with different prompts. If not provided, a latents
    tensor will be generated by sampling using the supplied random `generator`."""

    output_type: str | None = "pil"
    """ The output format of the generate image. Choose between
    [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`."""


class LTX2Pipeline(DiffusionPipeline):
    """A LTX2 pipeline for text-to-video and image-to-video generation."""

    vae: AutoencoderKLLTX2VideoModel
    vae_audio: AutoencoderKLLTX2AudioModel
    text_encoder: Gemma3_MultiModalModel
    transformer: LTX2Transformer2DModel

    components = {
        "vae": AutoencoderKLLTX2VideoModel,
        "vae_audio": AutoencoderKLLTX2AudioModel,
        "text_encoder": Gemma3_MultiModalModel,
        "transformer": LTX2Transformer2DModel,
    }

    def init_remaining_components(self) -> None:
        self.vae_scale_factor = (
            2 ** (len(self.vae.config.block_out_channels) - 1)
            if getattr(self, "vae", None)
            else 8
        )

    def prepare_inputs(self, context: PixelContext) -> LTX2ModelInputs:
        return LTX2ModelInputs.from_context(context)

    def load_model(
        self, session: InferenceSession
    ) -> tuple[
        AutoencoderKLLTX2Video,
        AutoencoderKLLTX2Audio,
        Model,
        LTX2Transformer2DModel,
    ]:
        """Loads the compiled LTX2 model into the MAX Engine session.

        Args:
            session: The MAX Engine inference session.

        Returns:
            The loaded MAX Engine model object.
        """
        if self.pipeline_config.max_batch_size is None:
            raise ValueError("Expected max_batch_size to be set")
        self._input_row_offsets_prealloc = F.arange(
            0,
            self.pipeline_config.max_batch_size + 1,
            dtype=DType.uint32,
            device=self.devices[0],
        )

        logger.info("Building and compiling the whole pipeline...")
        before = time.perf_counter()

        if not isinstance(self.weights, SafetensorWeights):
            raise ValueError("LTX2 currently only supports safetensors weights")

        # Partition raw safetensor weights by their originating file path.
        # This lets us treat diffusers-style folders (vae/, text_encoder/, transformer/)
        # as separate components without relying on key-name heuristics.
        st_weights: SafetensorWeights = self.weights
        filepaths = [Path(p) for p in st_weights._filepaths]

        vae_weights: dict[str, Weights] = {}
        vae_audio_weights: dict[str, Weights] = {}
        text_encoder_weights: dict[str, Weights] = {}
        transformer_weights: dict[str, Weights] = {}

        for key, weight in st_weights.items():
            file_idx = st_weights._tensors_to_file_idx[key]
            filepath = filepaths[file_idx]
            parts = set(filepath.parts)

            if "vae" in parts:
                vae_weights[key] = weight
            elif "vae_audio" in parts:
                vae_audio_weights[key] = weight
            elif "text_encoder" in parts:
                text_encoder_weights[key] = weight
            elif "transformer" in parts:
                transformer_weights[key] = weight
            else:
                logger.debug(
                    "Unrecognized LTX2 weight file '%s' for key '%s', assigning to transformer",
                    filepath,
                    key,
                )
                transformer_weights[key] = weight

        # Materialize VAE and transformer weights directly.
        vae_state_dict: dict[str, WeightData] = {
            key: weight.data() for key, weight in vae_weights.items()
        }
        vae_audio_state_dict: dict[str, WeightData] = {
            key: weight.data() for key, weight in vae_audio_weights.items()
        }
        text_encoder_state_dict: dict[str, WeightData] = {
            key: weight.data() for key, weight in text_encoder_weights.items()
        }
        transformer_state_dict: dict[str, WeightData] = {
            key: weight.data() for key, weight in transformer_weights.items()
        }

        # Apply the architecture weight adapter (if any) only to the text encoder
        # subset. For Z-Image this maps Qwen3-VL style names to the expected
        # ? format.
        if self.adapter:
            text_encoder_llm_state_dict: dict[str, WeightData] = self.adapter(
                text_encoder_weights,
                huggingface_config=self.huggingface_config,
                pipeline_config=self.pipeline_config,
            )
        else:
            text_encoder_llm_state_dict = {
                key: weight.data()
                for key, weight in text_encoder_weights.items()
            }

        text_encoder_state_dict: dict[str, dict[str, WeightData]] = {
            "llm_state_dict": text_encoder_llm_state_dict,
        }

        # Generate LTX2 config from HuggingFace config
        ltx2_config = LTX2Config.generate(
            pipeline_config=self.pipeline_config,
            scheduler_config=self.scheduler_config,
            vae_config=self.vae_config,
            text_encoder_config=self.text_encoder_config,
            transformer_config=self.transformer_config,
            vae_state_dict=vae_state_dict,
            text_encoder_state_dict=text_encoder_state_dict,
            transformer_state_dict=transformer_state_dict,
            dtype=self.dtype,
            n_devices=len(self.devices),
            cache_dtype=self.encoding.cache_dtype,
            kv_cache_config=self.kv_cache_config,
            return_logits=self.return_logits,
        )
        self.model_config = ltx2_config

        if self.model_config is None:
            raise ValueError("Model config must be initialized")

        # Instantiate LTX2 container to build sub-models
        device0 = self.devices[0]
        with F.lazy():
            nn_model: Module = LTX2(self.model_config, device=device0)
            nn_model.to(device0)

        # graph_inputs = nn_model.text_encoder.input_types(
        #     nn_model.text_encoder.kv_params
        # )
        # nn_model.text_encoder.load_state_dict(
        #     text_encoder_llm_state_dict,
        #     override_quantization_encoding=True,
        #     weight_alignment=1,
        #     # Stops strict from raising error when sharing LM head weights
        #     # (as LM head is never technically loaded from the state dict)
        #     strict=(
        #         not getattr(
        #             self.huggingface_config, "tie_word_embeddings", False
        #         )
        #     ),
        # )

        device_ref = DeviceRef(device0.label, device0.id)
        sample_type = TensorType(
            DType.bfloat16, shape=(1, 16, 128, 128), device=device_ref
        )

        # Strip 'decoder.' prefix from VAE weights for the Decoder module.
        # The safetensors file has keys like 'decoder.conv_in.weight' but the
        # Decoder module's parameters are named 'conv_in.weight'.
        decoder_weights: dict[str, WeightData] = {
            key.removeprefix("decoder."): value
            for key, value in vae_state_dict.items()
            if key.startswith("decoder.")
        }

        # Load weights into the models before compiling
        nn_model.transformer.load_state_dict(transformer_state_dict)
        nn_model.vae.decoder.load_state_dict(decoder_weights)

        with F.lazy():
            nn_model.transformer.to(device0)
            nn_model.vae.to(device0)

        logger.info("Building and compiling VAE's decoder...")
        before_vae_decode_build = time.perf_counter()
        # Compile VAE decoder for batched latents (B, C, H, W)
        vae_input_type = TensorType(
            DType.bfloat16,
            shape=(self.pipeline_config.max_batch_size, 16, 128, 128),
            device=device_ref,
        )
        compiled_vae_decoder_model = nn_model.vae.decoder.compile(
            vae_input_type
        )
        after_vae_decode_build = time.perf_counter()
        logger.info(
            f"Building and compiling VAE's decoder took {after_vae_decode_build - before_vae_decode_build:.6f} seconds"
        )
        nn_model.vae.decoder = compiled_vae_decoder_model

        # Prepare compilation types for backbone transformer
        # We compile with max_batch_size to support variable batch sizes
        max_batch_size = self.pipeline_config.max_batch_size

        # Define missing dimension variables
        C = self.model_config.in_channels
        inner_dim = (
            self.model_config.num_attention_heads
            * self.model_config.attention_head_dim
        )
        # For LTX2 images, we use 32 as the spatial reduction factor (32x32)
        # and 1 as the default frame dimension.
        F_dim = 1
        H_dim = 32
        W_dim = 32
        cap_seq_len = 128  # Default caption sequence length for Gemma3

        hidden_states_type = TensorType(
            DType.bfloat16,
            shape=(max_batch_size, C, F_dim, H_dim, W_dim),
            device=device_ref,
        )
        t_type = TensorType(
            DType.float32, shape=(max_batch_size,), device=device_ref
        )
        cap_feats_type = TensorType(
            DType.bfloat16,
            shape=(max_batch_size, cap_seq_len, inner_dim),
            device=device_ref,
        )

        logger.info("Building and compiling the backbone transformer...")
        before_transformer_build = time.perf_counter()
        compiled_transformer_model = nn_model.transformer.compile(
            hidden_states_type,
            t_type,
            cap_feats_type,
        )
        after_transformer_build = time.perf_counter()
        logger.info(
            f"Building and compiling the backbone transformer took {after_transformer_build - before_transformer_build:.6f} seconds"
        )
        nn_model.transformer = compiled_transformer_model

        # Prepare text encoder graph inputs.
        # Gemma3 expects (tokens, input_row_offsets, return_n_logits, *kv_cache_inputs)
        # We use dummy kv_params for type discovery
        graph_inputs = nn_model.text_encoder.input_types()

        logger.info("Building and compiling text encoder...")
        before_text_encoder_build = time.perf_counter()
        text_encoder_graph = self._build_text_encoder_graph(graph_inputs)
        after_text_encoder_build = time.perf_counter()

        logger.info(
            f"Building text encoder's graph took {after_text_encoder_build - before_text_encoder_build:.6f} seconds"
        )

        before_text_encoder_compile = time.perf_counter()
        compiled_text_encoder_model = session.load(
            text_encoder_graph,
            weights_registry=text_encoder_llm_state_dict,
        )
        nn_model.text_encoder = compiled_text_encoder_model
        after = time.perf_counter()

        # logger.info(
        #     f"Compiling text encoder's model took {after - before_text_encoder_compile:.6f} seconds"
        # )
        logger.info(
            f"Building and compiling the whole pipeline took {after - before:.6f} seconds"
        )
        return nn_model

    def _build_text_encoder_graph(
        self, graph_inputs: tuple[TensorType, ...]
    ) -> Graph:
        """Build the MAX graph for the text encoder.

        Uses native Qwen3 with return_hidden_states=SECOND_TO_LAST configured in
        model_config.py. SECOND_TO_LAST returns the second-to-last layer's hidden
        states directly (matching diffusers' hidden_states[-2] pattern).

        Args:
            graph_inputs: Tuple of TensorType defining the graph inputs.

        Returns:
            Compiled MAX graph that outputs hidden states of shape
            (seq_len, hidden_size) from the second-to-last layer.
        """
        with Graph("qwen3_text_encoder", input_types=graph_inputs) as graph:
            tokens, input_row_offsets, return_n_logits, *kv_cache_inputs = (
                graph.inputs
            )
            kv_collection = PagedCacheValues(
                kv_blocks=kv_cache_inputs[0].buffer,
                cache_lengths=kv_cache_inputs[1].tensor,
                lookup_table=kv_cache_inputs[2].tensor,
                max_lengths=kv_cache_inputs[3].tensor,
            )
            # Qwen3 with ReturnHiddenStates.SECOND_TO_LAST_LAYER returns
            # (logits, second_to_last_hidden_states) directly
            outputs = self.model.text_encoder(
                tokens.tensor,
                kv_collection,
                return_n_logits.tensor,
                input_row_offsets.tensor,
            )
            # Extract second-to-last hidden states (last element of output tuple)
            # Shape: (seq_len, hidden_size)
            hidden_states = outputs[-1]
            graph.output(hidden_states)
            return graph

    @property
    def guidance_scale(self) -> float:
        return self._guidance_scale

    @property
    def do_classifier_free_guidance(self) -> bool:
        return self._guidance_scale > 1

    @property
    def joint_attention_kwargs(self) -> dict[str, Any] | None:
        return self._joint_attention_kwargs

    @property
    def num_timesteps(self) -> int:
        return self._num_timesteps

    def execute(
        self,
        model_inputs: ModelInputs,
    ) -> ModelOutputs:
        r"""
        Executes the LTX2 model with the prepared inputs.

        Args:
            model_inputs: A LTX2Inputs instance containing all image generation parameters
                including prompt, dimensions, guidance scale, etc.

        Returns:
            ModelOutputs containing the generated images.
        """
        # Use cast for type safety (same pattern as GptOssModel)
        model_inputs = cast(LTX2Inputs, model_inputs)

        # Extract parameters from model_inputs
        height = model_inputs.height or 1024
        width = model_inputs.width or 1024
        num_inference_steps = model_inputs.num_inference_steps
        sigmas = model_inputs.sigmas
        guidance_scale = model_inputs.guidance_scale
        cfg_normalization = model_inputs.cfg_normalization
        cfg_truncation = model_inputs.cfg_truncation
        negative_prompt = model_inputs.negative_prompt
        num_visuals_per_prompt = model_inputs.num_visuals_per_prompt or 1
        latents = model_inputs.latents
        prompt_embeds = model_inputs.prompt_embeds
        negative_prompt_embeds = model_inputs.negative_prompt_embeds
        output_type = model_inputs.output_type
        device = self.devices[0]

        prompt_embeds = (
            Tensor.from_dlpack(prompt_embeds_np).to(device).cast(DType.bfloat16)
        )

        self._guidance_scale = guidance_scale
        self._cfg_normalization = cfg_normalization
        self._cfg_truncation = cfg_truncation
        # 2. Define call parameters
        self.model.scheduler._step_index = 0
        timesteps = (
            Tensor.from_dlpack(model_inputs.timesteps).to(device).cast(DType.float32)
        )
        latents = (
            Tensor.from_dlpack(model_inputs.latents).to(device).cast(DType.bfloat16)
        )
        num_warmup_steps = model_inputs.num_warmup_steps

        # 6. Denoising loop
        with tqdm(total=num_inference_steps, desc="Denoising") as progress_bar:
            for i, t in enumerate(timesteps):
                # Normalize timestep for guidance scale check
                t_norm = t / 1000.0

                # Handle cfg truncation
                current_guidance_scale = self.guidance_scale
                if (
                    self.do_classifier_free_guidance
                    and self._cfg_truncation is not None
                    and float(self._cfg_truncation) <= 1
                ):
                    if t_norm > self._cfg_truncation:
                        current_guidance_scale = 0.0

                # Run CFG only if configured AND scale is non-zero
                apply_cfg = (
                    self.do_classifier_free_guidance
                    and current_guidance_scale > 0
                )

                if apply_cfg:
                    latent_model_input = latents.repeat(2, 1, 1, 1)
                    prompt_embeds_model_input = F.concat(
                        [prompt_embeds, negative_prompt_embeds], axis=0
                    )
                    timestep_model_input = t.repeat(2)
                else:
                    latent_model_input = latents
                    prompt_embeds_model_input = prompt_embeds
                    timestep_model_input = t

                # Backbone transformer expects (B, C, F, H, W)
                latent_model_input = latent_model_input.unsqueeze(2)

                model_out = self.model.transformer(
                    latent_model_input,
                    timestep_model_input,
                    prompt_embeds_model_input,
                )

                if apply_cfg:
                    # CFG Implementation
                    noise_pred_uncond, noise_pred_text = F.chunk(
                        model_out, 2, axis=0
                    )
                    noise_pred = noise_pred_uncond + current_guidance_scale * (
                        noise_pred_text - noise_pred_uncond
                    )
                else:
                    noise_pred = model_out

                noise_pred = -noise_pred.cast(DType.float32).squeeze(2)
                # compute the previous noisy sample x_t -> x_t-1
                latents = self.model.scheduler.step(
                    noise_pred,
                    t,
                    latents,
                ).prev_sample

                if i == len(timesteps) - 1 or (
                    (i + 1) > num_warmup_steps
                    and (i + 1) % self.model.scheduler.order == 0
                ):
                    progress_bar.update()

        # 3. Decode
        outputs = self._decode_latents(
            latents,
            model_inputs.height,
            model_inputs.width,
            output_type=output_type,
        )

        return ModelOutputs(
            hidden_states=cast(Buffer, outputs.driver_tensor),
            logits=None,
        )
