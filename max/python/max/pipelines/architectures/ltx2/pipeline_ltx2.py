# ===----------------------------------------------------------------------=== #
# Copyright (c) 2025, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import logging
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Literal, cast

import numpy as np

import max.functional as F
from max._core.engine import Model
from max import random
from max.driver import Buffer, CPU
from max.dtype import DType
from max.engine.api import InferenceSession
from max.graph import DeviceRef, TensorType
from max.graph.weights import (
    SafetensorWeights,
    WeightData,
    Weights,
)
from max.nn import Module
from max.pipelines import PixelContext
from max.pipelines.lib import (
    ModelInputs,
    ModelOutputs,
)
from max.pipelines.lib.interfaces import (
    DiffusionPipeline,
    PixelModelInputs,
)
from max.tensor import Tensor
from tqdm.auto import tqdm

from ..autoencoders import (
    AutoencoderKLLTX2AudioModel,
    AutoencoderKLLTX2VideoModel,
)
from .ltx2 import LTX2
from .model_config import LTX2Config
from .nn.transformer_ltx2 import LTX2Transformer2DModel

logger = logging.getLogger("max.pipelines")


@dataclass(kw_only=True)
class LTX2ModelInputs(PixelModelInputs):
    """A class representing inputs for the LTX2 model.

    This class encapsulates the input tensors required for the LTX2 model execution,
    including both text and vision inputs. Vision inputs are optional and can be None
    for text-only processing."""

    height: int | None = None
    """ The height in pixels of the generated image."""

    width: int | None = None
    """ The width in pixels of the generated image."""

    num_inference_steps: int = 50
    """ The number of denoising steps. More denoising steps usually lead to a higher quality image at the
    expense of slower inference."""

    num_warmup_steps: int = 0
    """ The number of warmup steps."""

    guidance_scale: float = 7.5
    """ Guidance scale as defined in Classifier-Free Diffusion Guidance.
    Higher values encourage images closely linked to the prompt, usually at the expense of lower image quality."""

    cfg_normalization: bool = False
    """ Whether to apply configuration normalization."""

    cfg_truncation: float = 1.0
    """ The truncation value for configuration."""

    token_ids: Buffer
    """ Tokenized prompt IDs from PixelGenerationTokenizer. Used for text encoder input."""

    negative_token_ids: Buffer | None = None
    """ Negative prompt token IDs from PixelGenerationTokenizer. Used for text encoder input."""

    mask: Buffer | None = None
    """ Mask for the input tokens."""

    attention_kwargs: dict[str, Any] | None = None
    """ Additional keyword arguments for the attention heads."""

    timesteps: Buffer | None = None
    """ Timesteps for the denoising process."""

    sigmas: list[Buffer] | None = None
    """ Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in
    their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed
    will be used."""

    num_videos_per_prompt: int | None = 1
    """ The number of videos to generate per prompt."""

    num_frames: int = 1
    """ The number of frames to generate for video output."""

    frame_rate: int = 24
    """ The frame rate for generated video."""

    seed: int | None = 0
    """ Seed for the random number generator to make generation deterministic.
    If None, random seed is used. Equivalent to torch.Generator().manual_seed(seed).
    """

    latents: Buffer | None = None
    """ Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image
    generation. Can be used to tweak the same generation with different prompts. If not provided, a latents
    tensor will be generated by sampling using the supplied random `generator`."""

    output_type: str | None = "pil"
    """ The output format of the generate image. Choose between
    [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`."""


class LTX2Pipeline(DiffusionPipeline):
    """A LTX2 pipeline for text-to-video and image-to-video generation."""

    vae: AutoencoderKLLTX2VideoModel
    vae_audio: AutoencoderKLLTX2AudioModel
    transformer: LTX2Transformer2DModel

    components = {
        "vae": AutoencoderKLLTX2VideoModel,
        "vae_audio": AutoencoderKLLTX2AudioModel,
        "transformer": LTX2Transformer2DModel,
    }

    def init_remaining_components(self) -> None:
        self.vae_spatial_compression_ratio = (
            self.model.vae.config.vae_scale_factors[1]
            if hasattr(self.model.vae.config, "vae_scale_factors")
            else 32
        )
        self.vae_temporal_compression_ratio = (
            self.model.vae.config.vae_scale_factors[0]
            if hasattr(self.model.vae.config, "vae_scale_factors")
            else 4
        )
        self.audio_vae_mel_compression_ratio = 4
        self.audio_hop_length = 160
        self.audio_sampling_rate = 16000

    def prepare_inputs(self, context: PixelContext) -> LTX2ModelInputs:
        return LTX2ModelInputs.from_context(context)

    def load_model(
        self, session: InferenceSession
    ) -> tuple[
        AutoencoderKLLTX2Video,
        AutoencoderKLLTX2Audio,
        Model,
        LTX2Transformer2DModel,
    ]:
        """Loads the compiled LTX2 model into the MAX Engine session.

        Args:
            session: The MAX Engine inference session.

        Returns:
            The loaded MAX Engine model object.
        """
        if self.pipeline_config.max_batch_size is None:
            raise ValueError("Expected max_batch_size to be set")
        self._input_row_offsets_prealloc = F.arange(
            0,
            self.pipeline_config.max_batch_size + 1,
            dtype=DType.uint32,
            device=self.devices[0],
        )

        logger.info("Building and compiling the whole pipeline...")
        before = time.perf_counter()

        if not isinstance(self.weights, SafetensorWeights):
            raise ValueError("LTX2 currently only supports safetensors weights")

        # Partition raw safetensor weights by their originating file path.
        # This lets us treat diffusers-style folders (vae/, text_encoder/, transformer/)
        # as separate components without relying on key-name heuristics.
        st_weights: SafetensorWeights = self.weights
        filepaths = [Path(p) for p in st_weights._filepaths]

        vae_weights: dict[str, Weights] = {}
        vae_audio_weights: dict[str, Weights] = {}
        text_encoder_weights: dict[str, Weights] = {}
        transformer_weights: dict[str, Weights] = {}

        for key, weight in st_weights.items():
            file_idx = st_weights._tensors_to_file_idx[key]
            filepath = filepaths[file_idx]
            parts = set(filepath.parts)

            if "vae" in parts:
                vae_weights[key] = weight
            elif "vae_audio" in parts:
                vae_audio_weights[key] = weight
            elif "text_encoder" in parts:
                text_encoder_weights[key] = weight
            elif "transformer" in parts:
                transformer_weights[key] = weight
            else:
                logger.debug(
                    "Unrecognized LTX2 weight file '%s' for key '%s', assigning to transformer",
                    filepath,
                    key,
                )
                transformer_weights[key] = weight

        # Materialize VAE and transformer weights directly.
        vae_state_dict: dict[str, WeightData] = {
            key: weight.data() for key, weight in vae_weights.items()
        }
        vae_audio_state_dict: dict[str, WeightData] = {
            key: weight.data() for key, weight in vae_audio_weights.items()
        }
        text_encoder_state_dict: dict[str, WeightData] = {
            key: weight.data() for key, weight in text_encoder_weights.items()
        }
        transformer_state_dict: dict[str, WeightData] = {
            key: weight.data() for key, weight in transformer_weights.items()
        }

        # Apply the architecture weight adapter (if any) only to the text encoder
        # subset. For Z-Image this maps Qwen3-VL style names to the expected
        # ? format.
        if self.adapter:
            text_encoder_llm_state_dict: dict[str, WeightData] = self.adapter(
                text_encoder_weights,
                huggingface_config=self.huggingface_config,
                pipeline_config=self.pipeline_config,
            )
        else:
            text_encoder_llm_state_dict = {
                key: weight.data()
                for key, weight in text_encoder_weights.items()
            }

        text_encoder_state_dict: dict[str, dict[str, WeightData]] = {
            "llm_state_dict": text_encoder_llm_state_dict,
        }

        # Generate LTX2 config from HuggingFace config
        ltx2_config = LTX2Config.generate(
            pipeline_config=self.pipeline_config,
            scheduler_config=self.scheduler_config,
            vae_config=self.vae_config,
            text_encoder_config=self.text_encoder_config,
            transformer_config=self.transformer_config,
            vae_state_dict=vae_state_dict,
            text_encoder_state_dict=text_encoder_state_dict,
            transformer_state_dict=transformer_state_dict,
            dtype=self.dtype,
            n_devices=len(self.devices),
            cache_dtype=self.encoding.cache_dtype,
            kv_cache_config=self.kv_cache_config,
            return_logits=self.return_logits,
        )
        self.model_config = ltx2_config

        if self.model_config is None:
            raise ValueError("Model config must be initialized")

        # Instantiate LTX2 container to build sub-models
        device0 = self.devices[0]
        with F.lazy():
            nn_model: Module = LTX2(self.model_config, device=device0)
            nn_model.to(device0)

        # graph_inputs = nn_model.text_encoder.input_types(
        #     nn_model.text_encoder.kv_params
        # )
        # nn_model.text_encoder.load_state_dict(
        #     text_encoder_llm_state_dict,
        #     override_quantization_encoding=True,
        #     weight_alignment=1,
        #     # Stops strict from raising error when sharing LM head weights
        #     # (as LM head is never technically loaded from the state dict)
        #     strict=(
        #         not getattr(
        #             self.huggingface_config, "tie_word_embeddings", False
        #         )
        #     ),
        # )

        device_ref = DeviceRef(device0.label, device0.id)
        sample_type = TensorType(
            DType.bfloat16, shape=(1, 16, 128, 128), device=device_ref
        )

        # Strip 'decoder.' prefix from VAE weights for the Decoder module.
        # The safetensors file has keys like 'decoder.conv_in.weight' but the
        # Decoder module's parameters are named 'conv_in.weight'.
        decoder_weights: dict[str, WeightData] = {
            key.removeprefix("decoder."): value
            for key, value in vae_state_dict.items()
            if key.startswith("decoder.")
        }

        # Load weights into the models before compiling
        nn_model.transformer.load_state_dict(transformer_state_dict)
        nn_model.vae.decoder.load_state_dict(decoder_weights)

        with F.lazy():
            nn_model.transformer.to(device0)
            nn_model.vae.to(device0)

        logger.info("Building and compiling VAE's decoder...")
        before_vae_decode_build = time.perf_counter()
        # Compile VAE decoder for batched latents (B, C, H, W)
        vae_input_type = TensorType(
            DType.bfloat16,
            shape=(self.pipeline_config.max_batch_size, 16, 128, 128),
            device=device_ref,
        )
        compiled_vae_decoder_model = nn_model.vae.decoder.compile(
            vae_input_type
        )
        after_vae_decode_build = time.perf_counter()
        logger.info(
            f"Building and compiling VAE's decoder took {after_vae_decode_build - before_vae_decode_build:.6f} seconds"
        )
        nn_model.vae.decoder = compiled_vae_decoder_model

        # Prepare compilation types for backbone transformer
        # We compile with max_batch_size to support variable batch sizes
        max_batch_size = self.pipeline_config.max_batch_size

        # Define missing dimension variables
        C = self.model_config.in_channels
        inner_dim = (
            self.model_config.num_attention_heads
            * self.model_config.attention_head_dim
        )
        # For LTX2 images, we use 32 as the spatial reduction factor (32x32)
        # and 1 as the default frame dimension.
        F_dim = 1
        H_dim = 32
        W_dim = 32
        cap_seq_len = 128  # Default caption sequence length for Gemma3

        hidden_states_type = TensorType(
            DType.bfloat16,
            shape=(max_batch_size, C, F_dim, H_dim, W_dim),
            device=device_ref,
        )
        t_type = TensorType(
            DType.float32, shape=(max_batch_size,), device=device_ref
        )
        cap_feats_type = TensorType(
            DType.bfloat16,
            shape=(max_batch_size, cap_seq_len, inner_dim),
            device=device_ref,
        )

        logger.info("Building and compiling the backbone transformer...")
        before_transformer_build = time.perf_counter()
        compiled_transformer_model = nn_model.transformer.compile(
            hidden_states_type,
            t_type,
            cap_feats_type,
        )
        after_transformer_build = time.perf_counter()
        logger.info(
            f"Building and compiling the backbone transformer took {after_transformer_build - before_transformer_build:.6f} seconds"
        )
        nn_model.transformer = compiled_transformer_model

        # Text encoder is now loaded via transformers (Gemma3ForConditionalGeneration)
        # No MAX graph compilation needed - it runs on PyTorch/CUDA directly
        logger.info(
            "Text encoder loaded via transformers (Gemma3ForConditionalGeneration)"
        )

        after = time.perf_counter()
        logger.info(
            f"Building and compiling the whole pipeline took {after - before:.6f} seconds"
        )
        return nn_model

    def _encode_tokens(self, token_ids: Tensor, device: str = "cuda") -> Tensor:
        """Encode token_ids using transformers Gemma3ForConditionalGeneration.

        The token_ids come from PixelGenerationTokenizer which already handles
        tokenization. This method just runs them through the text encoder to
        get hidden states.

        Args:
            token_ids: Token IDs from PixelGenerationTokenizer (via model_inputs.token_ids).
            device: Device to run encoding on.

        Returns:
            Hidden states tensor from the text encoder, stacked across all layers.
        """
        import torch

        # Convert MAX Tensor to PyTorch tensor
        input_ids = torch.from_numpy(token_ids.to_numpy()).to(device)
        attention_mask = torch.ones_like(input_ids)

        with torch.no_grad():
            outputs = self.model.text_encoder(
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_hidden_states=True,
            )
            # Stack all hidden states: [batch_size, seq_len, hidden_dim, num_layers]
            hidden_states = torch.stack(outputs.hidden_states, dim=-1)

        # Convert to MAX Tensor
        return Tensor.from_dlpack(hidden_states.to(torch.bfloat16))

    @staticmethod
    def _pack_text_embeds(
        text_hidden_states: Tensor,
        attention_mask: Tensor,
        device: Device,
        scale_factor: int = 8,
        eps: float = 1e-6,
    ) -> Tensor:
        """
        Packs and normalizes text encoder hidden states, respecting padding.
        """
        batch_size, seq_len, hidden_dim, num_layers = text_hidden_states.shape
        # original_dtype = text_hidden_states.dtype

        # Create padding mask [batch_size, seq_len, 1, 1]
        mask = attention_mask.unsqueeze(-1).unsqueeze(-1)
        sequence_lengths = attention_mask.sum(axis=1)

        # Compute masked mean over non-padding positions
        masked_text_hidden_states = F.where(mask.cast(DType.bool), text_hidden_states, F.constant(0.0, dtype=text_hidden_states.dtype))
        num_valid_positions = (sequence_lengths * hidden_dim).reshape(batch_size, 1, 1, 1)

        # masked_mean = masked_text_hidden_states.sum(dim=(1, 2), keepdim=True) / (num_valid_positions + eps)
        # MAX doesn't support sum(dim=(1,2)), we do it sequentially
        masked_sum = masked_text_hidden_states.sum(axis=1).sum(axis=1).unsqueeze(1).unsqueeze(2)
        masked_mean = masked_sum / (num_valid_positions.cast(text_hidden_states.dtype) + eps)

        # Compute min/max over non-padding positions
        inf_val = F.constant(float("inf"), dtype=text_hidden_states.dtype)
        neg_inf_val = F.constant(float("-inf"), dtype=text_hidden_states.dtype)

        x_min = F.where(mask.cast(DType.bool), text_hidden_states, inf_val).min(axis=1).min(axis=1).unsqueeze(1).unsqueeze(2)
        x_max = F.where(mask.cast(DType.bool), text_hidden_states, neg_inf_val).max(axis=1).max(axis=1).unsqueeze(1).unsqueeze(2)

        # Normalization
        normalized_hidden_states = (text_hidden_states - masked_mean) / (x_max - x_min + eps)
        normalized_hidden_states = normalized_hidden_states * scale_factor

        # Pack the hidden states to a 3D tensor (batch_size, seq_len, hidden_dim * num_layers)
        normalized_hidden_states = normalized_hidden_states.reshape(batch_size, seq_len, -1)

        # Mask out padding in the final result
        mask_flat = attention_mask.unsqueeze(-1)
        normalized_hidden_states = F.where(mask_flat.cast(DType.bool), normalized_hidden_states, F.constant(0.0, dtype=normalized_hidden_states.dtype))

        return normalized_hidden_states

    @staticmethod
    def _pack_latents(latents: Tensor, patch_size: int = 1, patch_size_t: int = 1) -> Tensor:
        """Pack latents from [B, C, F, H, W] to [B, S, D] token sequence."""
        batch_size, num_channels, num_frames, height, width = latents.shape
        post_patch_num_frames = num_frames // patch_size_t
        post_patch_height = height // patch_size
        post_patch_width = width // patch_size
        latents = latents.reshape(
            batch_size,
            -1,
            post_patch_num_frames,
            patch_size_t,
            post_patch_height,
            patch_size,
            post_patch_width,
            patch_size,
        )
        latents = latents.permute(0, 2, 4, 6, 1, 3, 5, 7).flatten(4, 7).flatten(1, 3)
        return latents

    @staticmethod
    def _unpack_latents(
        latents: Tensor, num_frames: int, height: int, width: int, patch_size: int = 1, patch_size_t: int = 1
    ) -> Tensor:
        """Unpack latents from [B, S, D] to [B, C, F, H, W]."""
        batch_size = latents.shape[0]
        latents = latents.reshape(batch_size, num_frames, height, width, -1, patch_size_t, patch_size, patch_size)
        latents = latents.permute(0, 4, 1, 5, 2, 6, 3, 7).flatten(6, 7).flatten(4, 5).flatten(2, 3)
        return latents

    @staticmethod
    def _pack_audio_latents(latents: Tensor) -> Tensor:
        """Pack audio latents from [B, C, L, M] to [B, L, C*M]."""
        # Transpose [B, C, L, M] -> [B, L, C, M] then flatten to [B, L, C*M]
        latents = latents.transpose(1, 2).flatten(2, 3)
        return latents

    @staticmethod
    def _unpack_audio_latents(latents: Tensor, latent_length: int, num_mel_bins: int) -> Tensor:
        """Unpack audio latents from [B, L, C*M] to [B, C, L, M]."""
        # Unflatten [B, L, C*M] -> [B, L, C, M] then transpose to [B, C, L, M]
        latents = latents.unflatten(2, (-1, num_mel_bins)).transpose(1, 2)
        return latents

    def _denormalize_latents(self, latents: Tensor) -> Tensor:
        """Denormalize video latents."""
        latents_mean = self.model.vae.latents_mean
        latents_std = self.model.vae.latents_std
        scaling_factor = self.model.vae.config.scaling_factor
        # Reshape mean/std for broadcasting [1, C, 1, 1, 1]
        latents = latents * latents_std / scaling_factor + latents_mean
        return latents

    def _denormalize_audio_latents(self, latents: Tensor) -> Tensor:
        """Denormalize audio latents."""
        latents_mean = self.model.vae_audio.latents_mean
        latents_std = self.model.vae_audio.latents_std
        return (latents * latents_std) + latents_mean

    @property
    def guidance_scale(self) -> float:
        return self._guidance_scale

    @property
    def do_classifier_free_guidance(self) -> bool:
        return self._guidance_scale > 1

    @property
    def joint_attention_kwargs(self) -> dict[str, Any] | None:
        return self._joint_attention_kwargs

    @property
    def num_timesteps(self) -> int:
        return self._num_timesteps

    def _decode_latents(
        self,
        latents: Tensor,
        height: int,
        width: int,
        output_type: Literal["np", "latent", "pil"] = "np",
    ) -> Tensor | np.ndarray:
        if output_type == "latent":
            return latents
        latents = Tensor.from_dlpack(latents)
        latents = self._unpack_latents(
            latents, height, width, self.vae_spatial_compression_ratio
        )
        # Denormalize
        latents = self._denormalize_latents(latents)

        return self._to_numpy(self.model.vae.decode(latents.cast(DType.bfloat16)))

    def _to_numpy(self, image: Tensor) -> np.ndarray:
        cpu_image: Tensor = image.cast(DType.float32).to(CPU())
        return np.from_dlpack(cpu_image)

    def _scheduler_step(
        self,
        latents: Tensor,
        noise_pred: Tensor,
        sigmas: Tensor,
        step_index: int,
    ) -> Tensor:
        latents_dtype = latents.dtype
        latents = latents.cast(DType.float32)
        sigma = sigmas[step_index]
        sigma_next = sigmas[step_index + 1]
        dt = sigma_next - sigma
        latents = latents + dt * noise_pred
        latents = latents.cast(latents_dtype)
        return latents

    def execute(
        self,
        model_inputs: ModelInputs,
    ) -> ModelOutputs:
        r"""
        Executes the LTX2 model with the prepared inputs.

        Args:
            model_inputs: A LTX2Inputs instance containing all image generation parameters
                including prompt, dimensions, guidance scale, etc.

        Returns:
            ModelOutputs containing the generated images.
        """
        # Use cast for type safety
        model_inputs = cast(LTX2ModelInputs, model_inputs)

        # Extract parameters from model_inputs
        height = model_inputs.height or 512
        width = model_inputs.width or 768
        num_frames = model_inputs.num_frames or 121
        frame_rate = model_inputs.frame_rate or 24
        num_inference_steps = model_inputs.num_inference_steps
        guidance_scale = model_inputs.guidance_scale
        device = self.devices[0]

        self._guidance_scale = guidance_scale

        # 1. Compute latent dimensions
        latent_num_frames = (num_frames - 1) // self.vae_temporal_compression_ratio + 1
        latent_height = height // self.vae_spatial_compression_ratio
        latent_width = width // self.vae_spatial_compression_ratio

        # 2. Get timesteps and sigmas
        timesteps = (
            Tensor.from_dlpack(model_inputs.timesteps)
            .to(device)
            .cast(DType.float32)
        )
        sigmas = (
            Tensor.from_dlpack(model_inputs.sigmas)
            .to(device)
            .cast(DType.float32)
        )

        # 3. Encode text with Gemma3 (via transformers)
        token_ids = Tensor.from_dlpack(model_inputs.token_ids).to(device)
        prompt_attention_mask = Tensor.from_dlpack(model_inputs.mask).to(device)

        text_encoder_hidden_states = self._encode_tokens(token_ids, device="cuda")
        prompt_embeds = self._pack_text_embeds(text_encoder_hidden_states, prompt_attention_mask, device)

        # Encode negative prompt if doing CFG
        if self.do_classifier_free_guidance:
            if model_inputs.negative_token_ids is not None:
                negative_token_ids = Tensor.from_dlpack(model_inputs.negative_token_ids).to(device)
                negative_attention_mask = Tensor.from_dlpack(model_inputs.mask).to(device) # Usually same mask shape

                negative_hidden_states = self._encode_tokens(negative_token_ids, device="cuda")
                negative_prompt_embeds = self._pack_text_embeds(negative_hidden_states, negative_attention_mask, device)
            else:
                # Use zeros for negative prompt if not provided
                negative_prompt_embeds = F.zeros_like(prompt_embeds)
            # Concatenate for CFG: [negative, positive]
            prompt_embeds = F.concat([negative_prompt_embeds, prompt_embeds], axis=0)
            prompt_attention_mask = F.concat([prompt_attention_mask, prompt_attention_mask], axis=0)

        # 4. Process text embeddings through connectors
        additive_attention_mask = (1 - prompt_attention_mask.cast(prompt_embeds.dtype)) * -1000000.0
        connector_prompt_embeds, connector_audio_prompt_embeds, connector_attention_mask = self.model.connectors(
            prompt_embeds, additive_attention_mask, additive_mask=True
        )

        # 5. Prepare video latents
        latents = (
            Tensor.from_dlpack(model_inputs.latents)
            .to(device)
            .cast(DType.bfloat16)
        )
        # Pack latents: [B, C, F, H, W] -> [B, S, D]
        batch_size = latents.shape[0]
        latents = self._pack_latents(latents)

        # 6. Prepare audio latents
        num_mel_bins = 64  # From audio VAE config
        latent_mel_bins = num_mel_bins // self.audio_vae_mel_compression_ratio
        duration_s = num_frames / frame_rate
        audio_latents_per_second = 16_000 / 160 / 4.0
        audio_num_frames = round(duration_s * audio_latents_per_second)
        audio_shape = (batch_size, 8, audio_num_frames, latent_mel_bins)
        audio_latents = random.gaussian(audio_shape, dtype=DType.float32).to(device)
        audio_latents = self._pack_audio_latents(audio_latents)

        # 7. Pre-compute positional embeddings
        video_coords = self.model.transformer.rope.prepare_video_coords(
            latents.shape[0], latent_num_frames, latent_height, latent_width, device, fps=frame_rate
        )
        audio_coords = self.model.transformer.audio_rope.prepare_audio_coords(
            audio_latents.shape[0], audio_num_frames, device
        )

        num_warmup_steps = model_inputs.num_warmup_steps

        # 8. Denoising loop
        with tqdm(total=num_inference_steps, desc="Denoising") as progress_bar:
            for i, t in enumerate(timesteps):
                # Prepare CFG inputs
                if self.do_classifier_free_guidance:
                    latent_model_input = F.concat([latents, latents], axis=0)
                    audio_latent_model_input = F.concat([audio_latents, audio_latents], axis=0)
                else:
                    latent_model_input = latents
                    audio_latent_model_input = audio_latents

                latent_model_input = latent_model_input.cast(prompt_embeds.dtype)
                audio_latent_model_input = audio_latent_model_input.cast(prompt_embeds.dtype)

                # Broadcast timestep
                if self.do_classifier_free_guidance:
                    timestep = F.concat([t.unsqueeze(0), t.unsqueeze(0)], axis=0)
                else:
                    timestep = t.unsqueeze(0)

                noise_pred_video, noise_pred_audio = self.model.transformer(
                    hidden_states=latent_model_input,
                    audio_hidden_states=audio_latent_model_input,
                    encoder_hidden_states=connector_prompt_embeds,
                    audio_encoder_hidden_states=connector_audio_prompt_embeds,
                    timestep=timestep,
                    encoder_attention_mask=connector_attention_mask,
                    audio_encoder_attention_mask=connector_attention_mask,
                    num_frames=latent_num_frames,
                    height=latent_height,
                    width=latent_width,
                    fps=frame_rate,
                    audio_num_frames=audio_num_frames,
                    video_coords=video_coords,
                    audio_coords=audio_coords,
                    attention_kwargs=model_inputs.attention_kwargs,
                )
                noise_pred_video = noise_pred_video.cast(DType.float32)
                noise_pred_audio = noise_pred_audio.cast(DType.float32)

                if self.do_classifier_free_guidance:
                    # Split into uncond and cond predictions
                    noise_pred_video_uncond = noise_pred_video[0:1]
                    noise_pred_video_text = noise_pred_video[1:2]
                    noise_pred_video = noise_pred_video_uncond + guidance_scale * (
                        noise_pred_video_text - noise_pred_video_uncond
                    )

                    noise_pred_audio_uncond = noise_pred_audio[0:1]
                    noise_pred_audio_text = noise_pred_audio[1:2]
                    noise_pred_audio = noise_pred_audio_uncond + guidance_scale * (
                        noise_pred_audio_text - noise_pred_audio_uncond
                    )

                # Scheduler step
                audio_latents = self._scheduler_step(audio_latents, noise_pred_audio, sigmas, i)
                latents = self._scheduler_step(latents, noise_pred_video, sigmas, i)

        # 9. Decode latents to video
        # Unpack latents: [B, S, D] -> [B, C, F, H, W]
        latents = self._unpack_latents(latents, latent_num_frames, latent_height, latent_width)
        latents = self._denormalize_latents(latents)

        video = self.model.vae.decode(latents.cast(DType.bfloat16))

        # Scale to [0, 1] and permute to [B, F, H, W, C]
        video = (video / 2.0 + 0.5).clip(0.0, 1.0)
        video = video.permute(0, 2, 3, 4, 1)

        # 10. Decode audio
        audio_latents = self._unpack_audio_latents(audio_latents, audio_num_frames, latent_mel_bins)
        audio_latents = self._denormalize_audio_latents(audio_latents)
        mel_spectrograms = self.model.vae_audio.decode(audio_latents.cast(DType.bfloat16))
        audio = self.model.vocoder(mel_spectrograms)

        return ModelOutputs(
            hidden_states=cast(Buffer, video.driver_tensor),
            logits=cast(Buffer, audio.driver_tensor),
        )
