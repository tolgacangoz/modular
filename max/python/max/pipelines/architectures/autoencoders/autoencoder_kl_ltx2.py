# ===----------------------------------------------------------------------=== #
# Copyright (c) 2026, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
"""LTX2 Video Autoencoder Architecture."""

from typing import Any

import max.experimental.functional as F
import max.nn.module_v3 as nn
from max.driver import Device
from max.dtype import DType
from max.experimental import random
from max.experimental.tensor import Tensor
from max.graph import DeviceRef, TensorType
from max.graph.weights import Weights
from max.pipelines.lib import SupportedEncoding

from ..common_layers.activation import activation_function_from_name
from ..embeddings import PixArtAlphaCombinedTimestepSizeEmbeddings
from .model import BaseAutoencoderModel
from .model_config import AutoencoderKLLTX2VideoConfig


def pixel_shuffle_3d_merge(x: Tensor, stride: tuple[int, int, int]) -> Tensor:
    """Robust 3D pixel shuffle merge using F.concat instead of reshape.

    This bypasses MAX compiler symbolic validation issues with symbolic products.
    Input x is rank 8: [B, C, D, d, H, h, W, w]
    Output is rank 5: [B, C, D*d, H*h, W*w]
    """
    d, h_s, w_s = stride

    # Merge (2, 3) -> D, d
    slices = [x[:, :, :, i, :, :, :, :] for i in range(d)]
    x = F.concat(slices, axis=2)  # [B, C, D*d, H, h, W, w]

    # Merge (4, 5) -> H, h
    slices = [x[:, :, :, :, i, :, :] for i in range(h_s)]
    x = F.concat(slices, axis=3)  # [B, C, D*d, H*h, W, w]

    # Merge (6, 7) -> W, w
    slices = [x[:, :, :, :, :, i] for i in range(w_s)]
    x = F.concat(slices, axis=4)  # [B, C, D*d, H*h, W*w]

    return x


class PerChannelRMSNorm(nn.Module[[Tensor], Tensor]):
    """Per-channel RMS normalization layer."""

    def __init__(self, channel_dim: int = 1, eps: float = 1e-8) -> None:
        super().__init__()
        self.channel_dim = channel_dim
        self.eps = eps

    def forward(self, x: Tensor) -> Tensor:
        mean_sq = F.mean(x**2, axis=self.channel_dim)
        rms = F.sqrt(mean_sq + self.eps)
        return x / rms


class LTX2VideoCausalConv3d(nn.Module[[Tensor, bool], Tensor]):
    """Causal or non-causal 3D convolution."""

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int | tuple[int, int, int] = 3,
        stride: int | tuple[int, int, int] = 1,
        dilation: int | tuple[int, int, int] = 1,
        groups: int = 1,
        spatial_padding_mode: str = "zeros",  # Placeholder for padding logic
    ) -> None:
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = (
            kernel_size
            if isinstance(kernel_size, tuple)
            else (kernel_size, kernel_size, kernel_size)
        )

        dilation = dilation if isinstance(dilation, tuple) else (dilation, 1, 1)
        stride = (
            stride if isinstance(stride, tuple) else (stride, stride, stride)
        )

        # Spatial padding (height and width)
        height_pad = self.kernel_size[1] // 2
        width_pad = self.kernel_size[2] // 2
        # Padding for [depth, height, width]
        padding = (0, height_pad, width_pad)

        self.conv = nn.Conv3d(
            kernel_size=self.kernel_size,
            in_channels=in_channels,
            out_channels=out_channels,
            stride=stride,
            dilation=dilation,
            num_groups=groups,
            padding=padding,
            permute=True,  # LTX2 uses PyTorch format internally but we refactoring to MAX
        )

    def forward(self, hidden_states: Tensor, causal: bool = True) -> Tensor:
        tk = self.kernel_size[0]

        if causal:
            # Pad left (past) for causality
            # x shape: [B, C, D, H, W]
            # Use concat instead of tile for 5D
            pad_left = F.concat(
                [hidden_states[:, :, :1, :, :]] * (tk - 1), axis=2
            )
            hidden_states = F.concat([pad_left, hidden_states], axis=2)
        else:
            # Pad left (past) and right (future) for non-causal
            # Use concat instead of tile for 5D
            pad_left = F.concat(
                [hidden_states[:, :, :1, :, :]] * (tk // 2), axis=2
            )
            pad_right = F.concat(
                [hidden_states[:, :, -1:, :, :]] * (tk // 2), axis=2
            )
            hidden_states = F.concat(
                [pad_left, hidden_states, pad_right], axis=2
            )

        return self.conv(hidden_states)


class LTX2VideoResnetBlock3d(nn.Module[[Tensor, Tensor | None, bool], Tensor]):
    """3D ResNet block used in LTX2 Video decoder."""

    def __init__(
        self,
        in_channels: int,
        out_channels: int | None = None,
        dropout: float = 0.0,
        eps: float = 1e-6,
        non_linearity: str = "swish",
        inject_noise: bool = False,
        timestep_conditioning: bool = False,
        spatial_padding_mode: str = "zeros",
    ) -> None:
        super().__init__()
        out_channels = out_channels or in_channels

        self.nonlinearity = activation_function_from_name(non_linearity)

        self.norm1 = PerChannelRMSNorm()
        self.conv1 = LTX2VideoCausalConv3d(
            in_channels,
            out_channels,
            kernel_size=3,
            spatial_padding_mode=spatial_padding_mode,
        )

        self.norm2 = PerChannelRMSNorm()
        self.dropout = nn.Dropout(dropout)
        self.conv2 = LTX2VideoCausalConv3d(
            out_channels,
            out_channels,
            kernel_size=3,
            spatial_padding_mode=spatial_padding_mode,
        )

        self.norm3: nn.LayerNorm | None = None
        self.conv_shortcut: nn.Module[[Tensor], Tensor] | None = None
        if in_channels != out_channels:
            self.norm3 = nn.LayerNorm(
                in_channels, eps=eps, elementwise_affine=True, use_bias=True
            )
            # LTX 2.0 uses a normal Conv3d here rather than LTXVideoCausalConv3d
            self.conv_shortcut = nn.Conv3d(
                kernel_size=1,
                in_channels=in_channels,
                out_channels=out_channels,
                stride=1,
                permute=True,
            )

        self.per_channel_scale1: Tensor | None = None
        self.per_channel_scale2: Tensor | None = None
        if inject_noise:
            self.per_channel_scale1 = Tensor.constant(
                Tensor.zeros((in_channels, 1, 1))
            )
            self.per_channel_scale2 = Tensor.constant(
                Tensor.zeros((in_channels, 1, 1))
            )

        self.scale_shift_table: Tensor | None = None
        if timestep_conditioning:
            self.scale_shift_table = Tensor.constant(
                random.gaussian((4, in_channels)) / in_channels**0.5
            )

    def forward(
        self,
        inputs: Tensor,
        temb: Tensor | None = None,
        seed: int | None = None,
        causal: bool = True,
    ) -> Tensor:
        hidden_states = inputs

        hidden_states = self.norm1(hidden_states)

        if self.scale_shift_table is not None:
            temb = (
                temb.reshape((temb.shape[0], 4, -1))[..., None, None, None]
                + self.scale_shift_table[None, ..., None, None, None]
            )
            shift_1 = temb[:, 0]
            scale_1 = temb[:, 1]
            shift_2 = temb[:, 2]
            scale_2 = temb[:, 3]
            hidden_states = hidden_states * (1 + scale_1) + shift_1

        hidden_states = self.nonlinearity(hidden_states)
        hidden_states = self.conv1(hidden_states, causal=causal)

        if self.per_channel_scale1 is not None:
            spatial_shape = hidden_states.shape[-2:]
            spatial_noise = random.gaussian(
                spatial_shape,
                device=hidden_states.device,
                dtype=hidden_states.dtype,
            )[None]
            hidden_states = (
                hidden_states
                + (spatial_noise * self.per_channel_scale1)[None, :, None, ...]
            )

        hidden_states = self.norm2(hidden_states)

        if self.scale_shift_table is not None:
            hidden_states = hidden_states * (1 + scale_2) + shift_2

        hidden_states = self.nonlinearity(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.conv2(hidden_states, causal=causal)

        if self.per_channel_scale2 is not None:
            spatial_shape = hidden_states.shape[-2:]
            spatial_noise = random.gaussian(
                spatial_shape,
                device=hidden_states.device,
                dtype=hidden_states.dtype,
            )[None]
            hidden_states = (
                hidden_states
                + (spatial_noise * self.per_channel_scale2)[None, :, None, ...]
            )

        if self.norm3 is not None:
            inputs = self.norm3(inputs.permute((0, 4, 2, 3, 1))).permute(
                (0, 4, 2, 3, 1)
            )

        if self.conv_shortcut is not None:
            inputs = self.conv_shortcut(inputs)

        hidden_states = hidden_states + inputs
        return hidden_states


class LTXVideoDownsampler3d(nn.Module[[Tensor, bool], Tensor]):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        stride: int | tuple[int, int, int] = 1,
        spatial_padding_mode: str = "zeros",
    ):
        super().__init__()

        self.stride = (
            stride if isinstance(stride, tuple) else (stride, stride, stride)
        )
        self.group_size = (
            in_channels * stride[0] * stride[1] * stride[2]
        ) // out_channels

        out_channels = out_channels // (
            self.stride[0] * self.stride[1] * self.stride[2]
        )

        self.conv = LTX2VideoCausalConv3d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=3,
            stride=1,
            spatial_padding_mode=spatial_padding_mode,
        )

    def forward(self, hidden_states: Tensor, causal: bool = True) -> Tensor:
        hidden_states = F.concat(
            [hidden_states[:, :, : self.stride[0] - 1], hidden_states], axis=2
        )

        N, C, D, H, W = hidden_states.shape

        # Rebind to satisfy symbolic shape matching
        depth, h_stride, w_stride = self.stride
        intermediate_5d_shape = (
            N,
            C,
            (D // depth) * depth,
            (H // h_stride) * h_stride,
            (W // w_stride) * w_stride,
        )
        residual = hidden_states.rebind(intermediate_5d_shape).reshape(
            (
                N,
                C,
                D // depth,
                depth,
                H // h_stride,
                h_stride,
                W // w_stride,
                w_stride,
            )
        )
        residual = residual.permute((0, 1, 3, 5, 7, 2, 4, 6))
        residual = F.flatten(residual, 1, 4)

        r_shape = residual.shape
        # Rebind to satisfy symbolic shape matching
        new_c = r_shape[1] // self.group_size
        intermediate_5d_shape = (
            r_shape[0],
            new_c * self.group_size,
            r_shape[2],
            r_shape[3],
            r_shape[4],
        )
        residual = residual.rebind(intermediate_5d_shape).reshape(
            (
                r_shape[0],
                new_c,
                self.group_size,
                r_shape[2],
                r_shape[3],
                r_shape[4],
            )
        )
        residual = residual.mean(axis=2)

        hidden_states = self.conv(hidden_states, causal=causal)

        N, C, D, H, W = hidden_states.shape
        intermediate_5d_shape = (
            N,
            C,
            (D // depth) * depth,
            (H // h_stride) * h_stride,
            (W // w_stride) * w_stride,
        )
        hidden_states = hidden_states.rebind(intermediate_5d_shape).reshape(
            (
                N,
                C,
                D // depth,
                depth,
                H // h_stride,
                h_stride,
                W // w_stride,
                w_stride,
            )
        )
        hidden_states = hidden_states.permute((0, 1, 3, 5, 7, 2, 4, 6))
        hidden_states = F.flatten(hidden_states, 1, 4)

        hidden_states = hidden_states + residual

        return hidden_states


class LTXVideoUpsampler3d(nn.Module[[Tensor, bool], Tensor]):
    def __init__(
        self,
        in_channels: int,
        stride: int | tuple[int, int, int] = 1,
        residual: bool = False,
        upscale_factor: int = 1,
        spatial_padding_mode: str = "zeros",
    ) -> None:
        super().__init__()

        self.stride = (
            stride if isinstance(stride, tuple) else (stride, stride, stride)
        )
        self.residual = residual
        self.upscale_factor = upscale_factor

        out_channels = (
            in_channels * stride[0] * stride[1] * stride[2]
        ) // upscale_factor

        self.conv = LTX2VideoCausalConv3d(
            in_channels,
            out_channels,
            kernel_size=3,
            stride=1,
            spatial_padding_mode=spatial_padding_mode,
        )

    def forward(self, hidden_states: Tensor, causal: bool = True) -> Tensor:
        batch_size, num_channels, num_frames, height, width = (
            hidden_states.shape
        )
        stride_prod = self.stride[0] * self.stride[1] * self.stride[2]

        if self.residual:
            residual = hidden_states.reshape(
                (
                    batch_size,
                    num_channels // stride_prod,
                    self.stride[0],
                    self.stride[1],
                    self.stride[2],
                    num_frames,
                    height,
                    width,
                )
            )
            residual = residual.permute((0, 1, 5, 2, 6, 3, 7, 4))
            depth, h_stride, w_stride = self.stride
            # Use concat-based merge to avoid symbolic product issues in reshape
            residual = pixel_shuffle_3d_merge(
                residual, (depth, h_stride, w_stride)
            )
            # Rebind to clean symbolic shape for downstream ops
            residual = residual.rebind(
                (
                    batch_size,
                    num_channels // stride_prod,
                    num_frames * depth,
                    height * h_stride,
                    width * w_stride,
                )
            )
            # Already 5D [B, C, D, H, W]
            repeats = (
                self.stride[0] * self.stride[1] * self.stride[2]
            ) // self.upscale_factor
            if repeats > 1:
                # Use concat instead of tile for 5D
                residual = F.concat([residual] * repeats, axis=1)
            residual = residual[:, :, self.stride[0] - 1 :]

        hidden_states = self.conv(hidden_states, causal=causal)
        num_channels = hidden_states.shape[1]
        hidden_states = hidden_states.reshape(
            (
                batch_size,
                num_channels // stride_prod,
                self.stride[0],
                self.stride[1],
                self.stride[2],
                num_frames,
                height,
                width,
            )
        )
        hidden_states = hidden_states.permute((0, 1, 5, 2, 6, 3, 7, 4))
        depth, h_stride, w_stride = self.stride
        # Use concat-based merge to avoid symbolic product issues in reshape
        hidden_states = pixel_shuffle_3d_merge(
            hidden_states, (depth, h_stride, w_stride)
        )
        # Rebind to clean symbolic shape for downstream ops
        hidden_states = hidden_states.rebind(
            (
                batch_size,
                num_channels // stride_prod,
                num_frames * depth,
                height * h_stride,
                width * w_stride,
            )
        )
        # Already 5D [B, C, D, H, W]
        hidden_states = hidden_states[:, :, self.stride[0] - 1 :]

        if self.residual:
            hidden_states = hidden_states + residual

        return hidden_states


class LTX2VideoDownBlock3D(
    nn.Module[[Tensor, int | None, int | None, bool], Tensor]
):
    def __init__(
        self,
        in_channels: int,
        out_channels: int | None = None,
        num_layers: int = 1,
        dropout: float = 0.0,
        resnet_eps: float = 1e-6,
        resnet_act_fn: str = "swish",
        spatio_temporal_scale: bool = True,
        downsample_type: str = "conv",
        spatial_padding_mode: str = "zeros",
    ):
        super().__init__()

        out_channels = out_channels or in_channels

        resnets = []
        for _ in range(num_layers):
            resnets.append(
                LTX2VideoResnetBlock3d(
                    in_channels=in_channels,
                    out_channels=in_channels,
                    dropout=dropout,
                    eps=resnet_eps,
                    non_linearity=resnet_act_fn,
                    spatial_padding_mode=spatial_padding_mode,
                )
            )
        self.resnets = nn.ModuleList(resnets)

        self.downsamplers = None
        if spatio_temporal_scale:
            self.downsamplers = nn.ModuleList()

            if downsample_type == "conv":
                self.downsamplers.append(
                    LTX2VideoCausalConv3d(
                        in_channels=in_channels,
                        out_channels=in_channels,
                        kernel_size=3,
                        stride=(2, 2, 2),
                        spatial_padding_mode=spatial_padding_mode,
                    )
                )
            elif downsample_type == "spatial":
                self.downsamplers.append(
                    LTXVideoDownsampler3d(
                        in_channels=in_channels,
                        out_channels=out_channels,
                        stride=(1, 2, 2),
                        spatial_padding_mode=spatial_padding_mode,
                    )
                )
            elif downsample_type == "temporal":
                self.downsamplers.append(
                    LTXVideoDownsampler3d(
                        in_channels=in_channels,
                        out_channels=out_channels,
                        stride=(2, 1, 1),
                        spatial_padding_mode=spatial_padding_mode,
                    )
                )
            elif downsample_type == "spatiotemporal":
                self.downsamplers.append(
                    LTXVideoDownsampler3d(
                        in_channels=in_channels,
                        out_channels=out_channels,
                        stride=(2, 2, 2),
                        spatial_padding_mode=spatial_padding_mode,
                    )
                )

    def forward(
        self,
        hidden_states: Tensor,
        temb: Tensor | None = None,
        seed: int | None = None,
        causal: bool = True,
    ) -> Tensor:
        r"""Forward method of the `LTXDownBlock3D` class."""

        for resnet in self.resnets:
            hidden_states = resnet(hidden_states, temb, seed, causal=causal)

        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states, causal=causal)

        return hidden_states


class LTX2VideoMidBlock3d(
    nn.Module[[Tensor, Tensor | None, int | None, bool], Tensor]
):
    def __init__(
        self,
        in_channels: int,
        num_layers: int = 1,
        dropout: float = 0.0,
        resnet_eps: float = 1e-6,
        resnet_act_fn: str = "swish",
        inject_noise: bool = False,
        timestep_conditioning: bool = False,
        spatial_padding_mode: str = "zeros",
    ) -> None:
        super().__init__()

        self.time_embedder = None
        if timestep_conditioning:
            self.time_embedder = PixArtAlphaCombinedTimestepSizeEmbeddings(
                in_channels * 4, 0
            )

        resnets = []
        for _ in range(num_layers):
            resnets.append(
                LTX2VideoResnetBlock3d(
                    in_channels=in_channels,
                    out_channels=in_channels,
                    dropout=dropout,
                    eps=resnet_eps,
                    non_linearity=resnet_act_fn,
                    inject_noise=inject_noise,
                    timestep_conditioning=timestep_conditioning,
                    spatial_padding_mode=spatial_padding_mode,
                )
            )
        self.resnets = nn.ModuleList(resnets)

    def forward(
        self,
        hidden_states: Tensor,
        temb: Tensor | None = None,
        seed: int | None = None,
        causal: bool = True,
    ) -> Tensor:
        r"""Forward method of the `LTXMidBlock3D` class."""

        if self.time_embedder is not None:
            temb = self.time_embedder(
                timestep=F.flatten(temb),
                resolution=None,
                aspect_ratio=None,
                batch_size=hidden_states.shape[0],
                hidden_dtype=hidden_states.dtype,
            )
            temb = temb.reshape((hidden_states.shape[0], -1, 1, 1, 1))

        for resnet in self.resnets:
            hidden_states = resnet(hidden_states, temb, seed, causal=causal)

        return hidden_states


class LTX2VideoUpBlock3d(
    nn.Module[[Tensor, Tensor | None, int | None, bool], Tensor]
):
    r"""
    Up block used in the LTXVideo model.

    Args:
        in_channels (`int`):
            Number of input channels.
        out_channels (`int`, *optional*):
            Number of output channels. If None, defaults to `in_channels`.
        num_layers (`int`, defaults to `1`):
            Number of resnet layers.
        dropout (`float`, defaults to `0.0`):
            Dropout rate.
        resnet_eps (`float`, defaults to `1e-6`):
            Epsilon value for normalization layers.
        resnet_act_fn (`str`, defaults to `"swish"`):
            Activation function to use.
        spatio_temporal_scale (`bool`, defaults to `True`):
            Whether or not to use a downsampling layer. If not used, output dimension would be same as input dimension.
            Whether or not to downsample across temporal dimension.
        is_causal (`bool`, defaults to `True`):
            Whether this layer behaves causally (future frames depend only on past frames) or not.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int | None = None,
        num_layers: int = 1,
        dropout: float = 0.0,
        resnet_eps: float = 1e-6,
        resnet_act_fn: str = "swish",
        spatio_temporal_scale: bool = True,
        inject_noise: bool = False,
        timestep_conditioning: bool = False,
        upsample_residual: bool = False,
        upscale_factor: int = 1,
        spatial_padding_mode: str = "zeros",
    ):
        super().__init__()

        out_channels = out_channels or in_channels

        self.time_embedder = None
        if timestep_conditioning:
            self.time_embedder = PixArtAlphaCombinedTimestepSizeEmbeddings(
                in_channels * 4, 0
            )

        self.conv_in = None
        if in_channels != out_channels:
            self.conv_in = LTX2VideoResnetBlock3d(
                in_channels=in_channels,
                out_channels=out_channels,
                dropout=dropout,
                eps=resnet_eps,
                non_linearity=resnet_act_fn,
                inject_noise=inject_noise,
                timestep_conditioning=timestep_conditioning,
                spatial_padding_mode=spatial_padding_mode,
            )

        self.upsamplers = None
        if spatio_temporal_scale:
            self.upsamplers = nn.ModuleList(
                [
                    LTXVideoUpsampler3d(
                        out_channels * upscale_factor,
                        stride=(2, 2, 2),
                        residual=upsample_residual,
                        upscale_factor=upscale_factor,
                        spatial_padding_mode=spatial_padding_mode,
                    )
                ]
            )

        resnets = []
        for _ in range(num_layers):
            resnets.append(
                LTX2VideoResnetBlock3d(
                    in_channels=out_channels,
                    out_channels=out_channels,
                    dropout=dropout,
                    eps=resnet_eps,
                    non_linearity=resnet_act_fn,
                    inject_noise=inject_noise,
                    timestep_conditioning=timestep_conditioning,
                    spatial_padding_mode=spatial_padding_mode,
                )
            )
        self.resnets = nn.ModuleList(resnets)

    def forward(
        self,
        hidden_states: Tensor,
        temb: Tensor | None = None,
        seed: int | None = None,
        causal: bool = True,
    ) -> Tensor:
        if self.conv_in is not None:
            hidden_states = self.conv_in(
                hidden_states, temb, seed, causal=causal
            )

        if self.time_embedder is not None:
            temb = self.time_embedder(
                timestep=F.flatten(temb),
                resolution=None,
                aspect_ratio=None,
                batch_size=hidden_states.shape[0],
                hidden_dtype=hidden_states.dtype,
            )
            temb = temb.reshape((hidden_states.shape[0], -1, 1, 1, 1))

        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states, causal=causal)

        for resnet in self.resnets:
            hidden_states = resnet(hidden_states, temb, seed, causal=causal)

        return hidden_states


class LTX2VideoDecoder3d(nn.Module[[Tensor, Tensor | None, bool], Tensor]):
    def __init__(
        self,
        in_channels: int = 128,
        out_channels: int = 3,
        block_out_channels: tuple[int, ...] = (256, 512, 1024),
        spatio_temporal_scaling: tuple[bool, ...] = (True, True, True),
        layers_per_block: tuple[int, ...] = (5, 5, 5, 5),
        patch_size: int = 4,
        patch_size_t: int = 1,
        resnet_norm_eps: float = 1e-6,
        is_causal: bool = False,
        inject_noise: tuple[bool, ...] = (False, False, False),
        timestep_conditioning: bool = False,
        upsample_residual: tuple[bool, ...] = (True, True, True),
        upsample_factor: tuple[int, ...] = (2, 2, 2),
        spatial_padding_mode: str = "reflect",
        device: DeviceRef | None = None,
        dtype: DType | None = None,
    ) -> None:
        super().__init__()

        self.patch_size = patch_size
        self.patch_size_t = patch_size_t
        self.out_channels = out_channels * patch_size**2
        self.is_causal = is_causal
        self.device = device
        self.dtype = dtype
        self.in_channels = in_channels

        block_out_channels = tuple(reversed(block_out_channels))
        spatio_temporal_scaling = tuple(reversed(spatio_temporal_scaling))
        layers_per_block = tuple(reversed(layers_per_block))
        inject_noise = tuple(reversed(inject_noise))
        upsample_residual = tuple(reversed(upsample_residual))
        upsample_factor = tuple(reversed(upsample_factor))
        output_channel = block_out_channels[0]

        self.conv_in = LTX2VideoCausalConv3d(
            in_channels,
            output_channel,
            kernel_size=3,
            stride=1,
            spatial_padding_mode=spatial_padding_mode,
        )

        self.mid_block = LTX2VideoMidBlock3d(
            in_channels=output_channel,
            num_layers=layers_per_block[0],
            resnet_eps=resnet_norm_eps,
            inject_noise=inject_noise[0],
            timestep_conditioning=timestep_conditioning,
            spatial_padding_mode=spatial_padding_mode,
        )

        # up blocks
        num_block_out_channels = len(block_out_channels)
        self.up_blocks = nn.ModuleList([])
        for i in range(num_block_out_channels):
            input_channel = output_channel // upsample_factor[i]
            output_channel = block_out_channels[i] // upsample_factor[i]

            up_block = LTX2VideoUpBlock3d(
                in_channels=input_channel,
                out_channels=output_channel,
                num_layers=layers_per_block[i + 1],
                resnet_eps=resnet_norm_eps,
                spatio_temporal_scale=spatio_temporal_scaling[i],
                inject_noise=inject_noise[i + 1],
                timestep_conditioning=timestep_conditioning,
                upsample_residual=upsample_residual[i],
                upscale_factor=upsample_factor[i],
                spatial_padding_mode=spatial_padding_mode,
            )

            self.up_blocks.append(up_block)

        # out
        self.norm_out = PerChannelRMSNorm()
        self.conv_act = activation_function_from_name("silu")
        self.conv_out = LTX2VideoCausalConv3d(
            output_channel,
            self.out_channels,
            kernel_size=3,
            stride=1,
            spatial_padding_mode=spatial_padding_mode,
        )

        # timestep embedding
        self.time_embedder = None
        self.scale_shift_table = None
        self.timestep_scale_multiplier = None
        if timestep_conditioning:
            self.timestep_scale_multiplier = Tensor.constant(
                1000.0, dtype=DType.float32
            )
            self.time_embedder = PixArtAlphaCombinedTimestepSizeEmbeddings(
                output_channel * 2, 0
            )
            self.scale_shift_table = Tensor.constant(
                random.gaussian(2, output_channel) / output_channel**0.5
            )

    def input_types(self) -> tuple[TensorType, ...]:
        if self.dtype is None:
            raise ValueError("dtype must be set for input_types")
        if self.device is None:
            raise ValueError("device must be set for input_types")

        # Hardcoded for height=512, width=768, num_frames=121, frame_rate=24:
        #   in_channels       = 128
        #   latent_num_frames = (121-1)//8+1 = 16
        #   latent_height     = 512//32      = 16
        #   latent_width      = 768//32      = 24
        return (
            TensorType(
                self.dtype,
                shape=[1, self.in_channels, 16, 16, 24],
                device=self.device,
            ),
        )

    def forward(
        self,
        hidden_states: Tensor,
        temb: Tensor | None = None,
        causal: bool | None = None,
    ) -> Tensor:
        causal = causal or self.is_causal

        hidden_states = self.conv_in(hidden_states, causal=causal)

        if self.timestep_scale_multiplier is not None:
            temb = temb * self.timestep_scale_multiplier

        hidden_states = self.mid_block(hidden_states, temb, causal=causal)

        for up_block in self.up_blocks:
            hidden_states = up_block(hidden_states, temb, causal=causal)

        hidden_states = self.norm_out(hidden_states)

        if self.time_embedder is not None:
            temb = self.time_embedder(
                timestep=F.flatten(temb),
                resolution=None,
                aspect_ratio=None,
                batch_size=hidden_states.shape[0],
                hidden_dtype=hidden_states.dtype,
            )
            temb = temb.reshape((hidden_states.shape[0], -1, 1, 1, 1)).reshape(
                (hidden_states.shape[0], 2, -1, 1, 1, 1)
            )
            temb = temb + self.scale_shift_table[None, ..., None, None, None]
            shift = temb[:, 0]
            scale = temb[:, 1]
            hidden_states = hidden_states * (1 + scale) + shift

        hidden_states = self.conv_act(hidden_states)
        hidden_states = self.conv_out(hidden_states, causal=causal)

        p = self.patch_size
        p_t = self.patch_size_t

        batch_size, num_channels, num_frames, height, width = (
            hidden_states.shape
        )
        target_padding_channels = num_channels // (p_t * p * p)
        # Rebind to satisfy symbolic shape matching
        intermediate_5d_shape = (
            batch_size,
            target_padding_channels * p_t * p * p,
            num_frames,
            height,
            width,
        )
        hidden_states = hidden_states.rebind(intermediate_5d_shape).reshape(
            (
                batch_size,
                target_padding_channels,
                p_t,
                p,
                p,
                num_frames,
                height,
                width,
            ),
        )
        hidden_states = hidden_states.permute((0, 1, 5, 2, 6, 4, 7, 3))
        # Use concat-based merge to avoid symbolic product issues in reshape
        hidden_states = pixel_shuffle_3d_merge(hidden_states, (p_t, p, p))
        # Rebind to clean symbolic shape for downstream ops
        hidden_states = hidden_states.rebind(
            (
                batch_size,
                target_padding_channels,
                num_frames * p_t,
                height * p,
                width * p,
            )
        )
        # Already 5D [B, C, D, H, W]

        return hidden_states


class AutoencoderKLLTX2Video(nn.Module[[Tensor, Tensor | None, bool], Tensor]):
    """Refactored LTX2 Video Autoencoder."""

    def __init__(self, config: AutoencoderKLLTX2VideoConfig) -> None:
        super().__init__()
        self.config = config
        self.decoder = LTX2VideoDecoder3d(
            in_channels=config.latent_channels,
            out_channels=config.out_channels,
            patch_size=config.patch_size,
            patch_size_t=config.patch_size_t,
            is_causal=config.decoder_causal,
            block_out_channels=config.decoder_block_out_channels,
            layers_per_block=config.decoder_layers_per_block,
            inject_noise=config.decoder_inject_noise,
            upsample_residual=config.upsample_residual,
            upsample_factor=config.upsample_factor,
            spatio_temporal_scaling=config.decoder_spatio_temporal_scaling,
            resnet_norm_eps=config.resnet_norm_eps,
            timestep_conditioning=config.timestep_conditioning,
            spatial_padding_mode=config.decoder_spatial_padding_mode,
            device=config.device,
            dtype=config.dtype,
        )

    def forward(
        self,
        z: Tensor,
        timestep: Tensor | None = None,
        causal: bool | None = None,
    ) -> Tensor:
        if causal is None:
            causal = self.config.decoder_causal
        return self.decoder(z, timestep, causal=causal)


class AutoencoderKLLTX2VideoModel(BaseAutoencoderModel):
    """ComponentModel wrapper for LTX2 Video Autoencoder."""

    def __init__(
        self,
        config: dict[str, Any],
        encoding: SupportedEncoding,
        devices: list[Device],
        weights: Weights,
    ) -> None:
        super().__init__(
            config=config,
            encoding=encoding,
            devices=devices,
            weights=weights,
            config_class=AutoencoderKLLTX2VideoConfig,
            autoencoder_class=AutoencoderKLLTX2Video,
        )
